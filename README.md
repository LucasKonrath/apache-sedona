# Apache Sedona Docker Environment

A production-ready Docker setup for **Apache Sedona** spatial analytics with **Apache Spark 3.4.0**, featuring a complete data science stack and advanced spatial capabilities.

> **‚ö†Ô∏è Important Notes**:
> 1. This project uses **Sedona 1.8.0** with API changes. See [SEDONA_1.8_CHANGES.md](SEDONA_1.8_CHANGES.md)
> 2. **When opening `advanced-sedona-examples.ipynb`**: Run `Kernel ‚Üí Restart & Run All` to ensure all geometry columns are created correctly. See [MUST_RERUN_CELLS.md](MUST_RERUN_CELLS.md)

## üéØ Features

### **Core Spatial Stack**
- **Apache Spark 3.4.0** - Distributed computing with Hadoop 3 support
- **Apache Sedona 1.8.0** - Complete spatial data processing system
- **GeoTools Wrapper** - Full spatial functionality ‚úÖ **(Fixes FactoryException)**
- **Python/PySpark Integration** - Seamless spatial analytics in Python

### **Advanced Analytics & ML**
- **PySpark ML** - Machine learning with spatial features
- **Scikit-learn** - Additional ML algorithms and metrics
- **NumPy & Pandas** - High-performance data manipulation
- **SciPy** - Scientific computing and optimization

### **Rich Visualization Suite**
- **Jupyter Notebook/Lab** - Interactive development environment
- **Matplotlib & Seaborn** - Statistical plotting and visualization
- **Plotly** - Interactive charts and dashboards  
- **Folium** - Interactive maps with heatmaps and overlays
- **GeoPandas & Shapely** - Geospatial data manipulation
- **Multi-service setup** with Docker Compose   
- **Example notebooks** and spatial data processing workflows

## Quick Start

### Option 1: Using Docker Compose (Recommended)

```bash
# Start all services (Master, Worker, Jupyter)
docker-compose up -d

# Access Jupyter Notebook
open http://localhost:8888

# Access Spark Master UI
open http://localhost:8080

# Access Spark Worker UI
open http://localhost:8081
```

### Option 2: Using Docker directly

```bash
# Build the image
docker build -t apache-sedona .

# Run different services
docker run -p 8888:8888 apache-sedona jupyter     # Jupyter Notebook
docker run -p 8080:8080 apache-sedona master      # Spark Master
docker run -it apache-sedona pyspark              # Interactive PySpark shell
docker run -it apache-sedona shell                # Spark Scala shell
```

## Available Services

### 1. Jupyter Notebook Environment
- **URL**: http://localhost:8888
- **Features**: Interactive notebooks with Sedona pre-configured
- **Usage**: Perfect for data exploration and analysis

### 2. Spark Master/Worker Cluster
- **Master UI**: http://localhost:8080
- **Worker UI**: http://localhost:8081
- **Features**: Distributed processing capabilities

### 3. Interactive Shells
- **PySpark**: Python shell with Sedona
- **Spark Shell**: Scala shell with Sedona

## Directory Structure

```
apache-sedona/
‚îú‚îÄ‚îÄ Dockerfile                      # Main container definition
‚îú‚îÄ‚îÄ docker-compose.yml              # Multi-service orchestration
‚îú‚îÄ‚îÄ .dockerignore                   # Docker build optimization
‚îú‚îÄ‚îÄ notebooks/                      # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ advanced-sedona-examples.ipynb  # 18 comprehensive examples
‚îÇ   ‚îî‚îÄ‚îÄ sedona-example.ipynb        # Basic examples
‚îú‚îÄ‚îÄ data/                           # Spatial data files
‚îú‚îÄ‚îÄ ADVANCED_EXAMPLES.md            # Detailed guide to all examples
‚îú‚îÄ‚îÄ QUICK_REFERENCE.md              # Quick reference for common patterns
‚îú‚îÄ‚îÄ UPGRADE_NOTES.md                # Version upgrade information
‚îî‚îÄ‚îÄ README.md                       # This file
```

## Example Usage

### 1. Start the Environment
```bash
docker-compose up -d
```

### 2. Open Jupyter Notebook
Navigate to http://localhost:8888 and open `sedona-example.ipynb`

### 3. Run Spatial Queries
```python
from pyspark.sql import SparkSession
from sedona.register import SedonaRegistrator

# Initialize Spark with Sedona
spark = SparkSession.builder.appName("SedonaApp").getOrCreate()
SedonaRegistrator.registerAll(spark)

# Create spatial data
df = spark.sql("""
    SELECT ST_Point(CAST(RAND() * 360 - 180 AS DECIMAL(10,6)), 
                    CAST(RAND() * 180 - 90 AS DECIMAL(10,6))) as geometry
    FROM range(1000)
""")

# Spatial operations
df.createOrReplaceTempView("points")
result = spark.sql("""
    SELECT ST_X(geometry) as lon, ST_Y(geometry) as lat 
    FROM points 
    WHERE ST_Within(geometry, ST_PolygonFromEnvelope(-10, -10, 10, 10))
""")
result.show()
```

## Spatial Data Formats Supported

- **Shapefile** (.shp, .shx, .dbf)
- **GeoJSON** (.geojson, .json)
- **CSV with WKT** (Well-Known Text geometry)
- **Parquet with spatial columns**
- **PostGIS databases** (with additional configuration)

## Development Workflow

### 1. Add Your Data
Place spatial data files in the `data/` directory:
```bash
data/
‚îú‚îÄ‚îÄ cities.shp
‚îú‚îÄ‚îÄ countries.geojson
‚îî‚îÄ‚îÄ points.csv
```

### 2. Create Notebooks
Add your analysis notebooks to the `notebooks/` directory.

### 3. Access from Container
Data and notebooks are automatically mounted and accessible from all services.

## Advanced Configuration

### Custom Spark Configuration
Edit the Spark configuration in the Dockerfile or mount a custom `spark-defaults.conf`:

```bash
docker run -v ./custom-spark.conf:/opt/spark/conf/spark-defaults.conf apache-sedona pyspark
```

### Memory and CPU Limits
Adjust resources in `docker-compose.yml`:
```yaml
services:
  spark-master:
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
```

## Troubleshooting

### Common Issues

1. **NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException**
   - **Cause**: Missing GeoTools dependencies
   - **Solution**: Run `./sedona.sh fix-geotools` to rebuild with the fix
   - **Alternative**: `docker-compose build --no-cache && docker-compose up -d`

2. **Port conflicts**: Change ports in `docker-compose.yml` if already in use

3. **Memory issues**: Increase Docker memory allocation

4. **Data access**: Ensure data files are in the `data/` directory

5. **Spark initialization errors**: Check logs with `./sedona.sh logs`

### Logs and Debugging
```bash
# View service logs
docker-compose logs spark-master
docker-compose logs jupyter

# Interactive debugging
docker run -it apache-sedona bash
```

## Contributing

1. Fork the repository
2. Create your feature branch
3. Add/modify Dockerfile or configurations
4. Test with sample spatial data
5. Submit a pull request

## Resources

### Documentation
- [Apache Sedona Documentation](https://sedona.apache.org/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Spatial SQL Reference](https://sedona.apache.org/api/sql/Overview/)
- [Python API Documentation](https://sedona.apache.org/api/python/)

### Project Guides
- **[ADVANCED_EXAMPLES.md](ADVANCED_EXAMPLES.md)** - Comprehensive guide to 18 spatial analysis examples
- **[QUICK_REFERENCE.md](QUICK_REFERENCE.md)** - Quick reference for common spatial patterns
- **[SQL_CHEATSHEET.md](SQL_CHEATSHEET.md)** - Copy-paste SQL patterns for common tasks
- **[UPGRADE_NOTES.md](UPGRADE_NOTES.md)** - Version upgrade information and changelog
- **[SEDONA_1.8_CHANGES.md](SEDONA_1.8_CHANGES.md)** - ‚ö†Ô∏è Important API changes in version 1.8.0

### Example Notebooks
- **advanced-sedona-examples.ipynb** - 18 comprehensive spatial analysis scenarios including:
  - Buffer zones & proximity analysis
  - Distance matrix calculations
  - Origin-destination flow analysis
  - Spatial outlier detection
  - Grid-based aggregation
  - Interactive visualizations
  - Performance optimization techniques
- **sedona-example.ipynb** - Basic introduction to Sedona features

## License

This Docker configuration is provided under the Apache License 2.0, consistent with Apache Sedona's licensing.
