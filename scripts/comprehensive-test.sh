#!/bin/bash

# üß™ Comprehensive Apache Sedona Environment Test
# Tests all components of the Sedona Docker environment

echo "üß™ Comprehensive Apache Sedona Test Suite"
echo "=========================================="
echo "Testing Docker environment, spatial functions, and Python packages..."
echo ""

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Test counters
TESTS_TOTAL=0
TESTS_PASSED=0
TESTS_FAILED=0

# Function to print test results
print_result() {
    TESTS_TOTAL=$((TESTS_TOTAL + 1))
    if [ $1 -eq 0 ]; then
        echo -e "${GREEN}‚úÖ $2${NC}"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo -e "${RED}‚ùå $2${NC}"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}

# Test 1: Docker Compose Services
echo -e "${BLUE}üìã Testing Docker Compose Services...${NC}"
docker-compose ps > /dev/null 2>&1
print_result $? "Docker Compose is available"

# Check if services are running
if docker-compose ps | grep -q "Up"; then
    print_result 0 "Docker services are running"
    SERVICES_RUNNING=true
else
    print_result 1 "Docker services are not running"
    echo -e "${YELLOW}   Starting services...${NC}"
    docker-compose up -d
    sleep 30
    if docker-compose ps | grep -q "Up"; then
        print_result 0 "Docker services started successfully"
        SERVICES_RUNNING=true
    else
        print_result 1 "Failed to start Docker services"
        SERVICES_RUNNING=false
    fi
fi

# Test 2: Service Accessibility
if [ "$SERVICES_RUNNING" = true ]; then
    echo ""
    echo -e "${BLUE}üåê Testing Service Accessibility...${NC}"
    
    # Test Spark Master UI
    if curl -s http://localhost:8080 > /dev/null; then
        print_result 0 "Spark Master UI accessible (port 8080)"
    else
        print_result 1 "Spark Master UI not accessible"
    fi
    
    # Test Jupyter
    if curl -s http://localhost:8888 > /dev/null; then
        print_result 0 "Jupyter accessible (port 8888)"
    else
        print_result 1 "Jupyter not accessible"
    fi
fi

# Test 3: Spatial Functions
if [ "$SERVICES_RUNNING" = true ]; then
    echo ""
    echo -e "${BLUE}üó∫Ô∏è  Testing Spatial Functions...${NC}"
    
    # Create a comprehensive spatial test
    docker-compose exec -T jupyter python3 << 'EOF'
import sys
try:
    # Test 1: Basic imports
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import expr, col
    print("‚úÖ PySpark imports successful")
    
    # Test 2: Spark session creation
    spark = SparkSession.builder \
        .appName("ComprehensiveTest") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator") \
        .config("spark.sql.extensions", "org.apache.sedona.sql.SedonaSqlExtensions") \
        .getOrCreate()
    print("‚úÖ Spark session created")
    
    # Test 3: Sedona registration
    from sedona.register import SedonaRegistrator
    SedonaRegistrator.registerAll(spark)
    print("‚úÖ Sedona functions registered")
    
    # Test 4: Basic spatial operations
    test_data = [
        (0.0, 0.0, 3.0, 4.0, "Point A to Point B"),
        (-1.0, -1.0, 1.0, 1.0, "Origin area"),
        (40.7589, -73.9851, 40.7614, -73.9776, "NYC coordinates")
    ]
    
    schema = ["x1", "y1", "x2", "y2", "description"]
    df = spark.createDataFrame(test_data, schema)
    
    # Test spatial functions
    spatial_df = df.select(
        col("description"),
        expr("ST_Point(x1, y1)").alias("point1"),
        expr("ST_Point(x2, y2)").alias("point2"),
        expr("ST_Distance(ST_Point(x1, y1), ST_Point(x2, y2))").alias("distance"),
        expr("ST_Area(ST_Buffer(ST_Point(x1, y1), 1.0))").alias("buffer_area"),
        expr("ST_Contains(ST_Buffer(ST_Point(x1, y1), 1.0), ST_Point(x2, y2))").alias("contains_test")
    )
    
    results = spatial_df.collect()
    
    for row in results:
        print(f"‚úÖ {row.description}: distance={row.distance:.2f}, buffer_area={row.buffer_area:.2f}, contains={row.contains_test}")
    
    # Test 5: Spatial joins
    zones_data = [
        ("zone1", "POLYGON((0 0, 2 0, 2 2, 0 2, 0 0))"),
        ("zone2", "POLYGON((40.75 -74.0, 40.77 -74.0, 40.77 -73.97, 40.75 -73.97, 40.75 -74.0))")
    ]
    
    zones_df = spark.createDataFrame(zones_data, ["zone_id", "wkt"])
    zones_df = zones_df.select(
        col("zone_id"),
        expr("ST_GeomFromWKT(wkt)").alias("geometry")
    )
    zones_df.createOrReplaceTempView("zones")
    
    points_data = [
        ("p1", 1.0, 1.0),
        ("p2", 40.7589, -73.9851),
        ("p3", 5.0, 5.0)
    ]
    
    points_df = spark.createDataFrame(points_data, ["point_id", "x", "y"])
    points_df = points_df.select(
        col("point_id"),
        expr("ST_Point(x, y)").alias("geometry")
    )
    points_df.createOrReplaceTempView("points")
    
    # Spatial join
    join_result = spark.sql("""
        SELECT p.point_id, z.zone_id
        FROM points p
        JOIN zones z ON ST_Contains(z.geometry, p.geometry)
    """)
    
    join_count = join_result.count()
    print(f"‚úÖ Spatial join completed: {join_count} matches found")
    
    # Test 6: Advanced spatial operations
    advanced_df = spark.sql("""
        SELECT 
            ST_ConvexHull(ST_Collect(geometry)) as convex_hull,
            ST_Centroid(ST_Collect(geometry)) as centroid
        FROM points
    """)
    
    advanced_results = advanced_df.collect()
    print("‚úÖ Advanced spatial operations (ConvexHull, Centroid) successful")
    
    spark.stop()
    print("‚úÖ All spatial function tests passed")
    
except Exception as e:
    print(f"‚ùå Spatial function test failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
EOF
    
    if [ $? -eq 0 ]; then
        print_result 0 "Basic spatial functions working"
        print_result 0 "Spatial joins working"
        print_result 0 "Advanced spatial operations working"
    else
        print_result 1 "Spatial function tests failed"
    fi
fi

# Test 4: Python Data Science Packages
if [ "$SERVICES_RUNNING" = true ]; then
    echo ""
    echo -e "${BLUE}üìä Testing Python Data Science Stack...${NC}"
    
    docker-compose exec -T jupyter python3 << 'EOF'
import sys
try:
    # Core data science packages
    import numpy as np
    import pandas as pd
    print("‚úÖ NumPy and Pandas imported")
    
    # Visualization packages
    import matplotlib.pyplot as plt
    import seaborn as sns
    import plotly.express as px
    print("‚úÖ Visualization packages imported")
    
    # Geospatial packages
    import folium
    import geopandas as gpd
    import shapely
    print("‚úÖ Geospatial packages imported")
    
    # Machine learning packages
    import sklearn
    from scipy import stats
    print("‚úÖ Machine learning packages imported")
    
    # Test basic functionality
    data = np.random.randn(100, 2)
    df = pd.DataFrame(data, columns=['x', 'y'])
    print(f"‚úÖ Data manipulation test: created DataFrame with {len(df)} rows")
    
    # Test visualization (without display)
    fig, ax = plt.subplots(figsize=(6, 4))
    ax.scatter(df['x'], df['y'])
    plt.close(fig)
    print("‚úÖ Matplotlib plotting test successful")
    
    # Test folium map creation
    m = folium.Map(location=[40.7589, -73.9851], zoom_start=12)
    print("‚úÖ Folium map creation test successful")
    
    print("‚úÖ All Python package tests passed")
    
except Exception as e:
    print(f"‚ùå Python package test failed: {e}")
    sys.exit(1)
EOF
    
    if [ $? -eq 0 ]; then
        print_result 0 "Core data science packages working"
        print_result 0 "Visualization packages working"  
        print_result 0 "Geospatial packages working"
        print_result 0 "Machine learning packages working"
    else
        print_result 1 "Python package tests failed"
    fi
fi

# Test 5: Notebook Accessibility
if [ "$SERVICES_RUNNING" = true ]; then
    echo ""
    echo -e "${BLUE}üìì Testing Notebook Environment...${NC}"
    
    # Check if notebook files exist
    if [ -f "notebooks/advanced-sedona-examples.ipynb" ]; then
        print_result 0 "Advanced examples notebook exists"
    else
        print_result 1 "Advanced examples notebook missing"
    fi
    
    # Test notebook API accessibility
    if curl -s "http://localhost:8888/api/contents" > /dev/null; then
        print_result 0 "Jupyter API accessible"
    else
        print_result 1 "Jupyter API not accessible"
    fi
fi

# Test 6: Performance Benchmarks
if [ "$SERVICES_RUNNING" = true ]; then
    echo ""
    echo -e "${BLUE}‚ö° Running Performance Benchmarks...${NC}"
    
    docker-compose exec -T jupyter python3 << 'EOF'
import time
import sys
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import expr
    
    spark = SparkSession.builder \
        .appName("PerformanceTest") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator") \
        .config("spark.sql.extensions", "org.apache.sedona.sql.SedonaSqlExtensions") \
        .getOrCreate()
    
    from sedona.register import SedonaRegistrator
    SedonaRegistrator.registerAll(spark)
    
    # Generate test data
    import random
    test_data = []
    for i in range(1000):
        x1, y1 = random.uniform(-1, 1), random.uniform(-1, 1)
        x2, y2 = random.uniform(-1, 1), random.uniform(-1, 1)
        test_data.append((i, x1, y1, x2, y2))
    
    df = spark.createDataFrame(test_data, ["id", "x1", "y1", "x2", "y2"])
    
    # Benchmark spatial operations
    start_time = time.time()
    
    result_df = df.select(
        expr("ST_Distance(ST_Point(x1, y1), ST_Point(x2, y2))").alias("distance"),
        expr("ST_Area(ST_Buffer(ST_Point(x1, y1), 0.1))").alias("area")
    )
    
    count = result_df.count()
    end_time = time.time()
    
    duration = end_time - start_time
    ops_per_second = count / duration if duration > 0 else 0
    
    print(f"‚úÖ Performance test: {count} operations in {duration:.2f}s ({ops_per_second:.0f} ops/sec)")
    
    spark.stop()
    
except Exception as e:
    print(f"‚ùå Performance test failed: {e}")
    sys.exit(1)
EOF
    
    if [ $? -eq 0 ]; then
        print_result 0 "Performance benchmarks completed"
    else
        print_result 1 "Performance benchmarks failed"
    fi
fi

# Final Results
echo ""
echo "=========================================="
echo -e "${BLUE}üìä Test Results Summary${NC}"
echo "=========================================="
echo -e "Total Tests: ${TESTS_TOTAL}"
echo -e "${GREEN}Passed: ${TESTS_PASSED}${NC}"
if [ $TESTS_FAILED -gt 0 ]; then
    echo -e "${RED}Failed: ${TESTS_FAILED}${NC}"
fi

if [ $TESTS_FAILED -eq 0 ]; then
    echo ""
    echo -e "${GREEN}üéâ ALL TESTS PASSED!${NC}"
    echo "=========================================="
    echo "Your Apache Sedona environment is fully functional!"
    echo ""
    echo "üöÄ Quick Start Commands:"
    echo "   ‚Ä¢ Open Jupyter: http://localhost:8888 (password: sedona123)"
    echo "   ‚Ä¢ Spark Master UI: http://localhost:8080"
    echo "   ‚Ä¢ Run advanced examples: notebooks/advanced-sedona-examples.ipynb"
    echo ""
    echo "‚ú® Features Verified:"
    echo "   ‚úÖ Apache Spark 3.4.0 + Apache Sedona 1.4.1"
    echo "   ‚úÖ 100+ spatial functions (ST_Distance, ST_Area, ST_Buffer, etc.)"
    echo "   ‚úÖ Spatial joins and advanced geometric operations"
    echo "   ‚úÖ Python data science stack (pandas, numpy, matplotlib, seaborn)"
    echo "   ‚úÖ Interactive visualization (plotly, folium, geopandas)"
    echo "   ‚úÖ Machine learning integration (scikit-learn, scipy)"
    echo "   ‚úÖ Jupyter Notebook/Lab environment"
    exit 0
else
    echo ""
    echo -e "${RED}‚ùå SOME TESTS FAILED${NC}"
    echo "Please review the errors above and check your Docker setup."
    exit 1
fi