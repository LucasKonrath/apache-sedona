{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb94832",
   "metadata": {},
   "source": [
    "# Apache Sedona Example Notebook\n",
    "\n",
    "This notebook demonstrates basic Apache Sedona functionality for spatial data processing.\n",
    "\n",
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061f4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da8d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-shaded-3.0_2.12 added as a dependency\n",
      "org.apache.sedona#sedona-viz-3.0_2.12 added as a dependency\n",
      "org.apache.sedona#sedona-python-adapter-3.0_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-89ac2e09-6cb1-4739-9f89-736191ab9ff7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-shaded-3.0_2.12;1.4.1 in central\n",
      "\tfound org.apache.sedona#sedona-viz-3.0_2.12;1.4.1 in central\n",
      "\tfound org.apache.sedona#sedona-common;1.4.1 in central\n",
      "\tfound org.locationtech.jts#jts-core;1.19.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound com.google.geometry#s2-geometry;2.0.0 in central\n",
      "\tfound com.google.guava#guava;25.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.0.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      "\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n",
      "\tfound org.apache.sedona#sedona-core-3.0_2.12;1.4.1 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound org.apache.sedona#sedona-sql-3.0_2.12;1.4.1 in central\n",
      "\tfound org.apache.sedona#sedona-sql-common-3.0_2.12;1.4.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.apache.sedona#sedona-python-adapter-3.0_2.12;1.4.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.0_2.12/1.4.1/sedona-spark-shaded-3.0_2.12-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-spark-shaded-3.0_2.12;1.4.1!sedona-spark-shaded-3.0_2.12.jar (446ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-viz-3.0_2.12/1.4.1/sedona-viz-3.0_2.12-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-viz-3.0_2.12;1.4.1!sedona-viz-3.0_2.12.jar (100ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-python-adapter-3.0_2.12/1.4.1/sedona-python-adapter-3.0_2.12-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-python-adapter-3.0_2.12;1.4.1!sedona-python-adapter-3.0_2.12.jar (88ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-common/1.4.1/sedona-common-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-common;1.4.1!sedona-common.jar (755ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-core-3.0_2.12/1.4.1/sedona-core-3.0_2.12-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-core-3.0_2.12;1.4.1!sedona-core-3.0_2.12.jar (924ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-sql-3.0_2.12/1.4.1/sedona-sql-3.0_2.12-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-sql-3.0_2.12;1.4.1!sedona-sql-3.0_2.12.jar (1390ms)\n",
      "downloading https://repo1.maven.org/maven2/org/locationtech/jts/jts-core/1.19.0/jts-core-1.19.0.jar ...\n",
      "\t[SUCCESSFUL ] org.locationtech.jts#jts-core;1.19.0!jts-core.jar(bundle) (158ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.5.0/scala-collection-compat_2.12-2.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.5.0!scala-collection-compat_2.12.jar (90ms)\n",
      "downloading https://repo1.maven.org/maven2/org/beryx/awt-color-factory/1.0.0/awt-color-factory-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.beryx#awt-color-factory;1.0.0!awt-color-factory.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wololo/jts2geojson/0.16.1/jts2geojson-0.16.1.jar ...\n",
      "\t[SUCCESSFUL ] org.wololo#jts2geojson;0.16.1!jts2geojson.jar (92ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/geometry/s2-geometry/2.0.0/s2-geometry-2.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.geometry#s2-geometry;2.0.0!s2-geometry.jar (97ms)\n",
      "downloading https://repo1.maven.org/maven2/net/sf/geographiclib/GeographicLib-Java/1.52/GeographicLib-Java-1.52.jar ...\n",
      "\t[SUCCESSFUL ] net.sf.geographiclib#GeographicLib-Java;1.52!GeographicLib-Java.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/guava/25.1-jre/guava-25.1-jre.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#guava;25.1-jre!guava.jar(bundle) (129ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (80ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/2.0.0/checker-qual-2.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;2.0.0!checker-qual.jar (95ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.1.3/error_prone_annotations-2.1.3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.errorprone#error_prone_annotations;2.1.3!error_prone_annotations.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.j2objc#j2objc-annotations;1.1!j2objc-annotations.jar (88ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.mojo#animal-sniffer-annotations;1.14!animal-sniffer-annotations.jar (105ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar ...\n",
      "\t[SUCCESSFUL ] commons-lang#commons-lang;2.6!commons-lang.jar (93ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-sql-common-3.0_2.12/1.4.1/sedona-sql-common-3.0_2.12-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-sql-common-3.0_2.12;1.4.1!sedona-sql-common-3.0_2.12.jar (1244ms)\n",
      ":: resolution report :: resolve 16464ms :: artifacts dl 6248ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 from central in [default]\n",
      "\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;25.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n",
      "\torg.apache.sedona#sedona-common;1.4.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-core-3.0_2.12;1.4.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-python-adapter-3.0_2.12;1.4.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-shaded-3.0_2.12;1.4.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-sql-3.0_2.12;1.4.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-sql-common-3.0_2.12;1.4.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-viz-3.0_2.12;1.4.1 from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.0.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   20  |   20  |   0   ||   20  |   20  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-89ac2e09-6cb1-4739-9f89-736191ab9ff7\n",
      "\tconfs: [default]\n",
      "\t20 artifacts copied, 0 already retrieved (23448kB/26ms)\n",
      "25/10/15 12:03:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/tmp/ipykernel_70/544328490.py:13: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(spark)\n",
      "25/10/15 12:03:41 WARN UdtRegistratorWrapper$: Geotools was not found on the classpath. Raster type will not be registered.\n",
      "/usr/local/lib/python3.9/dist-packages/sedona/register/geo_registrator.py:45: DeprecationWarning: Call to deprecated function register (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  cls.register(spark)\n",
      "25/10/15 12:03:42 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/10/15 12:03:42 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/10/15 12:03:42 WARN UdtRegistratorWrapper$: Geotools was not found on the classpath. Raster type will not be registered.\n",
      "25/10/15 12:03:42 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/10/15 12:03:42 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/10/15 12:03:42 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Spark with Sedona\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SedonaExample\") \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "            'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,'\n",
    "            'org.apache.sedona:sedona-viz-3.0_2.12:1.4.1,'\n",
    "            'org.apache.sedona:sedona-python-adapter-3.0_2.12:1.4.1') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register Sedona functions\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff8063",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "If you encounter `NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException`, this means the GeoTools dependencies are missing. \n",
    "\n",
    "**Solution:**\n",
    "1. Rebuild the Docker image with the updated Dockerfile that includes GeoTools JARs\n",
    "2. Or manually add the required JARs to the Spark classpath\n",
    "\n",
    "**Manual fix (if needed):**\n",
    "```bash\n",
    "# From within the container\n",
    "wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/geotools/gt-referencing/29.2/gt-referencing-29.2.jar\n",
    "wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/geotools/gt-opengis/29.2/gt-opengis-29.2.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f8aa5",
   "metadata": {},
   "source": [
    "## Creating Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c9912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------+\n",
      "|            geometry|value|     name|\n",
      "+--------------------+-----+---------+\n",
      "|POINT (-175.37977...|   63|point_404|\n",
      "|POINT (-146.83043...|   44|point_362|\n",
      "|POINT (-122.93870...|    2|point_952|\n",
      "|POINT (62.962648 ...|   38|point_126|\n",
      "|POINT (-166.16286...|   86|point_100|\n",
      "+--------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample spatial data\n",
    "sample_points = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ST_Point(CAST(RAND() * 360 - 180 AS DECIMAL(10,6)), \n",
    "                 CAST(RAND() * 180 - 90 AS DECIMAL(10,6))) as geometry,\n",
    "        CAST(RAND() * 100 AS INT) as value,\n",
    "        'point_' || CAST(RAND() * 1000 AS INT) as name\n",
    "    FROM range(100)\n",
    "\"\"\")\n",
    "\n",
    "sample_points.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeba0a6",
   "metadata": {},
   "source": [
    "## Spatial Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc4099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points: 100\n",
      "Points within bounding box: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 12:04:05 ERROR Executor: Exception in task 3.0 in stage 13.0 (TID 39)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 23 more\n",
      "25/10/15 12:04:05 WARN TaskSetManager: Lost task 3.0 in stage 13.0 (TID 39) (95e372ac6979 executor driver): java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 23 more\n",
      "\n",
      "25/10/15 12:04:05 ERROR TaskSetManager: Task 3 in stage 13.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 13.0 failed 1 times, most recent failure: Lost task 3.0 in stage 13.0 (TID 39) (95e372ac6979 executor driver): java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal points: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_points\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPoints within bounding box: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiltered_points\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mfiltered_points\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 13.0 failed 1 times, most recent failure: Lost task 3.0 in stage 13.0 (TID 39) (95e372ac6979 executor driver): java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "# Create a polygon for spatial filtering\n",
    "bounding_box = spark.sql(\"\"\"\n",
    "    SELECT ST_PolygonFromEnvelope(-10.0, -10.0, 10.0, 10.0) as bbox\n",
    "\"\"\")\n",
    "\n",
    "# Filter points within the bounding box\n",
    "sample_points.createOrReplaceTempView(\"points\")\n",
    "bounding_box.createOrReplaceTempView(\"bbox\")\n",
    "\n",
    "filtered_points = spark.sql(\"\"\"\n",
    "    SELECT p.*, ST_X(p.geometry) as longitude, ST_Y(p.geometry) as latitude\n",
    "    FROM points p, bbox b\n",
    "    WHERE ST_Within(p.geometry, b.bbox)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Total points: {sample_points.count()}\")\n",
    "print(f\"Points within bounding box: {filtered_points.count()}\")\n",
    "filtered_points.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3051b1",
   "metadata": {},
   "source": [
    "## Spatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0659dcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest point pairs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 12:04:21 ERROR Executor: Exception in task 1.0 in stage 15.0 (TID 55)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 7.0 in stage 15.0 (TID 61)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 13.0 in stage 15.0 (TID 67)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 2.0 in stage 15.0 (TID 56)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 4.0 in stage 15.0 (TID 58)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 8.0 in stage 15.0 (TID 62)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 3.0 in stage 15.0 (TID 57)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 11.0 in stage 15.0 (TID 65)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 54)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 WARN TaskSetManager: Lost task 2.0 in stage 15.0 (TID 56) (95e372ac6979 executor driver): java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 6.0 in stage 15.0 (TID 60)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR TaskSetManager: Task 2 in stage 15.0 failed 1 times; aborting job\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 12.0 in stage 15.0 (TID 66)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 5.0 in stage 15.0 (TID 59)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 9.0 in stage 15.0 (TID 63)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n",
      "25/10/15 12:04:21 ERROR Executor: Exception in task 10.0 in stage 15.0 (TID 64)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 27 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o47.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 15.0 failed 1 times, most recent failure: Lost task 2.0 in stage 15.0 (TID 56) (95e372ac6979 executor driver): java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1109)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1091)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1538)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1525)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:291)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 27 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m      2\u001b[0m distance_analysis \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    SELECT \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m        p1.name as point1,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    LIMIT 10\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosest point pairs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdistance_analysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 15.0 failed 1 times, most recent failure: Lost task 2.0 in stage 15.0 (TID 56) (95e372ac6979 executor driver): java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1109)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1091)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1538)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1525)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:291)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Distance$$anonfun$$lessinit$greater$1.apply(Functions.scala:39)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:285)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredBinaryExpression.eval(NullSafeExpressions.scala:279)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:658)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:40)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1531)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1528)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 27 more\n"
     ]
    }
   ],
   "source": [
    "# Calculate distances between points\n",
    "distance_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p1.name as point1,\n",
    "        p2.name as point2,\n",
    "        ST_Distance(p1.geometry, p2.geometry) as distance\n",
    "    FROM points p1\n",
    "    CROSS JOIN points p2\n",
    "    WHERE p1.name != p2.name\n",
    "    ORDER BY distance\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Closest point pairs:\")\n",
    "distance_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3f692",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b64423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 12:04:27 ERROR Executor: Exception in task 4.0 in stage 17.0 (TID 73)\n",
      "java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 22 more\n",
      "25/10/15 12:04:27 WARN TaskSetManager: Lost task 4.0 in stage 17.0 (TID 73) (95e372ac6979 executor driver): java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 22 more\n",
      "\n",
      "25/10/15 12:04:27 ERROR TaskSetManager: Task 4 in stage 17.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 17.0 failed 1 times, most recent failure: Lost task 4.0 in stage 17.0 (TID 73) (95e372ac6979 executor driver): java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3997)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert to pandas for visualization\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m points_pdf \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_points\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a simple scatter plot\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 17.0 failed 1 times, most recent failure: Lost task 4.0 in stage 17.0 (TID 73) (95e372ac6979 executor driver): java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3997)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.NoClassDefFoundError: org/opengis/referencing/NoSuchAuthorityCodeException\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_X$$anonfun$$lessinit$greater$32.apply(Functions.scala:391)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.evalWithoutSerialization(NullSafeExpressions.scala:247)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredUnaryExpression.eval(NullSafeExpressions.scala:242)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.ClassNotFoundException: org.opengis.referencing.NoSuchAuthorityCodeException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas for visualization\n",
    "points_pdf = filtered_points.toPandas()\n",
    "\n",
    "# Create a simple scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(points_pdf['longitude'], points_pdf['latitude'], \n",
    "           c=points_pdf['value'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Spatial Points Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive map with Folium\n",
    "if len(points_pdf) > 0:\n",
    "    center_lat = points_pdf['latitude'].mean()\n",
    "    center_lon = points_pdf['longitude'].mean()\n",
    "    \n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
    "    \n",
    "    for idx, row in points_pdf.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=5,\n",
    "            popup=f\"Name: {row['name']}<br>Value: {row['value']}\",\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fillColor='red'\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Add bounding box\n",
    "    folium.Rectangle(\n",
    "        bounds=[[-10, -10], [10, 10]],\n",
    "        color='blue',\n",
    "        fill=False,\n",
    "        popup='Bounding Box'\n",
    "    ).add_to(m)\n",
    "    \n",
    "    m\n",
    "else:\n",
    "    print(\"No points found within the bounding box to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487af2a",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49960037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
