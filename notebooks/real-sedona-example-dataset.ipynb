{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0c4d02",
   "metadata": {},
   "source": [
    "# Advanced Apache Sedona Examples\n",
    "\n",
    "This notebook demonstrates complex spatial analytics scenarios using Apache Sedona with a complete data science stack.\n",
    "\n",
    "## üéØ Prerequisites Verified:\n",
    "- ‚úÖ **Apache Spark 3.4.0** with optimized configuration  \n",
    "- ‚úÖ **Apache Sedona 1.4.1** with GeoTools wrapper (fixes FactoryException)\n",
    "- ‚úÖ **Advanced Analytics**: PySpark ML, clustering, and feature engineering\n",
    "- ‚úÖ **Visualization Stack**: Matplotlib, Seaborn, Folium for interactive maps\n",
    "- ‚úÖ **Geospatial Libraries**: GeoPandas, Shapely for geometry operations\n",
    "- ‚úÖ **Data Processing**: Pandas, NumPy for data manipulation\n",
    "\n",
    "## üöÄ Complex Use Cases Covered:\n",
    "1. **Spatial ETL Pipeline** - Processing large spatial datasets (NYC Taxi data simulation)\n",
    "2. **Geofencing & Location Intelligence** - Multi-zone analysis with complex polygons\n",
    "3. **Spatial Clustering** - Grid-based clustering for hotspot detection\n",
    "4. **Route Optimization** - Network analysis and efficiency metrics\n",
    "5. **Heatmap Generation** - Interactive spatial density visualizations\n",
    "6. **Multi-scale Spatial Joins** - Performance optimization techniques\n",
    "7. **Spatial Machine Learning** - Predictive spatial modeling with K-means clustering\n",
    "\n",
    "## üìä Performance Features:\n",
    "- Optimized Spark configuration for spatial operations\n",
    "- Broadcast join strategies for large-scale spatial queries\n",
    "- Caching strategies for repeated spatial computations\n",
    "- Advanced indexing techniques for spatial performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efbff920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PySpark ML components imported\n",
      "‚úÖ Matplotlib and Seaborn imported\n",
      "‚úÖ Folium imported\n",
      "‚úÖ GeoPandas and Shapely imported\n",
      "üéØ All imports completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Advanced imports for complex spatial operations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Import ML components\n",
    "try:\n",
    "    from pyspark.ml.clustering import KMeans\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "    print(\"‚úÖ PySpark ML components imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  PySpark ML import warning: {e}\")\n",
    "\n",
    "# Import Sedona components\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "# Import data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import visualization libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.patches import Polygon as MPLPolygon\n",
    "    print(\"‚úÖ Matplotlib and Seaborn imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Matplotlib/Seaborn import warning: {e}\")\n",
    "\n",
    "try:\n",
    "    import folium\n",
    "    from folium.plugins import HeatMap\n",
    "    print(\"‚úÖ Folium imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Folium import warning: {e}\")\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point, Polygon\n",
    "    print(\"‚úÖ GeoPandas and Shapely imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  GeoPandas/Shapely import warning: {e}\")\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"üéØ All imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e4c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/25 12:48:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/tmp/ipykernel_24/2419152682.py:14: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(spark)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced Sedona environment initialized!\n",
      "Spark Version: 3.4.0\n",
      "Available cores: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sedona/register/geo_registrator.py:45: DeprecationWarning: Call to deprecated function register (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  cls.register(spark)\n",
      "25/10/25 12:48:44 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/10/25 12:48:44 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/10/25 12:48:44 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/10/25 12:48:44 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/10/25 12:48:44 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/10/25 12:48:44 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark with optimized configuration for spatial operations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AdvancedSedonaExamples\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "print(\"‚úÖ Advanced Sedona environment initialized!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95eec27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing package integration...\n",
      "‚úÖ Spark DataFrame: 1 rows\n",
      "‚úÖ Sedona functions: distance = 5.0\n",
      "‚úÖ Matplotlib working\n",
      "üéØ All integrations verified!\n"
     ]
    }
   ],
   "source": [
    "# Quick verification that all packages work together\n",
    "print(\"üß™ Testing package integration...\")\n",
    "\n",
    "# Test basic Spark functionality\n",
    "try:\n",
    "    test_df = spark.createDataFrame([(1, \"test\")], [\"id\", \"value\"])\n",
    "    print(f\"‚úÖ Spark DataFrame: {test_df.count()} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Spark test failed: {e}\")\n",
    "\n",
    "# Test Sedona spatial functions\n",
    "try:\n",
    "    test_spatial = spark.sql(\"SELECT ST_Point(1.0, 2.0) as point, ST_Distance(ST_Point(0.0, 0.0), ST_Point(3.0, 4.0)) as distance\")\n",
    "    result = test_spatial.collect()[0]\n",
    "    print(f\"‚úÖ Sedona functions: distance = {result['distance']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Sedona test failed: {e}\")\n",
    "\n",
    "# Test visualization libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(2, 2))\n",
    "    ax.plot([1, 2], [1, 2])\n",
    "    plt.close(fig)\n",
    "    print(\"‚úÖ Matplotlib working\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Matplotlib test failed: {e}\")\n",
    "\n",
    "print(\"üéØ All integrations verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbb0205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Setting up all prerequisites for advanced spatial analytics...\n",
      "This may take 2-3 minutes for the complete data pipeline...\n",
      "\n",
      "üìä Step 1: Generating NYC taxi trip data...\n",
      "Loading real taxi trips from: ../taxi-trips/taxi_trips/2019_taxi_trips.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:========================================>               (10 + 4) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Error during setup: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_longitude` cannot be resolved. Did you mean one of the following? [`lpep_pickup_datetime`, `payment_type`, `tip_amount`, `trip_type`, `trip_distance`].;\n",
      "'Project [lpep_pickup_datetime#111 AS pickup_datetime#148, 'pickup_longitude AS pickup_lon#149, 'pickup_latitude AS pickup_lat#150, 'dropoff_longitude AS dropoff_lon#151, 'dropoff_latitude AS dropoff_lat#152, fare_amount#119, trip_distance#118, passenger_count#117]\n",
      "+- Relation [VendorID#110,lpep_pickup_datetime#111,lpep_dropoff_datetime#112,store_and_fwd_flag#113,RatecodeID#114,PULocationID#115,DOLocationID#116,passenger_count#117,trip_distance#118,fare_amount#119,extra#120,mta_tax#121,tip_amount#122,tolls_amount#123,improvement_surcharge#124,total_amount#125,payment_type#126,trip_type#127,congestion_surcharge#128] csv\n",
      "\n",
      "Please check the error and try running individual cells.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                              \n",
      "  File \"/tmp/ipykernel_24/2985244721.py\", line 75, in <module>\n",
      "    taxi_df = raw_taxi_df.select(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py\", line 3036, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_longitude` cannot be resolved. Did you mean one of the following? [`lpep_pickup_datetime`, `payment_type`, `tip_amount`, `trip_type`, `trip_distance`].;\n",
      "'Project [lpep_pickup_datetime#111 AS pickup_datetime#148, 'pickup_longitude AS pickup_lon#149, 'pickup_latitude AS pickup_lat#150, 'dropoff_longitude AS dropoff_lon#151, 'dropoff_latitude AS dropoff_lat#152, fare_amount#119, trip_distance#118, passenger_count#117]\n",
      "+- Relation [VendorID#110,lpep_pickup_datetime#111,lpep_dropoff_datetime#112,store_and_fwd_flag#113,RatecodeID#114,PULocationID#115,DOLocationID#116,passenger_count#117,trip_distance#118,fare_amount#119,extra#120,mta_tax#121,tip_amount#122,tolls_amount#123,improvement_surcharge#124,total_amount#125,payment_type#126,trip_type#127,congestion_surcharge#128] csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üöÄ QUICK START: Run All Prerequisites\n",
    "# Execute this cell to set up all required data for the advanced examples\n",
    "\n",
    "print(\"üöÄ Setting up all prerequisites for advanced spatial analytics...\")\n",
    "print(\"This may take 2-3 minutes for the complete data pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Step 1: Generate taxi data if not exists\n",
    "    if 'taxi_df' not in locals() or taxi_df is None:\n",
    "        print(\"\\nüìä Step 1: Generating NYC taxi trip data...\")\n",
    "        \n",
    "        def generate_nyc_taxi_data(num_trips=50000):\n",
    "            # NYC bounding box (approximate)\n",
    "            nyc_bounds = {\n",
    "                'min_lat': 40.4774, 'max_lat': 40.9176,\n",
    "                'min_lon': -74.2591, 'max_lon': -73.7004\n",
    "            }\n",
    "            \n",
    "            trips = []\n",
    "            base_time = datetime(2024, 1, 1)\n",
    "            \n",
    "            for i in range(num_trips):\n",
    "                # Pickup location (slightly clustered around Manhattan)\n",
    "                pickup_lat = np.random.normal(40.7589, 0.05)\n",
    "                pickup_lon = np.random.normal(-73.9851, 0.05)\n",
    "                \n",
    "                # Dropoff location (random within NYC)\n",
    "                dropoff_lat = np.random.uniform(nyc_bounds['min_lat'], nyc_bounds['max_lat'])\n",
    "                dropoff_lon = np.random.uniform(nyc_bounds['min_lon'], nyc_bounds['max_lon'])\n",
    "                \n",
    "                trip_time = base_time + timedelta(minutes=np.random.randint(0, 525600))\n",
    "                fare = np.random.uniform(5.0, 50.0)\n",
    "                distance = np.random.uniform(0.1, 20.0)\n",
    "                \n",
    "                trips.append({\n",
    "                    'trip_id': f'trip_{i:06d}',\n",
    "                    'pickup_datetime': trip_time.isoformat(),\n",
    "                    'pickup_lat': pickup_lat,\n",
    "                    'pickup_lon': pickup_lon,\n",
    "                    'dropoff_lat': dropoff_lat,\n",
    "                    'dropoff_lon': dropoff_lon,\n",
    "                    'fare_amount': fare,\n",
    "                    'trip_distance': distance,\n",
    "                    'passenger_count': np.random.randint(1, 7)\n",
    "                })\n",
    "            \n",
    "            return trips\n",
    "\n",
    "        taxi_data = generate_nyc_taxi_data(25000)  # Reduced for faster execution\n",
    "        \n",
    "        taxi_schema = StructType([\n",
    "            StructField(\"trip_id\", StringType(), True),\n",
    "            StructField(\"pickup_datetime\", StringType(), True),\n",
    "            StructField(\"pickup_lat\", DoubleType(), True),\n",
    "            StructField(\"pickup_lon\", DoubleType(), True),\n",
    "            StructField(\"dropoff_lat\", DoubleType(), True),\n",
    "            StructField(\"dropoff_lon\", DoubleType(), True),\n",
    "            StructField(\"fare_amount\", DoubleType(), True),\n",
    "            StructField(\"trip_distance\", DoubleType(), True),\n",
    "            StructField(\"passenger_count\", IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "        # --- Load Real Taxi Trip Data ---\n",
    "        # Path to the real taxi trip data CSV\n",
    "        # Using a sample for faster processing. Update the path to use other files.\n",
    "        taxi_trips_path = \"../taxi-trips/taxi_trips/2019_taxi_trips.csv\" \n",
    "\n",
    "        print(f\"Loading real taxi trips from: {taxi_trips_path}\")\n",
    "\n",
    "        # Load the data, inferring schema and using the header\n",
    "        raw_taxi_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(taxi_trips_path)\n",
    "\n",
    "        # Rename columns to match the notebook's expected schema and filter bad data\n",
    "        # The original notebook expects: pickup_datetime, pickup_lon, pickup_lat, etc.\n",
    "        taxi_df = raw_taxi_df.select(\n",
    "            col(\"lpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "            col(\"pickup_longitude\").alias(\"pickup_lon\"),\n",
    "            col(\"pickup_latitude\").alias(\"pickup_lat\"),\n",
    "            col(\"dropoff_longitude\").alias(\"dropoff_lon\"),\n",
    "            col(\"dropoff_latitude\").alias(\"dropoff_lat\"),\n",
    "            col(\"fare_amount\"),\n",
    "            col(\"trip_distance\"),\n",
    "            col(\"passenger_count\")\n",
    "        ).withColumn(\"trip_id\", monotonically_increasing_id()) \\\n",
    "        .filter(\"pickup_lon IS NOT NULL AND pickup_lat IS NOT NULL AND dropoff_lon IS NOT NULL AND dropoff_lat IS NOT NULL\")\n",
    "\n",
    "        taxi_df.createOrReplaceTempView(\"taxi_trips\")\n",
    "        print(f\"‚úÖ Loaded and prepared {taxi_df.count()} real taxi trips.\")\n",
    "        taxi_df.show(5)\n",
    "        print(f\"    ‚úÖ Generated {taxi_df.count()} taxi trips\")\n",
    "    else:\n",
    "        print(\"    ‚úÖ Taxi data already available\")\n",
    "\n",
    "    # Step 2: Create spatial trips\n",
    "    if 'spatial_trips' not in locals() or spatial_trips is None:\n",
    "        print(\"\\nüó∫Ô∏è  Step 2: Processing spatial operations...\")\n",
    "        \n",
    "        spatial_trips = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                trip_id,\n",
    "                pickup_datetime,\n",
    "                ST_Point(pickup_lon, pickup_lat) as pickup_point,\n",
    "                ST_Point(dropoff_lon, dropoff_lat) as dropoff_point,\n",
    "                ST_Distance(ST_Point(pickup_lon, pickup_lat), ST_Point(dropoff_lon, dropoff_lat)) as euclidean_distance,\n",
    "                fare_amount,\n",
    "                trip_distance,\n",
    "                passenger_count,\n",
    "                CASE \n",
    "                    WHEN HOUR(pickup_datetime) BETWEEN 7 AND 9 THEN 'Morning Rush'\n",
    "                    WHEN HOUR(pickup_datetime) BETWEEN 17 AND 19 THEN 'Evening Rush'\n",
    "                    WHEN HOUR(pickup_datetime) BETWEEN 22 AND 5 THEN 'Night'\n",
    "                    ELSE 'Regular'\n",
    "                END as time_period\n",
    "            FROM taxi_trips\n",
    "            WHERE pickup_lat BETWEEN 40.4 AND 41.0 \n",
    "              AND pickup_lon BETWEEN -74.5 AND -73.5\n",
    "              AND dropoff_lat BETWEEN 40.4 AND 41.0 \n",
    "              AND dropoff_lon BETWEEN -74.5 AND -73.5\n",
    "        \"\"\")\n",
    "        \n",
    "        spatial_trips.cache()\n",
    "        print(f\"    ‚úÖ Created {spatial_trips.count()} spatial trips\")\n",
    "    else:\n",
    "        print(\"    ‚úÖ Spatial trips already available\")\n",
    "\n",
    "    # Step 3: Create zones\n",
    "    if 'spatial_zones' not in locals() or spatial_zones is None:\n",
    "        print(\"\\nüèôÔ∏è  Step 3: Creating NYC zones...\")\n",
    "        \n",
    "        zones_data = [\n",
    "            {\n",
    "                'zone_id': 'manhattan_south', \n",
    "                'zone_name': 'Lower Manhattan',\n",
    "                'polygon': 'POLYGON((-74.0479 40.6829, -73.9441 40.6829, -73.9441 40.7589, -74.0479 40.7589, -74.0479 40.6829))'\n",
    "            },\n",
    "            {\n",
    "                'zone_id': 'manhattan_central', \n",
    "                'zone_name': 'Midtown Manhattan',\n",
    "                'polygon': 'POLYGON((-74.0479 40.7589, -73.9441 40.7589, -73.9441 40.8176, -74.0479 40.8176, -74.0479 40.7589))'\n",
    "            },\n",
    "            {\n",
    "                'zone_id': 'brooklyn_west', \n",
    "                'zone_name': 'West Brooklyn',\n",
    "                'polygon': 'POLYGON((-74.0479 40.6000, -73.9000 40.6000, -73.9000 40.7000, -74.0479 40.7000, -74.0479 40.6000))'\n",
    "            },\n",
    "            {\n",
    "                'zone_id': 'queens_central', \n",
    "                'zone_name': 'Central Queens',\n",
    "                'polygon': 'POLYGON((-73.9000 40.7000, -73.7500 40.7000, -73.7500 40.8000, -73.9000 40.8000, -73.9000 40.7000))'\n",
    "            }\n",
    "        ]\n",
    "\n",
    "            # --- Load Real Taxi Zone Data ---\n",
    "        from sedona.read import ShapefileReader\n",
    "\n",
    "        # Path to the taxi zones shapefile\n",
    "        taxi_zones_path = \"../taxi-trips/taxi_zones_map/taxi_zones_map_shapefiles/\"\n",
    "\n",
    "        print(f\"Loading taxi zones from shapefile: {taxi_zones_path}\")\n",
    "\n",
    "        # Use Sedona's ShapefileReader to load the zone geometries\n",
    "        zones_gdf = ShapefileReader.readToGeometryDF(spark.sparkContext, taxi_zones_path)\n",
    "\n",
    "        # Convert to a Spark DataFrame and prepare for use\n",
    "        spatial_zones = zones_gdf.select(\n",
    "            col(\"LocationID\").alias(\"zone_id\"),\n",
    "            col(\"zone\").alias(\"zone_name\"),\n",
    "            col(\"geometry\").alias(\"zone_geometry\")\n",
    "        ).withColumn(\"zone_area\", ST_Area(col(\"zone_geometry\")))\n",
    "\n",
    "        spatial_zones.cache()\n",
    "        spatial_zones.createOrReplaceTempView(\"spatial_zones\")\n",
    "        spatial_zones.show(5)\n",
    "        print(f\"    ‚úÖ Created {spatial_zones.count()} spatial zones\")\n",
    "    else:\n",
    "        print(\"    ‚úÖ Spatial zones already available\")\n",
    "\n",
    "    print(\"\\nüéâ All prerequisites completed successfully!\")\n",
    "    print(\"üìù You can now run any analysis cell in this notebook.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during setup: {e}\")\n",
    "    print(\"Please check the error and try running individual cells.\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7bd39",
   "metadata": {},
   "source": [
    "## 1. Spatial ETL Pipeline: Processing NYC Taxi Data\n",
    "\n",
    "Simulating processing of millions of taxi trips with spatial operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate large-scale taxi trip data (simulating 1M+ trips)\n",
    "def generate_nyc_taxi_data(num_trips=100000):\n",
    "    # NYC bounding box (approximate)\n",
    "    nyc_bounds = {\n",
    "        'min_lat': 40.4774, 'max_lat': 40.9176,\n",
    "        'min_lon': -74.2591, 'max_lon': -73.7004\n",
    "    }\n",
    "    \n",
    "    # Generate trip data\n",
    "    trips = []\n",
    "    base_time = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(num_trips):\n",
    "        # Pickup location (slightly clustered around Manhattan)\n",
    "        pickup_lat = np.random.normal(40.7589, 0.05)  # Centered on Manhattan\n",
    "        pickup_lon = np.random.normal(-73.9851, 0.05)\n",
    "        \n",
    "        # Dropoff location (random within NYC)\n",
    "        dropoff_lat = np.random.uniform(nyc_bounds['min_lat'], nyc_bounds['max_lat'])\n",
    "        dropoff_lon = np.random.uniform(nyc_bounds['min_lon'], nyc_bounds['max_lon'])\n",
    "        \n",
    "        # Trip details\n",
    "        trip_time = base_time + timedelta(minutes=np.random.randint(0, 525600))  # Random time in year\n",
    "        fare = np.random.uniform(5.0, 50.0)\n",
    "        distance = np.random.uniform(0.1, 20.0)\n",
    "        \n",
    "        trips.append({\n",
    "            'trip_id': f'trip_{i:06d}',\n",
    "            'pickup_datetime': trip_time.isoformat(),\n",
    "            'pickup_lat': pickup_lat,\n",
    "            'pickup_lon': pickup_lon,\n",
    "            'dropoff_lat': dropoff_lat,\n",
    "            'dropoff_lon': dropoff_lon,\n",
    "            'fare_amount': fare,\n",
    "            'trip_distance': distance,\n",
    "            'passenger_count': np.random.randint(1, 7)\n",
    "        })\n",
    "    \n",
    "    return trips\n",
    "\n",
    "print(\"Generating NYC taxi trip data...\")\n",
    "taxi_data = generate_nyc_taxi_data(50000)  # 50K trips for demo\n",
    "print(f\"Generated {len(taxi_data)} taxi trips\")\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "taxi_schema = StructType([\n",
    "    StructField(\"trip_id\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", StringType(), True),\n",
    "    StructField(\"pickup_lat\", DoubleType(), True),\n",
    "    StructField(\"pickup_lon\", DoubleType(), True),\n",
    "    StructField(\"dropoff_lat\", DoubleType(), True),\n",
    "    StructField(\"dropoff_lon\", DoubleType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "taxi_df = spark.createDataFrame(taxi_data, schema=taxi_schema)\n",
    "print(f\"Created Spark DataFrame with {taxi_df.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex Spatial ETL Operations\n",
    "taxi_df.createOrReplaceTempView(\"taxi_trips\")\n",
    "\n",
    "# 1. Create spatial geometries and calculate trip vectors\n",
    "spatial_trips = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        trip_id,\n",
    "        pickup_datetime,\n",
    "        ST_Point(pickup_lon, pickup_lat) as pickup_point,\n",
    "        ST_Point(dropoff_lon, dropoff_lat) as dropoff_point,\n",
    "        ST_Distance(ST_Point(pickup_lon, pickup_lat), ST_Point(dropoff_lon, dropoff_lat)) as euclidean_distance,\n",
    "        fare_amount,\n",
    "        trip_distance,\n",
    "        passenger_count,\n",
    "        CASE \n",
    "            WHEN HOUR(pickup_datetime) BETWEEN 7 AND 9 THEN 'Morning Rush'\n",
    "            WHEN HOUR(pickup_datetime) BETWEEN 17 AND 19 THEN 'Evening Rush'\n",
    "            WHEN HOUR(pickup_datetime) BETWEEN 22 AND 5 THEN 'Night'\n",
    "            ELSE 'Regular'\n",
    "        END as time_period\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_lat BETWEEN 40.4 AND 41.0 \n",
    "      AND pickup_lon BETWEEN -74.5 AND -73.5\n",
    "      AND dropoff_lat BETWEEN 40.4 AND 41.0 \n",
    "      AND dropoff_lon BETWEEN -74.5 AND -73.5\n",
    "\"\"\")\n",
    "\n",
    "spatial_trips.cache()\n",
    "print(f\"Processed {spatial_trips.count()} valid spatial trips\")\n",
    "spatial_trips.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline Validation\n",
    "print(\"üîç Validating data pipeline...\")\n",
    "\n",
    "# Check if required DataFrames/tables exist\n",
    "required_tables = ['taxi_trips', 'spatial_trips', 'spatial_zones']\n",
    "missing_tables = []\n",
    "\n",
    "for table in required_tables:\n",
    "    try:\n",
    "        # Check if table exists by trying to get its count\n",
    "        if table == 'taxi_trips':\n",
    "            count = taxi_df.count() if 'taxi_df' in locals() else 0\n",
    "        elif table == 'spatial_trips':  \n",
    "            count = spatial_trips.count() if 'spatial_trips' in locals() else 0\n",
    "        elif table == 'spatial_zones':\n",
    "            count = spatial_zones.count() if 'spatial_zones' in locals() else 0\n",
    "        \n",
    "        if count > 0:\n",
    "            print(f\"‚úÖ {table}: {count} records\")\n",
    "        else:\n",
    "            missing_tables.append(table)\n",
    "            print(f\"‚ùå {table}: Missing or empty\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        missing_tables.append(table)\n",
    "        print(f\"‚ùå {table}: Not found - {str(e)[:50]}\")\n",
    "\n",
    "if missing_tables:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing tables: {missing_tables}\")\n",
    "    print(\"üìù Please run the previous cells in order to create the required data!\")\n",
    "    print(\"   1. Run taxi data generation cell (creates taxi_df)\")\n",
    "    print(\"   2. Run spatial processing cell (creates spatial_trips)\")  \n",
    "    print(\"   3. Run zone creation cell (creates spatial_zones)\")\n",
    "else:\n",
    "    print(\"\\nüéØ All required data available - ready for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d603dc",
   "metadata": {},
   "source": [
    "## 2. Advanced Geofencing: Multi-Zone Analysis\n",
    "\n",
    "Creating complex geofences and analyzing spatial patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NYC borough-like zones using polygons\n",
    "zones_data = [\n",
    "    {\n",
    "        'zone_id': 'manhattan_south', \n",
    "        'zone_name': 'Lower Manhattan',\n",
    "        'polygon': 'POLYGON((-74.0479 40.6829, -73.9441 40.6829, -73.9441 40.7589, -74.0479 40.7589, -74.0479 40.6829))'\n",
    "    },\n",
    "    {\n",
    "        'zone_id': 'manhattan_central', \n",
    "        'zone_name': 'Midtown Manhattan',\n",
    "        'polygon': 'POLYGON((-74.0479 40.7589, -73.9441 40.7589, -73.9441 40.8176, -74.0479 40.8176, -74.0479 40.7589))'\n",
    "    },\n",
    "    {\n",
    "        'zone_id': 'brooklyn_west', \n",
    "        'zone_name': 'West Brooklyn',\n",
    "        'polygon': 'POLYGON((-74.0479 40.6000, -73.9000 40.6000, -73.9000 40.7000, -74.0479 40.7000, -74.0479 40.6000))'\n",
    "    },\n",
    "    {\n",
    "        'zone_id': 'queens_central', \n",
    "        'zone_name': 'Central Queens',\n",
    "        'polygon': 'POLYGON((-73.9000 40.7000, -73.7500 40.7000, -73.7500 40.8000, -73.9000 40.8000, -73.9000 40.7000))'\n",
    "    }\n",
    "]\n",
    "\n",
    "zones_df = spark.createDataFrame(zones_data)\n",
    "zones_df.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "# Create spatial zones\n",
    "spatial_zones = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        zone_id,\n",
    "        zone_name,\n",
    "        ST_GeomFromWKT(polygon) as zone_geometry,\n",
    "        ST_Area(ST_GeomFromWKT(polygon)) as zone_area\n",
    "    FROM zones\n",
    "\"\"\")\n",
    "\n",
    "spatial_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1310287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex spatial join: Assign pickup and dropoff zones\n",
    "# SMART EXECUTION: This cell will create required data if missing!\n",
    "\n",
    "print(\"üîç Checking for required data...\")\n",
    "\n",
    "# Check if spatial_trips exists, if not run the Quick Start setup\n",
    "if 'spatial_trips' not in locals() or spatial_trips is None:\n",
    "    print(\"‚ö†Ô∏è  spatial_trips not found. Running quick setup...\")\n",
    "    \n",
    "    # Generate basic taxi data if needed\n",
    "    if 'taxi_df' not in locals() or taxi_df is None:\n",
    "        print(\"üìä Generating taxi data...\")\n",
    "        from datetime import datetime, timedelta\n",
    "        \n",
    "        taxi_data = []\n",
    "        base_time = datetime(2024, 1, 1)\n",
    "        \n",
    "        for i in range(10000):  # Smaller dataset for faster execution\n",
    "            pickup_lat = np.random.normal(40.7589, 0.05)\n",
    "            pickup_lon = np.random.normal(-73.9851, 0.05)\n",
    "            dropoff_lat = np.random.uniform(40.6, 40.9)\n",
    "            dropoff_lon = np.random.uniform(-74.1, -73.7)\n",
    "            \n",
    "            taxi_data.append({\n",
    "                'trip_id': f'trip_{i:06d}',\n",
    "                'pickup_datetime': (base_time + timedelta(minutes=np.random.randint(0, 1440))).isoformat(),\n",
    "                'pickup_lat': pickup_lat,\n",
    "                'pickup_lon': pickup_lon,\n",
    "                'dropoff_lat': dropoff_lat,\n",
    "                'dropoff_lon': dropoff_lon,\n",
    "                'fare_amount': np.random.uniform(5.0, 50.0),\n",
    "                'trip_distance': np.random.uniform(0.1, 20.0),\n",
    "                'passenger_count': np.random.randint(1, 7)\n",
    "            })\n",
    "        \n",
    "        taxi_schema = StructType([\n",
    "            StructField(\"trip_id\", StringType(), True),\n",
    "            StructField(\"pickup_datetime\", StringType(), True),\n",
    "            StructField(\"pickup_lat\", DoubleType(), True),\n",
    "            StructField(\"pickup_lon\", DoubleType(), True),\n",
    "            StructField(\"dropoff_lat\", DoubleType(), True),\n",
    "            StructField(\"dropoff_lon\", DoubleType(), True),\n",
    "            StructField(\"fare_amount\", DoubleType(), True),\n",
    "            StructField(\"trip_distance\", DoubleType(), True),\n",
    "            StructField(\"passenger_count\", IntegerType(), True)\n",
    "        ])\n",
    "        \n",
    "        taxi_df = spark.createDataFrame(taxi_data, schema=taxi_schema)\n",
    "        taxi_df.createOrReplaceTempView(\"taxi_trips\")\n",
    "        print(f\"    ‚úÖ Created {taxi_df.count()} taxi trips\")\n",
    "    \n",
    "    # Create spatial_trips\n",
    "    spatial_trips = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            trip_id,\n",
    "            pickup_datetime,\n",
    "            ST_Point(pickup_lon, pickup_lat) as pickup_point,\n",
    "            ST_Point(dropoff_lon, dropoff_lat) as dropoff_point,\n",
    "            ST_Distance(ST_Point(pickup_lon, pickup_lat), ST_Point(dropoff_lon, dropoff_lat)) as euclidean_distance,\n",
    "            fare_amount,\n",
    "            trip_distance,\n",
    "            passenger_count,\n",
    "            CASE \n",
    "                WHEN HOUR(pickup_datetime) BETWEEN 7 AND 9 THEN 'Morning Rush'\n",
    "                WHEN HOUR(pickup_datetime) BETWEEN 17 AND 19 THEN 'Evening Rush'\n",
    "                WHEN HOUR(pickup_datetime) BETWEEN 22 AND 5 THEN 'Night'\n",
    "                ELSE 'Regular'\n",
    "            END as time_period\n",
    "        FROM taxi_trips\n",
    "        WHERE pickup_lat BETWEEN 40.4 AND 41.0 \n",
    "          AND pickup_lon BETWEEN -74.5 AND -73.5\n",
    "          AND dropoff_lat BETWEEN 40.4 AND 41.0 \n",
    "          AND dropoff_lon BETWEEN -74.5 AND -73.5\n",
    "    \"\"\")\n",
    "    spatial_trips.cache()\n",
    "    print(f\"    ‚úÖ Created {spatial_trips.count()} spatial trips\")\n",
    "\n",
    "# Check if spatial_zones exists, if not create it\n",
    "if 'spatial_zones' not in locals() or spatial_zones is None:\n",
    "    print(\"üèôÔ∏è  Creating spatial zones...\")\n",
    "    \n",
    "    zones_data = [\n",
    "        {'zone_id': 'manhattan_south', 'zone_name': 'Lower Manhattan',\n",
    "         'polygon': 'POLYGON((-74.0479 40.6829, -73.9441 40.6829, -73.9441 40.7589, -74.0479 40.7589, -74.0479 40.6829))'},\n",
    "        {'zone_id': 'manhattan_central', 'zone_name': 'Midtown Manhattan',\n",
    "         'polygon': 'POLYGON((-74.0479 40.7589, -73.9441 40.7589, -73.9441 40.8176, -74.0479 40.8176, -74.0479 40.7589))'},\n",
    "        {'zone_id': 'brooklyn_west', 'zone_name': 'West Brooklyn',\n",
    "         'polygon': 'POLYGON((-74.0479 40.6000, -73.9000 40.6000, -73.9000 40.7000, -74.0479 40.7000, -74.0479 40.6000))'},\n",
    "        {'zone_id': 'queens_central', 'zone_name': 'Central Queens',\n",
    "         'polygon': 'POLYGON((-73.9000 40.7000, -73.7500 40.7000, -73.7500 40.8000, -73.9000 40.8000, -73.9000 40.7000))'}\n",
    "    ]\n",
    "\n",
    "    zones_df = spark.createDataFrame(zones_data)\n",
    "    zones_df.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "    spatial_zones = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            zone_id,\n",
    "            zone_name,\n",
    "            ST_GeomFromWKT(polygon) as zone_geometry,\n",
    "            ST_Area(ST_GeomFromWKT(polygon)) as zone_area\n",
    "        FROM zones\n",
    "    \"\"\")\n",
    "    spatial_zones.cache()\n",
    "    print(f\"    ‚úÖ Created {spatial_zones.count()} spatial zones\")\n",
    "\n",
    "# IMPORTANT: Create temp views BEFORE using them in SQL queries\n",
    "try:\n",
    "    spatial_trips.createOrReplaceTempView(\"spatial_trips\")\n",
    "    spatial_zones.createOrReplaceTempView(\"spatial_zones\")\n",
    "    print(\"‚úÖ Created temporary views for spatial_trips and spatial_zones\")\n",
    "    \n",
    "    # Now perform the spatial join\n",
    "    trips_with_zones = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            t.trip_id,\n",
    "            t.pickup_datetime,\n",
    "            t.pickup_point,\n",
    "            t.dropoff_point,\n",
    "            t.euclidean_distance,\n",
    "            t.fare_amount,\n",
    "            t.trip_distance,\n",
    "            t.passenger_count,\n",
    "            t.time_period,\n",
    "            pz.zone_id as pickup_zone,\n",
    "            pz.zone_name as pickup_zone_name,\n",
    "            dz.zone_id as dropoff_zone,\n",
    "            dz.zone_name as dropoff_zone_name,\n",
    "            CASE \n",
    "                WHEN pz.zone_id = dz.zone_id THEN 'Intra-zone'\n",
    "                ELSE 'Inter-zone'\n",
    "            END as trip_type\n",
    "        FROM spatial_trips t\n",
    "        LEFT JOIN spatial_zones pz ON ST_Within(t.pickup_point, pz.zone_geometry)\n",
    "        LEFT JOIN spatial_zones dz ON ST_Within(t.dropoff_point, dz.zone_geometry)\n",
    "    \"\"\")\n",
    "    \n",
    "    trips_with_zones.cache()\n",
    "    # CRITICAL: Create temp view for trips_with_zones so other cells can use it\n",
    "    trips_with_zones.createOrReplaceTempView(\"trips_with_zones\")\n",
    "    print(f\"‚úÖ trips_with_zones created with {trips_with_zones.count()} records\")\n",
    "    print(\"‚úÖ Created temporary view 'trips_with_zones' for SQL queries\")\n",
    "\n",
    "    # Now we can run SQL queries on trips_with_zones\n",
    "    zone_analysis = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            pickup_zone_name,\n",
    "            dropoff_zone_name,\n",
    "            trip_type,\n",
    "            time_period,\n",
    "            COUNT(*) as trip_count,\n",
    "            AVG(fare_amount) as avg_fare,\n",
    "            AVG(euclidean_distance) as avg_distance,\n",
    "            SUM(passenger_count) as total_passengers\n",
    "        FROM trips_with_zones\n",
    "        WHERE pickup_zone IS NOT NULL AND dropoff_zone IS NOT NULL\n",
    "        GROUP BY pickup_zone_name, dropoff_zone_name, trip_type, time_period\n",
    "        ORDER BY trip_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"üéØ Zone Analysis Results:\")\n",
    "    zone_analysis.show(20)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating spatial join: {e}\")\n",
    "    print(\"Try running the 'üöÄ QUICK START' cell at the top of the notebook first!\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac75ccd",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è **EXECUTION ORDER IMPORTANT**\n",
    "\n",
    "**If you get `TABLE_OR_VIEW_NOT_FOUND` errors**, it means you need to run cells in order:\n",
    "\n",
    "1. **First**: Run the **\"üöÄ QUICK START: Run All Prerequisites\"** cell above (Cell 5)\n",
    "2. **Then**: You can run any analysis cell below\n",
    "\n",
    "**OR** run these cells in sequence:\n",
    "- Cell 8: Generate taxi data ‚Üí Creates `taxi_df` \n",
    "- Cell 9: Process spatial trips ‚Üí Creates `spatial_trips`\n",
    "- Cell 11: Create zones ‚Üí Creates `spatial_zones`\n",
    "- Cell 12: **This cell** ‚Üí Creates `trips_with_zones`\n",
    "\n",
    "üí° **Tip**: The Quick Start cell does everything at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543b0f2",
   "metadata": {},
   "source": [
    "## üí° **Important Note**\n",
    "\n",
    "**Execute cells in order!** This notebook builds a data pipeline where each cell depends on previous ones:\n",
    "\n",
    "1. **Cell 2-3**: Initialize Spark and verify setup\n",
    "2. **Cell 4-5**: Generate and process taxi trip data ‚Üí Creates `taxi_df` and `spatial_trips`\n",
    "3. **Cell 6**: Create spatial zones ‚Üí Creates `spatial_zones`  \n",
    "4. **Cell 7**: **This cell** ‚Üí Creates `trips_with_zones` (depends on previous cells)\n",
    "5. **Cell 8+**: Analysis cells that use `trips_with_zones`\n",
    "\n",
    "If you get `TABLE_OR_VIEW_NOT_FOUND` errors, **run all previous cells first**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79102e3e",
   "metadata": {},
   "source": [
    "## 3. Spatial Clustering: DBSCAN-like Analysis\n",
    "\n",
    "Finding hotspots and clusters in pickup locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b917b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial clustering using grid-based approach (DBSCAN alternative for big data)\n",
    "\n",
    "# Check if trips_with_zones exists\n",
    "try:\n",
    "    # Test if the table exists by trying to get its schema\n",
    "    spark.table(\"trips_with_zones\").schema\n",
    "    print(\"‚úÖ trips_with_zones table found\")\n",
    "except Exception:\n",
    "    print(\"‚ö†Ô∏è  trips_with_zones table not found. Using spatial_trips instead.\")\n",
    "    print(\"üí° Run the spatial join cell above first for complete analysis!\")\n",
    "    \n",
    "    # Fallback to spatial_trips if available\n",
    "    if 'spatial_trips' in locals() and spatial_trips is not None:\n",
    "        spatial_trips.createOrReplaceTempView(\"trips_with_zones\")\n",
    "        print(\"‚úÖ Using spatial_trips as fallback\")\n",
    "    else:\n",
    "        print(\"‚ùå No spatial data available. Please run previous cells first!\")\n",
    "        raise Exception(\"Required data not available - run the Quick Start cell or previous cells in order\")\n",
    "\n",
    "def create_spatial_grid(table_name=\"trips_with_zones\", grid_size=0.001):  # ~100m grid cells\n",
    "    \"\"\"\n",
    "    Create a spatial grid for clustering analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        grid_df = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                FLOOR(ST_X(pickup_point) / {grid_size}) * {grid_size} as grid_x,\n",
    "                FLOOR(ST_Y(pickup_point) / {grid_size}) * {grid_size} as grid_y,\n",
    "                COUNT(*) as point_count,\n",
    "                AVG(fare_amount) as avg_fare,\n",
    "                time_period,\n",
    "                COLLECT_LIST(trip_id) as trip_ids\n",
    "            FROM {table_name}\n",
    "            WHERE pickup_point IS NOT NULL\n",
    "            GROUP BY grid_x, grid_y, time_period\n",
    "            HAVING point_count >= 3\n",
    "            ORDER BY point_count DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        return grid_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating spatial grid: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create hotspot analysis\n",
    "hotspots = create_spatial_grid()\n",
    "\n",
    "if hotspots is not None:\n",
    "    hotspots.cache()\n",
    "    hotspots.createOrReplaceTempView(\"hotspots\")  # Create view for other cells\n",
    "    print(f\"‚úÖ Identified {hotspots.count()} spatial hotspots\")\n",
    "    hotspots.show(10)\n",
    "else:\n",
    "    print(\"‚ùå Could not create hotspots analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667db870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced hotspot analysis with density metrics\n",
    "hotspots.createOrReplaceTempView(\"hotspots\")\n",
    "\n",
    "hotspot_analysis = spark.sql(\"\"\"\n",
    "    WITH hotspot_stats AS (\n",
    "        SELECT \n",
    "            grid_x,\n",
    "            grid_y,\n",
    "            time_period,\n",
    "            point_count,\n",
    "            avg_fare,\n",
    "            ST_Point(grid_x, grid_y) as grid_center,\n",
    "            -- Calculate local density (points within 500m)\n",
    "            (\n",
    "                SELECT SUM(h2.point_count) \n",
    "                FROM hotspots h2 \n",
    "                WHERE h2.time_period = h1.time_period\n",
    "                  AND ST_Distance(ST_Point(h1.grid_x, h1.grid_y), ST_Point(h2.grid_x, h2.grid_y)) <= 0.005\n",
    "            ) as neighborhood_density\n",
    "        FROM hotspots h1\n",
    "    ),\n",
    "    ranked_hotspots AS (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (PARTITION BY time_period ORDER BY neighborhood_density DESC) as density_rank,\n",
    "            CASE \n",
    "                WHEN neighborhood_density >= 100 THEN 'Super Hotspot'\n",
    "                WHEN neighborhood_density >= 50 THEN 'Major Hotspot'\n",
    "                WHEN neighborhood_density >= 20 THEN 'Minor Hotspot'\n",
    "                ELSE 'Regular Area'\n",
    "            END as hotspot_category\n",
    "        FROM hotspot_stats\n",
    "    )\n",
    "    SELECT \n",
    "        time_period,\n",
    "        hotspot_category,\n",
    "        COUNT(*) as num_areas,\n",
    "        AVG(point_count) as avg_pickups_per_area,\n",
    "        AVG(avg_fare) as avg_fare_in_category,\n",
    "        MAX(neighborhood_density) as max_density\n",
    "    FROM ranked_hotspots\n",
    "    GROUP BY time_period, hotspot_category\n",
    "    ORDER BY time_period, max_density DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Hotspot Analysis by Time Period:\")\n",
    "hotspot_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee5d8c",
   "metadata": {},
   "source": [
    "## 4. Route Optimization & Network Analysis\n",
    "\n",
    "Analyzing optimal routes and identifying inefficient trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ea123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route efficiency analysis\n",
    "trips_with_zones.createOrReplaceTempView(\"trips_analysis\")\n",
    "\n",
    "route_efficiency = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        trip_id,\n",
    "        pickup_zone_name,\n",
    "        dropoff_zone_name,\n",
    "        euclidean_distance,\n",
    "        trip_distance,\n",
    "        fare_amount,\n",
    "        time_period,\n",
    "        -- Calculate efficiency metrics\n",
    "        CASE \n",
    "            WHEN euclidean_distance > 0 THEN trip_distance / euclidean_distance \n",
    "            ELSE NULL \n",
    "        END as detour_ratio,\n",
    "        \n",
    "        CASE \n",
    "            WHEN trip_distance > 0 THEN fare_amount / trip_distance \n",
    "            ELSE NULL \n",
    "        END as fare_per_mile,\n",
    "        \n",
    "        -- Classify trip efficiency\n",
    "        CASE \n",
    "            WHEN trip_distance / euclidean_distance <= 1.2 THEN 'Efficient'\n",
    "            WHEN trip_distance / euclidean_distance <= 1.5 THEN 'Moderate'\n",
    "            ELSE 'Inefficient'\n",
    "        END as route_efficiency\n",
    "    FROM trips_analysis\n",
    "    WHERE euclidean_distance > 0.001  -- Filter out very short trips\n",
    "      AND trip_distance > 0\n",
    "      AND pickup_zone_name IS NOT NULL\n",
    "      AND dropoff_zone_name IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "route_efficiency.cache()\n",
    "print(f\"Route efficiency analysis for {route_efficiency.count()} trips\")\n",
    "route_efficiency.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d578491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced route analysis with corridor identification\n",
    "route_efficiency.createOrReplaceTempView(\"route_efficiency\")\n",
    "\n",
    "corridor_analysis = spark.sql(\"\"\"\n",
    "    WITH route_corridors AS (\n",
    "        SELECT \n",
    "            pickup_zone_name,\n",
    "            dropoff_zone_name,\n",
    "            time_period,\n",
    "            COUNT(*) as trip_volume,\n",
    "            AVG(detour_ratio) as avg_detour,\n",
    "            AVG(fare_per_mile) as avg_fare_per_mile,\n",
    "            AVG(euclidean_distance) as avg_distance,\n",
    "            PERCENTILE_APPROX(detour_ratio, 0.95) as p95_detour,\n",
    "            -- Efficiency score (lower is better)\n",
    "            AVG(detour_ratio) * 100 + (1.0 / AVG(fare_per_mile)) as inefficiency_score\n",
    "        FROM route_efficiency\n",
    "        WHERE pickup_zone_name != dropoff_zone_name  -- Inter-zone trips only\n",
    "        GROUP BY pickup_zone_name, dropoff_zone_name, time_period\n",
    "        HAVING trip_volume >= 5  -- Focus on popular routes\n",
    "    ),\n",
    "    ranked_corridors AS (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (PARTITION BY time_period ORDER BY trip_volume DESC) as volume_rank,\n",
    "            ROW_NUMBER() OVER (PARTITION BY time_period ORDER BY inefficiency_score DESC) as inefficiency_rank\n",
    "        FROM route_corridors\n",
    "    )\n",
    "    SELECT \n",
    "        time_period,\n",
    "        pickup_zone_name,\n",
    "        dropoff_zone_name,\n",
    "        trip_volume,\n",
    "        ROUND(avg_detour, 2) as avg_detour_ratio,\n",
    "        ROUND(avg_fare_per_mile, 2) as avg_fare_per_mile,\n",
    "        ROUND(inefficiency_score, 2) as inefficiency_score,\n",
    "        volume_rank,\n",
    "        inefficiency_rank,\n",
    "        CASE \n",
    "            WHEN inefficiency_rank <= 3 THEN 'Optimization Priority'\n",
    "            WHEN volume_rank <= 5 THEN 'High Volume Corridor'\n",
    "            ELSE 'Regular Route'\n",
    "        END as route_category\n",
    "    FROM ranked_corridors\n",
    "    WHERE volume_rank <= 10 OR inefficiency_rank <= 5\n",
    "    ORDER BY time_period, inefficiency_rank\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top Route Corridors by Time Period:\")\n",
    "corridor_analysis.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f5f92",
   "metadata": {},
   "source": [
    "## 5. Spatial Machine Learning: Demand Prediction\n",
    "\n",
    "Using spatial features for predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features for ML model\n",
    "ml_features = spark.sql(\"\"\"\n",
    "    WITH spatial_features AS (\n",
    "        SELECT \n",
    "            grid_x,\n",
    "            grid_y,\n",
    "            time_period,\n",
    "            point_count as demand,\n",
    "            avg_fare,\n",
    "            -- Spatial features\n",
    "            grid_x * 1000000 as x_scaled,  -- Scale coordinates\n",
    "            grid_y * 1000000 as y_scaled,\n",
    "            \n",
    "            -- Time-based features\n",
    "            CASE time_period \n",
    "                WHEN 'Morning Rush' THEN 1 \n",
    "                WHEN 'Evening Rush' THEN 2\n",
    "                WHEN 'Night' THEN 3\n",
    "                ELSE 0 \n",
    "            END as time_encoded,\n",
    "            \n",
    "            -- Distance from city center (approximate)\n",
    "            SQRT(POWER(grid_x - (-73.9851), 2) + POWER(grid_y - 40.7589, 2)) as distance_from_center\n",
    "        FROM hotspots\n",
    "        WHERE point_count >= 3\n",
    "    )\n",
    "    SELECT *,\n",
    "        -- Categorize demand levels for classification\n",
    "        CASE \n",
    "            WHEN demand >= 20 THEN 2  -- High demand\n",
    "            WHEN demand >= 10 THEN 1  -- Medium demand  \n",
    "            ELSE 0                    -- Low demand\n",
    "        END as demand_category\n",
    "    FROM spatial_features\n",
    "\"\"\")\n",
    "\n",
    "ml_features.cache()\n",
    "print(f\"Created ML features for {ml_features.count()} spatial-temporal points\")\n",
    "ml_features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ML clustering (spatial demand patterns)\n",
    "feature_cols = ['x_scaled', 'y_scaled', 'time_encoded', 'distance_from_center', 'avg_fare']\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "ml_data = assembler.transform(ml_features)\n",
    "\n",
    "# Apply K-means clustering to identify demand patterns\n",
    "kmeans = KMeans(k=5, seed=42, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(ml_data)\n",
    "predictions = model.transform(ml_data)\n",
    "\n",
    "# Analyze clusters\n",
    "predictions.createOrReplaceTempView(\"ml_predictions\")\n",
    "\n",
    "cluster_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cluster,\n",
    "        COUNT(*) as cluster_size,\n",
    "        AVG(demand) as avg_demand,\n",
    "        AVG(avg_fare) as avg_fare_in_cluster,\n",
    "        AVG(distance_from_center) as avg_distance_from_center,\n",
    "        \n",
    "        -- Most common time period in cluster\n",
    "        MODE() WITHIN GROUP (ORDER BY time_period) as dominant_time_period,\n",
    "        \n",
    "        -- Demand characteristics\n",
    "        MIN(demand) as min_demand,\n",
    "        MAX(demand) as max_demand,\n",
    "        STDDEV(demand) as demand_std\n",
    "    FROM ml_predictions\n",
    "    GROUP BY cluster\n",
    "    ORDER BY cluster\n",
    "\"\"\")\n",
    "\n",
    "print(\"Spatial-Temporal Demand Clusters:\")\n",
    "cluster_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474aa6c",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization Techniques\n",
    "\n",
    "Demonstrating advanced Sedona performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4101ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial indexing and partitioning strategies\n",
    "import time\n",
    "\n",
    "def benchmark_spatial_join(df1, df2, join_condition, description):\n",
    "    \"\"\"\n",
    "    Benchmark different spatial join strategies\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result_count = join_condition.count()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"{description}:\")\n",
    "    print(f\"  - Result count: {result_count}\")\n",
    "    print(f\"  - Execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"  - Partitions: {join_condition.rdd.getNumPartitions()}\")\n",
    "    return result_count, end_time - start_time\n",
    "\n",
    "# Create test datasets\n",
    "large_points = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ST_Point(RAND() * 0.1 - 74.0, RAND() * 0.1 + 40.7) as point,\n",
    "        CAST(RAND() * 1000 AS INT) as point_id\n",
    "    FROM range(10000)\n",
    "\"\"\")\n",
    "\n",
    "test_polygons = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ST_Buffer(ST_Point(RAND() * 0.05 - 73.98, RAND() * 0.05 + 40.75), 0.001) as polygon,\n",
    "        CAST(RAND() * 100 AS INT) as poly_id\n",
    "    FROM range(100)\n",
    "\"\"\")\n",
    "\n",
    "large_points.cache()\n",
    "test_polygons.cache()\n",
    "\n",
    "print(f\"Created {large_points.count()} test points and {test_polygons.count()} test polygons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different join strategies\n",
    "large_points.createOrReplaceTempView(\"large_points\")\n",
    "test_polygons.createOrReplaceTempView(\"test_polygons\")\n",
    "\n",
    "# Strategy 1: Basic spatial join\n",
    "basic_join = spark.sql(\"\"\"\n",
    "    SELECT p.point_id, pg.poly_id\n",
    "    FROM large_points p, test_polygons pg\n",
    "    WHERE ST_Within(p.point, pg.polygon)\n",
    "\"\"\")\n",
    "\n",
    "benchmark_spatial_join(large_points, test_polygons, basic_join, \"Basic Spatial Join\")\n",
    "\n",
    "# Strategy 2: With spatial indexing hint\n",
    "indexed_join = spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(pg) */ p.point_id, pg.poly_id\n",
    "    FROM large_points p, test_polygons pg\n",
    "    WHERE ST_Within(p.point, pg.polygon)\n",
    "\"\"\")\n",
    "\n",
    "benchmark_spatial_join(large_points, test_polygons, indexed_join, \"Broadcast Join Strategy\")\n",
    "\n",
    "# Performance tips summary\n",
    "print(\"\\nüöÄ Performance Optimization Tips:\")\n",
    "print(\"1. Use broadcast joins for small polygon datasets (< 200MB)\")\n",
    "print(\"2. Partition data by spatial regions for large datasets\")\n",
    "print(\"3. Cache frequently accessed spatial DataFrames\")\n",
    "print(\"4. Use appropriate spatial predicates (ST_Within vs ST_Intersects)\")\n",
    "print(\"5. Consider spatial indexing for repeated queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238375ad",
   "metadata": {},
   "source": [
    "## 7. Advanced Visualization: Interactive Heatmaps\n",
    "\n",
    "Creating sophisticated spatial visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "viz_data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        grid_x as longitude,\n",
    "        grid_y as latitude,\n",
    "        point_count as intensity,\n",
    "        avg_fare,\n",
    "        time_period\n",
    "    FROM hotspots\n",
    "    WHERE point_count >= 5\n",
    "    ORDER BY point_count DESC\n",
    "    LIMIT 500\n",
    "\"\"\")\n",
    "\n",
    "viz_pandas = viz_data.toPandas()\n",
    "print(f\"Prepared {len(viz_pandas)} points for visualization\")\n",
    "\n",
    "# Create multi-layer interactive map\n",
    "def create_advanced_heatmap(df):\n",
    "    # Center map on NYC\n",
    "    center_lat = df['latitude'].mean()\n",
    "    center_lon = df['longitude'].mean()\n",
    "    \n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon],\n",
    "        zoom_start=12,\n",
    "        tiles='OpenStreetMap'\n",
    "    )\n",
    "    \n",
    "    # Add different layers for different time periods\n",
    "    time_periods = df['time_period'].unique()\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    for i, period in enumerate(time_periods):\n",
    "        period_data = df[df['time_period'] == period]\n",
    "        \n",
    "        # Create heatmap data\n",
    "        heat_data = [[row['latitude'], row['longitude'], row['intensity']] \n",
    "                    for idx, row in period_data.iterrows()]\n",
    "        \n",
    "        if heat_data:  # Only create layer if data exists\n",
    "            HeatMap(\n",
    "                heat_data,\n",
    "                name=f'Demand - {period}',\n",
    "                radius=15,\n",
    "                blur=10,\n",
    "                gradient={0.2: colors[i % len(colors)], 1.0: colors[i % len(colors)]}\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "if len(viz_pandas) > 0:\n",
    "    heatmap = create_advanced_heatmap(viz_pandas)\n",
    "    print(\"‚úÖ Interactive heatmap created! (Display in Jupyter)\")\n",
    "    # heatmap  # Uncomment to display in Jupyter\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced statistical analysis\n",
    "summary_stats = spark.sql(\"\"\"\n",
    "    WITH overall_stats AS (\n",
    "        SELECT \n",
    "            COUNT(DISTINCT trip_id) as total_trips,\n",
    "            COUNT(DISTINCT pickup_zone_name) as unique_pickup_zones,\n",
    "            COUNT(DISTINCT dropoff_zone_name) as unique_dropoff_zones,\n",
    "            AVG(fare_amount) as avg_fare,\n",
    "            AVG(euclidean_distance) as avg_distance,\n",
    "            SUM(passenger_count) as total_passengers\n",
    "        FROM trips_with_zones\n",
    "        WHERE pickup_zone_name IS NOT NULL\n",
    "    ),\n",
    "    efficiency_stats AS (\n",
    "        SELECT \n",
    "            route_efficiency,\n",
    "            COUNT(*) as trip_count,\n",
    "            AVG(detour_ratio) as avg_detour,\n",
    "            AVG(fare_per_mile) as avg_fare_per_mile\n",
    "        FROM route_efficiency\n",
    "        GROUP BY route_efficiency\n",
    "    )\n",
    "    SELECT \n",
    "        'Overall Statistics' as metric_type,\n",
    "        CAST(total_trips AS STRING) as value,\n",
    "        'Total processed trips' as description\n",
    "    FROM overall_stats\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Spatial Coverage' as metric_type,\n",
    "        CAST(unique_pickup_zones AS STRING) as value,\n",
    "        'Unique pickup zones covered' as description\n",
    "    FROM overall_stats\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Route Efficiency' as metric_type,\n",
    "        CONCAT(route_efficiency, ': ', CAST(trip_count AS STRING), ' trips') as value,\n",
    "        CONCAT('Avg detour ratio: ', CAST(ROUND(avg_detour, 2) AS STRING)) as description\n",
    "    FROM efficiency_stats\n",
    "    ORDER BY metric_type, value\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä Advanced Spatial Analytics Summary:\")\n",
    "summary_stats.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"\\nüßπ Cleaning up resources...\")\n",
    "spark.catalog.clearCache()\n",
    "print(\"Cache cleared.\")\n",
    "\n",
    "print(\"\\nüéØ Complex Spatial Analytics Completed!\")\n",
    "print(\"\\nKey Capabilities Demonstrated:\")\n",
    "print(\"‚Ä¢ Large-scale spatial ETL processing\")\n",
    "print(\"‚Ä¢ Multi-zone geofencing analysis\")\n",
    "print(\"‚Ä¢ Spatial clustering and hotspot detection\")\n",
    "print(\"‚Ä¢ Route optimization analysis\")\n",
    "print(\"‚Ä¢ Spatial machine learning integration\")\n",
    "print(\"‚Ä¢ Performance optimization techniques\")\n",
    "print(\"‚Ä¢ Advanced interactive visualizations\")\n",
    "\n",
    "# Uncomment to stop Spark (keep running for interactive use)\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
