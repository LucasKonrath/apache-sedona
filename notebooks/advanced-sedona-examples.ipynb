{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0c4d02",
   "metadata": {},
   "source": [
    "# Advanced Apache Sedona Examples\n",
    "\n",
    "This notebook demonstrates complex spatial analytics scenarios using Apache Sedona.\n",
    "\n",
    "## Complex Use Cases Covered:\n",
    "1. **Spatial ETL Pipeline** - Processing large spatial datasets\n",
    "2. **Geofencing & Location Intelligence** - Real-time location analytics\n",
    "3. **Spatial Clustering** - DBSCAN clustering of spatial points\n",
    "4. **Route Optimization** - Spatial network analysis\n",
    "5. **Heatmap Generation** - Spatial density analysis\n",
    "6. **Multi-scale Spatial Joins** - Performance optimization techniques\n",
    "7. **Spatial Machine Learning** - Predictive spatial modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efbff920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced imports for complex spatial operations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Polygon as MPLPolygon\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e4c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/30 18:27:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/tmp/ipykernel_868/2419152682.py:14: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(spark)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Advanced Sedona environment initialized!\n",
      "Spark Version: 3.4.0\n",
      "Available cores: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sedona/register/geo_registrator.py:45: DeprecationWarning: Call to deprecated function register (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  cls.register(spark)\n",
      "25/10/30 18:27:20 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/10/30 18:27:20 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/10/30 18:27:20 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/10/30 18:27:20 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:20 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:20 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark with optimized configuration for spatial operations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AdvancedSedonaExamples\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "print(\"✅ Advanced Sedona environment initialized!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7bd39",
   "metadata": {},
   "source": [
    "## 1. Spatial ETL Pipeline: Processing NYC Taxi Data\n",
    "\n",
    "Simulating processing of millions of taxi trips with spatial operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7b5645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating NYC taxi trip data...\n",
      "Generated 50000 taxi trips\n",
      "Created Spark DataFrame with 50000 records\n"
     ]
    }
   ],
   "source": [
    "# Generate large-scale taxi trip data (simulating 1M+ trips)\n",
    "def generate_nyc_taxi_data(num_trips=100000):\n",
    "    # NYC bounding box (approximate)\n",
    "    nyc_bounds = {\n",
    "        'min_lat': 40.4774, 'max_lat': 40.9176,\n",
    "        'min_lon': -74.2591, 'max_lon': -73.7004\n",
    "    }\n",
    "    \n",
    "    # Generate trip data\n",
    "    trips = []\n",
    "    base_time = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(num_trips):\n",
    "        # Pickup location (slightly clustered around Manhattan)\n",
    "        pickup_lat = np.random.normal(40.7589, 0.05)  # Centered on Manhattan\n",
    "        pickup_lon = np.random.normal(-73.9851, 0.05)\n",
    "        \n",
    "        # Dropoff location (random within NYC)\n",
    "        dropoff_lat = np.random.uniform(nyc_bounds['min_lat'], nyc_bounds['max_lat'])\n",
    "        dropoff_lon = np.random.uniform(nyc_bounds['min_lon'], nyc_bounds['max_lon'])\n",
    "        \n",
    "        # Trip details\n",
    "        trip_time = base_time + timedelta(minutes=np.random.randint(0, 525600))  # Random time in year\n",
    "        fare = np.random.uniform(5.0, 50.0)\n",
    "        distance = np.random.uniform(0.1, 20.0)\n",
    "        \n",
    "        trips.append({\n",
    "            'trip_id': f'trip_{i:06d}',\n",
    "            'pickup_datetime': trip_time.isoformat(),\n",
    "            'pickup_lat': pickup_lat,\n",
    "            'pickup_lon': pickup_lon,\n",
    "            'dropoff_lat': dropoff_lat,\n",
    "            'dropoff_lon': dropoff_lon,\n",
    "            'fare_amount': fare,\n",
    "            'trip_distance': distance,\n",
    "            'passenger_count': np.random.randint(1, 7)\n",
    "        })\n",
    "    \n",
    "    return trips\n",
    "\n",
    "print(\"Generating NYC taxi trip data...\")\n",
    "taxi_data = generate_nyc_taxi_data(50000)  # 50K trips for demo\n",
    "print(f\"Generated {len(taxi_data)} taxi trips\")\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "taxi_schema = StructType([\n",
    "    StructField(\"trip_id\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", StringType(), True),\n",
    "    StructField(\"pickup_lat\", DoubleType(), True),\n",
    "    StructField(\"pickup_lon\", DoubleType(), True),\n",
    "    StructField(\"dropoff_lat\", DoubleType(), True),\n",
    "    StructField(\"dropoff_lon\", DoubleType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "taxi_df = spark.createDataFrame(taxi_data, schema=taxi_schema)\n",
    "print(f\"Created Spark DataFrame with {taxi_df.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdf9410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/30 18:27:21 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.core.ImageSerializableWrapper, which is already registered.\n",
      "25/10/30 18:27:21 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.utils.Pixel, which is already registered.\n",
      "25/10/30 18:27:21 WARN SimpleFunctionRegistry: The function st_pixelize replaced a previously registered function.\n",
      "25/10/30 18:27:21 WARN SimpleFunctionRegistry: The function st_tilename replaced a previously registered function.\n",
      "25/10/30 18:27:21 WARN SimpleFunctionRegistry: The function st_colorize replaced a previously registered function.\n",
      "25/10/30 18:27:21 WARN SimpleFunctionRegistry: The function st_encodeimage replaced a previously registered function.\n",
      "25/10/30 18:27:21 WARN SimpleFunctionRegistry: The function st_render replaced a previously registered function.\n",
      "25/10/30 18:27:21 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/10/30 18:27:21 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/10/30 18:27:21 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/10/30 18:27:21 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:21 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:21 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50000 valid spatial trips\n",
      "+-----------+-------------------+--------------------+--------------------+--------------------+------------------+------------------+---------------+------------+\n",
      "|    trip_id|    pickup_datetime|        pickup_point|       dropoff_point|  euclidean_distance|       fare_amount|     trip_distance|passenger_count| time_period|\n",
      "+-----------+-------------------+--------------------+--------------------+--------------------+------------------+------------------+---------------+------------+\n",
      "|trip_000000|2024-02-08T02:46:00|POINT (-73.992013...|POINT (-73.924629...| 0.06923145060550419|25.062473878411602|2.0895008247782574|              3|     Regular|\n",
      "|trip_000001|2024-08-11T12:39:00|POINT (-73.934574...|POINT (-74.247599...|  0.3134462732238153| 42.45991883601898| 4.325548302497695|              4|     Regular|\n",
      "|trip_000002|2024-12-13T08:26:00|POINT (-74.031304...|POINT (-74.017772...|0.025771611874429636| 32.53338026250708|2.8759278269756323|              4|Morning Rush|\n",
      "|trip_000003|2024-07-08T22:58:00|POINT (-74.023498...|POINT (-73.709768...|  0.3374360364999179| 43.69731830313442|13.638120017896815|              1|     Regular|\n",
      "|trip_000004|2024-06-27T03:08:00|POINT (-73.999684...|POINT (-74.013186...|0.051531646188783295|  42.4937710281274|3.5499566048046645|              1|     Regular|\n",
      "+-----------+-------------------+--------------------+--------------------+--------------------+------------------+------------------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complex Spatial ETL Operations\n",
    "taxi_df.createOrReplaceTempView(\"taxi_trips\")\n",
    "\n",
    "# 1. Create spatial geometries and calculate trip vectors\n",
    "spatial_trips = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        trip_id,\n",
    "        pickup_datetime,\n",
    "        ST_Point(pickup_lon, pickup_lat) as pickup_point,\n",
    "        ST_Point(dropoff_lon, dropoff_lat) as dropoff_point,\n",
    "        ST_Distance(ST_Point(pickup_lon, pickup_lat), ST_Point(dropoff_lon, dropoff_lat)) as euclidean_distance,\n",
    "        fare_amount,\n",
    "        trip_distance,\n",
    "        passenger_count,\n",
    "        CASE \n",
    "            WHEN HOUR(pickup_datetime) BETWEEN 7 AND 9 THEN 'Morning Rush'\n",
    "            WHEN HOUR(pickup_datetime) BETWEEN 17 AND 19 THEN 'Evening Rush'\n",
    "            WHEN HOUR(pickup_datetime) BETWEEN 22 AND 5 THEN 'Night'\n",
    "            ELSE 'Regular'\n",
    "        END as time_period\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_lat BETWEEN 40.4 AND 41.0 \n",
    "      AND pickup_lon BETWEEN -74.5 AND -73.5\n",
    "      AND dropoff_lat BETWEEN 40.4 AND 41.0 \n",
    "      AND dropoff_lon BETWEEN -74.5 AND -73.5\n",
    "\"\"\")\n",
    "\n",
    "spatial_trips.cache()\n",
    "print(f\"Processed {spatial_trips.count()} valid spatial trips\")\n",
    "spatial_trips.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d603dc",
   "metadata": {},
   "source": [
    "## 2. Advanced Geofencing: Multi-Zone Analysis\n",
    "\n",
    "Creating complex geofences and analyzing spatial patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03d55bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+--------------------+--------------------+\n",
      "|          zone_id|        zone_name|       zone_geometry|           zone_area|\n",
      "+-----------------+-----------------+--------------------+--------------------+\n",
      "|  manhattan_south|  Lower Manhattan|POLYGON ((-74.047...|0.007888799999999488|\n",
      "|manhattan_central|Midtown Manhattan|POLYGON ((-74.047...|0.006093059999999745|\n",
      "|    brooklyn_west|    West Brooklyn|POLYGON ((-74.047...|0.014789999999999491|\n",
      "|   queens_central|   Central Queens|POLYGON ((-73.9 4...|0.014999999999999715|\n",
      "+-----------------+-----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create NYC borough-like zones using polygons\n",
    "zones_data = [\n",
    "    {\n",
    "        'zone_id': 'manhattan_south', \n",
    "        'zone_name': 'Lower Manhattan',\n",
    "        'polygon': 'POLYGON((-74.0479 40.6829, -73.9441 40.6829, -73.9441 40.7589, -74.0479 40.7589, -74.0479 40.6829))'\n",
    "    },\n",
    "    {\n",
    "        'zone_id': 'manhattan_central', \n",
    "        'zone_name': 'Midtown Manhattan',\n",
    "        'polygon': 'POLYGON((-74.0479 40.7589, -73.9441 40.7589, -73.9441 40.8176, -74.0479 40.8176, -74.0479 40.7589))'\n",
    "    },\n",
    "    {\n",
    "        'zone_id': 'brooklyn_west', \n",
    "        'zone_name': 'West Brooklyn',\n",
    "        'polygon': 'POLYGON((-74.0479 40.6000, -73.9000 40.6000, -73.9000 40.7000, -74.0479 40.7000, -74.0479 40.6000))'\n",
    "    },\n",
    "    {\n",
    "        'zone_id': 'queens_central', \n",
    "        'zone_name': 'Central Queens',\n",
    "        'polygon': 'POLYGON((-73.9000 40.7000, -73.7500 40.7000, -73.7500 40.8000, -73.9000 40.8000, -73.9000 40.7000))'\n",
    "    }\n",
    "]\n",
    "\n",
    "zones_df = spark.createDataFrame(zones_data)\n",
    "zones_df.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "# Create spatial zones\n",
    "spatial_zones = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        zone_id,\n",
    "        zone_name,\n",
    "        ST_GeomFromWKT(polygon) as zone_geometry,\n",
    "        ST_Area(ST_GeomFromWKT(polygon)) as zone_area\n",
    "    FROM zones\n",
    "\"\"\")\n",
    "\n",
    "spatial_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1310287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/30 18:27:22 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.core.ImageSerializableWrapper, which is already registered.\n",
      "25/10/30 18:27:22 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.utils.Pixel, which is already registered.\n",
      "25/10/30 18:27:22 WARN SimpleFunctionRegistry: The function st_pixelize replaced a previously registered function.\n",
      "25/10/30 18:27:22 WARN SimpleFunctionRegistry: The function st_tilename replaced a previously registered function.\n",
      "25/10/30 18:27:22 WARN SimpleFunctionRegistry: The function st_colorize replaced a previously registered function.\n",
      "25/10/30 18:27:22 WARN SimpleFunctionRegistry: The function st_encodeimage replaced a previously registered function.\n",
      "25/10/30 18:27:22 WARN SimpleFunctionRegistry: The function st_render replaced a previously registered function.\n",
      "25/10/30 18:27:22 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/10/30 18:27:22 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/10/30 18:27:22 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/10/30 18:27:22 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:22 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:22 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips with zone assignments: 52318\n",
      "+-----------------+-----------------+----------+------------+----------+------------------+--------------------+----------------+\n",
      "| pickup_zone_name|dropoff_zone_name| trip_type| time_period|trip_count|          avg_fare|        avg_distance|total_passengers|\n",
      "+-----------------+-----------------+----------+------------+----------+------------------+--------------------+----------------+\n",
      "|  Lower Manhattan|   Central Queens|Inter-zone|     Regular|       733|27.427335100286825| 0.17449557917840078|            2604|\n",
      "|  Lower Manhattan|    West Brooklyn|Inter-zone|     Regular|       674|28.154813553742706| 0.09357235376383466|            2340|\n",
      "|Midtown Manhattan|    West Brooklyn|Inter-zone|     Regular|       586|27.437387431195283| 0.14627218923280066|            2059|\n",
      "|Midtown Manhattan|   Central Queens|Inter-zone|     Regular|       580|27.882216827449465| 0.17483526991486867|            2079|\n",
      "|  Lower Manhattan|  Lower Manhattan|Intra-zone|     Regular|       362|28.587763382100203| 0.04476665504867566|            1285|\n",
      "|  Lower Manhattan|Midtown Manhattan|Inter-zone|     Regular|       291|27.914554552474236| 0.07526832532797581|            1011|\n",
      "|Midtown Manhattan|  Lower Manhattan|Inter-zone|     Regular|       281| 28.56147521054784| 0.07440556578400272|             998|\n",
      "|    West Brooklyn|   Central Queens|Inter-zone|     Regular|       253|26.850839581335368|  0.1787852010068585|             947|\n",
      "|    West Brooklyn|    West Brooklyn|Intra-zone|     Regular|       215| 28.51463402095667|  0.0575741559372658|             776|\n",
      "|Midtown Manhattan|Midtown Manhattan|Intra-zone|     Regular|       203|27.359951366271677|0.040855000015207026|             704|\n",
      "|  Lower Manhattan|   Central Queens|Inter-zone|Morning Rush|       128|  28.0567994498105| 0.17462966479356348|             458|\n",
      "|    West Brooklyn|  Lower Manhattan|Inter-zone|     Regular|       125|28.471661997167836| 0.06339055315189518|             466|\n",
      "|Midtown Manhattan|   Central Queens|Inter-zone|Evening Rush|       121| 26.87913008987369| 0.16773945778406368|             409|\n",
      "|  Lower Manhattan|   Central Queens|Inter-zone|Evening Rush|       111|27.827842775240804|  0.1746089405549435|             381|\n",
      "|  Lower Manhattan|    West Brooklyn|Inter-zone|Morning Rush|       110| 26.96213779247492| 0.09137155768265638|             391|\n",
      "|Midtown Manhattan|    West Brooklyn|Inter-zone|Morning Rush|       110| 27.89685208989663|   0.149080272098035|             347|\n",
      "|  Lower Manhattan|    West Brooklyn|Inter-zone|Evening Rush|       108|27.517300584282832|  0.0962520411848434|             387|\n",
      "|    West Brooklyn|Midtown Manhattan|Inter-zone|     Regular|       100|28.254184605513924| 0.11728322716728329|             348|\n",
      "|Midtown Manhattan|   Central Queens|Inter-zone|Morning Rush|        98|27.691437201530746| 0.17826806402799802|             330|\n",
      "|Midtown Manhattan|    West Brooklyn|Inter-zone|Evening Rush|        96|26.596078921250008|  0.1448095538047467|             345|\n",
      "+-----------------+-----------------+----------+------------+----------+------------------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complex spatial join: Assign pickup and dropoff zones\n",
    "spatial_trips.createOrReplaceTempView(\"spatial_trips\")\n",
    "spatial_zones.createOrReplaceTempView(\"spatial_zones\")\n",
    "\n",
    "trips_with_zones = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        t.trip_id,\n",
    "        t.pickup_datetime,\n",
    "        t.pickup_point,\n",
    "        t.dropoff_point,\n",
    "        t.euclidean_distance,\n",
    "        t.fare_amount,\n",
    "        t.trip_distance,\n",
    "        t.passenger_count,\n",
    "        t.time_period,\n",
    "        pz.zone_id as pickup_zone,\n",
    "        pz.zone_name as pickup_zone_name,\n",
    "        dz.zone_id as dropoff_zone,\n",
    "        dz.zone_name as dropoff_zone_name,\n",
    "        CASE \n",
    "            WHEN pz.zone_id = dz.zone_id THEN 'Intra-zone'\n",
    "            ELSE 'Inter-zone'\n",
    "        END as trip_type\n",
    "    FROM spatial_trips t\n",
    "    LEFT JOIN spatial_zones pz ON ST_Within(t.pickup_point, pz.zone_geometry)\n",
    "    LEFT JOIN spatial_zones dz ON ST_Within(t.dropoff_point, dz.zone_geometry)\n",
    "\"\"\")\n",
    "\n",
    "trips_with_zones.cache()\n",
    "trips_with_zones.createOrReplaceTempView(\"trips_with_zones\")\n",
    "print(f\"Trips with zone assignments: {trips_with_zones.count()}\")\n",
    "\n",
    "# Analyze zone patterns\n",
    "zone_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pickup_zone_name,\n",
    "        dropoff_zone_name,\n",
    "        trip_type,\n",
    "        time_period,\n",
    "        COUNT(*) as trip_count,\n",
    "        AVG(fare_amount) as avg_fare,\n",
    "        AVG(euclidean_distance) as avg_distance,\n",
    "        SUM(passenger_count) as total_passengers\n",
    "    FROM trips_with_zones\n",
    "    WHERE pickup_zone IS NOT NULL AND dropoff_zone IS NOT NULL\n",
    "    GROUP BY pickup_zone_name, dropoff_zone_name, trip_type, time_period\n",
    "    ORDER BY trip_count DESC\n",
    "\"\"\")\n",
    "\n",
    "zone_analysis.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79102e3e",
   "metadata": {},
   "source": [
    "## 3. Spatial Clustering: DBSCAN-like Analysis\n",
    "\n",
    "Finding hotspots and clusters in pickup locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b917b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/30 18:27:23 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.core.ImageSerializableWrapper, which is already registered.\n",
      "25/10/30 18:27:23 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.utils.Pixel, which is already registered.\n",
      "25/10/30 18:27:23 WARN SimpleFunctionRegistry: The function st_pixelize replaced a previously registered function.\n",
      "25/10/30 18:27:23 WARN SimpleFunctionRegistry: The function st_tilename replaced a previously registered function.\n",
      "25/10/30 18:27:23 WARN SimpleFunctionRegistry: The function st_colorize replaced a previously registered function.\n",
      "25/10/30 18:27:23 WARN SimpleFunctionRegistry: The function st_encodeimage replaced a previously registered function.\n",
      "25/10/30 18:27:23 WARN SimpleFunctionRegistry: The function st_render replaced a previously registered function.\n",
      "25/10/30 18:27:23 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/10/30 18:27:23 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/10/30 18:27:23 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/10/30 18:27:23 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:23 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:23 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 4036 spatial hotspots\n",
      "+-------+------+-----------+------------------+-----------+--------------------+\n",
      "| grid_x|grid_y|point_count|          avg_fare|time_period|            trip_ids|\n",
      "+-------+------+-----------+------------------+-----------+--------------------+\n",
      "|-73.979|40.699|         10| 28.54039689992977|    Regular|[trip_001123, tri...|\n",
      "|-73.958|40.742|          9|30.729177281036957|    Regular|[trip_001315, tri...|\n",
      "|-74.001|40.699|          8|22.227245091971604|    Regular|[trip_004934, tri...|\n",
      "|-73.938|40.744|          8|28.560674990514304|    Regular|[trip_001734, tri...|\n",
      "|-73.956|40.736|          8|23.398327907427685|    Regular|[trip_007442, tri...|\n",
      "|-74.018|40.697|          8| 31.30807819106216|    Regular|[trip_021749, tri...|\n",
      "|-73.968|40.686|          8| 30.86401760066078|    Regular|[trip_022914, tri...|\n",
      "|-74.016|40.753|          8|30.124422570106365|    Regular|[trip_019266, tri...|\n",
      "|-73.998|40.692|          8| 36.54472547899647|    Regular|[trip_001216, tri...|\n",
      "|-74.006|40.692|          8| 32.34237900534062|    Regular|[trip_015173, tri...|\n",
      "+-------+------+-----------+------------------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/spark/jars/spark-core_2.12-3.4.0.jar) to field java.math.BigDecimal.intVal\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "# Spatial clustering using grid-based approach (DBSCAN alternative for big data)\n",
    "def create_spatial_grid(df, grid_size=0.001):  # ~100m grid cells\n",
    "    \"\"\"\n",
    "    Create a spatial grid for clustering analysis\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView(\"points\")\n",
    "    \n",
    "    grid_df = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            FLOOR(ST_X(pickup_point) / {grid_size}) * {grid_size} as grid_x,\n",
    "            FLOOR(ST_Y(pickup_point) / {grid_size}) * {grid_size} as grid_y,\n",
    "            COUNT(*) as point_count,\n",
    "            AVG(fare_amount) as avg_fare,\n",
    "            time_period,\n",
    "            COLLECT_LIST(trip_id) as trip_ids\n",
    "        FROM points\n",
    "        WHERE pickup_point IS NOT NULL\n",
    "        GROUP BY grid_x, grid_y, time_period\n",
    "        HAVING point_count >= 3\n",
    "        ORDER BY point_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    return grid_df\n",
    "\n",
    "# Create hotspot analysis\n",
    "hotspots = create_spatial_grid(trips_with_zones)\n",
    "hotspots.cache()\n",
    "\n",
    "print(f\"Identified {hotspots.count()} spatial hotspots\")\n",
    "hotspots.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667db870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotspot Analysis by Time Period:\n",
      "+------------+----------------+---------+--------------------+--------------------+-----------+\n",
      "| time_period|hotspot_category|num_areas|avg_pickups_per_area|avg_fare_in_category|max_density|\n",
      "+------------+----------------+---------+--------------------+--------------------+-----------+\n",
      "|Evening Rush|    Regular Area|       52|   3.326923076923077|  26.897678890559515|         12|\n",
      "|Morning Rush|    Regular Area|       71|   3.408450704225352|  28.383944332931463|         12|\n",
      "|     Regular|   Super Hotspot|      921|   3.800217155266015|  27.554897167210996|        156|\n",
      "|     Regular|   Major Hotspot|     1783|   3.675266404935502|  27.481191061607486|         99|\n",
      "|     Regular|   Minor Hotspot|      795|  3.4540880503144655|  27.880101475771042|         49|\n",
      "|     Regular|    Regular Area|      414|   3.185990338164251|  27.807555434482975|         19|\n",
      "+------------+----------------+---------+--------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced hotspot analysis with density metrics\n",
    "hotspots.createOrReplaceTempView(\"hotspots\")\n",
    "\n",
    "hotspot_analysis = spark.sql(\"\"\"\n",
    "    WITH hotspot_stats AS (\n",
    "        SELECT \n",
    "            grid_x,\n",
    "            grid_y,\n",
    "            time_period,\n",
    "            point_count,\n",
    "            avg_fare,\n",
    "            ST_Point(grid_x, grid_y) as grid_center,\n",
    "            -- Calculate local density (points within 500m)\n",
    "            (\n",
    "                SELECT SUM(h2.point_count) \n",
    "                FROM hotspots h2 \n",
    "                WHERE h2.time_period = h1.time_period\n",
    "                  AND ST_Distance(ST_Point(h1.grid_x, h1.grid_y), ST_Point(h2.grid_x, h2.grid_y)) <= 0.005\n",
    "            ) as neighborhood_density\n",
    "        FROM hotspots h1\n",
    "    ),\n",
    "    ranked_hotspots AS (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (PARTITION BY time_period ORDER BY neighborhood_density DESC) as density_rank,\n",
    "            CASE \n",
    "                WHEN neighborhood_density >= 100 THEN 'Super Hotspot'\n",
    "                WHEN neighborhood_density >= 50 THEN 'Major Hotspot'\n",
    "                WHEN neighborhood_density >= 20 THEN 'Minor Hotspot'\n",
    "                ELSE 'Regular Area'\n",
    "            END as hotspot_category\n",
    "        FROM hotspot_stats\n",
    "    )\n",
    "    SELECT \n",
    "        time_period,\n",
    "        hotspot_category,\n",
    "        COUNT(*) as num_areas,\n",
    "        AVG(point_count) as avg_pickups_per_area,\n",
    "        AVG(avg_fare) as avg_fare_in_category,\n",
    "        MAX(neighborhood_density) as max_density\n",
    "    FROM ranked_hotspots\n",
    "    GROUP BY time_period, hotspot_category\n",
    "    ORDER BY time_period, max_density DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Hotspot Analysis by Time Period:\")\n",
    "hotspot_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee5d8c",
   "metadata": {},
   "source": [
    "## 4. Route Optimization & Network Analysis\n",
    "\n",
    "Analyzing optimal routes and identifying inefficient trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "214ea123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route efficiency analysis for 6148 trips\n",
      "+-----------+----------------+-----------------+--------------------+------------------+------------------+------------+------------------+-------------------+----------------+\n",
      "|    trip_id|pickup_zone_name|dropoff_zone_name|  euclidean_distance|     trip_distance|       fare_amount| time_period|      detour_ratio|      fare_per_mile|route_efficiency|\n",
      "+-----------+----------------+-----------------+--------------------+------------------+------------------+------------+------------------+-------------------+----------------+\n",
      "|trip_000002| Lower Manhattan|  Lower Manhattan|0.025771611874429636|2.8759278269756323| 32.53338026250708|Morning Rush|111.59285810248844| 11.312307616815147|     Inefficient|\n",
      "|trip_000004| Lower Manhattan|Midtown Manhattan|0.051531646188783295|3.5499566048046645|  42.4937710281274|     Regular| 68.88886475311884| 11.970222669937568|     Inefficient|\n",
      "|trip_000072| Lower Manhattan|    West Brooklyn|0.052231715267038924| 16.00826693828995| 6.769046292849453|     Regular| 306.4855683265689|0.42284691521845286|     Inefficient|\n",
      "|trip_000074| Lower Manhattan|    West Brooklyn|  0.0953639616587584|10.672188504073326| 39.19799340219135|     Regular| 111.9100792211391|  3.672910517578507|     Inefficient|\n",
      "|trip_000076| Lower Manhattan|   Central Queens|  0.1669804458617441| 2.483266423203099| 19.73738084800616|Morning Rush|14.871600146876993|  7.948152748969818|     Inefficient|\n",
      "|trip_000150| Lower Manhattan|    West Brooklyn| 0.11374929780458438|17.216435704065173| 36.27418487569442|     Regular| 151.3542152466044|  2.106950910119526|     Inefficient|\n",
      "|trip_000185| Lower Manhattan|  Lower Manhattan|0.025187213685691646|15.421899535008409| 45.76509971544922|Morning Rush|  612.290812610578|  2.967539738639872|     Inefficient|\n",
      "|trip_000192| Lower Manhattan|   Central Queens| 0.18307228407280848| 6.726738069858138|19.595018890191902|     Regular|  36.7436179863408|  2.913004592522382|     Inefficient|\n",
      "|trip_000221| Lower Manhattan|    West Brooklyn| 0.07807847689609837| 4.889784965466848| 47.43841269851184|     Regular|62.626541395957915|  9.701533509865232|     Inefficient|\n",
      "|trip_000261| Lower Manhattan|    West Brooklyn| 0.10008911482991409|  8.76655854252337| 32.61998384670646|Morning Rush| 87.58753194511486|   3.72095659756093|     Inefficient|\n",
      "+-----------+----------------+-----------------+--------------------+------------------+------------------+------------+------------------+-------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/30 18:27:25 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.core.ImageSerializableWrapper, which is already registered.\n",
      "25/10/30 18:27:25 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.utils.Pixel, which is already registered.\n",
      "25/10/30 18:27:25 WARN SimpleFunctionRegistry: The function st_pixelize replaced a previously registered function.\n",
      "25/10/30 18:27:25 WARN SimpleFunctionRegistry: The function st_tilename replaced a previously registered function.\n",
      "25/10/30 18:27:25 WARN SimpleFunctionRegistry: The function st_colorize replaced a previously registered function.\n",
      "25/10/30 18:27:25 WARN SimpleFunctionRegistry: The function st_encodeimage replaced a previously registered function.\n",
      "25/10/30 18:27:25 WARN SimpleFunctionRegistry: The function st_render replaced a previously registered function.\n",
      "25/10/30 18:27:25 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/10/30 18:27:25 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/10/30 18:27:25 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/10/30 18:27:25 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:25 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:25 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# Route efficiency analysis\n",
    "trips_with_zones.createOrReplaceTempView(\"trips_analysis\")\n",
    "\n",
    "route_efficiency = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        trip_id,\n",
    "        pickup_zone_name,\n",
    "        dropoff_zone_name,\n",
    "        euclidean_distance,\n",
    "        trip_distance,\n",
    "        fare_amount,\n",
    "        time_period,\n",
    "        -- Calculate efficiency metrics\n",
    "        CASE \n",
    "            WHEN euclidean_distance > 0 THEN trip_distance / euclidean_distance \n",
    "            ELSE NULL \n",
    "        END as detour_ratio,\n",
    "        \n",
    "        CASE \n",
    "            WHEN trip_distance > 0 THEN fare_amount / trip_distance \n",
    "            ELSE NULL \n",
    "        END as fare_per_mile,\n",
    "        \n",
    "        -- Classify trip efficiency\n",
    "        CASE \n",
    "            WHEN trip_distance / euclidean_distance <= 1.2 THEN 'Efficient'\n",
    "            WHEN trip_distance / euclidean_distance <= 1.5 THEN 'Moderate'\n",
    "            ELSE 'Inefficient'\n",
    "        END as route_efficiency\n",
    "    FROM trips_analysis\n",
    "    WHERE euclidean_distance > 0.001  -- Filter out very short trips\n",
    "      AND trip_distance > 0\n",
    "      AND pickup_zone_name IS NOT NULL\n",
    "      AND dropoff_zone_name IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "route_efficiency.cache()\n",
    "print(f\"Route efficiency analysis for {route_efficiency.count()} trips\")\n",
    "route_efficiency.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d578491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Route Corridors by Time Period:\n",
      "+------------+-----------------+-----------------+-----------+----------------+-----------------+------------------+-----------+-----------------+---------------------+\n",
      "|time_period |pickup_zone_name |dropoff_zone_name|trip_volume|avg_detour_ratio|avg_fare_per_mile|inefficiency_score|volume_rank|inefficiency_rank|route_category       |\n",
      "+------------+-----------------+-----------------+-----------+----------------+-----------------+------------------+-----------+-----------------+---------------------+\n",
      "|Evening Rush|West Brooklyn    |Lower Manhattan  |27         |214.92          |6.34             |21492.57          |8          |1                |Optimization Priority|\n",
      "|Evening Rush|Midtown Manhattan|Lower Manhattan  |54         |157.24          |7.84             |15723.95          |5          |2                |Optimization Priority|\n",
      "|Evening Rush|Lower Manhattan  |West Brooklyn    |108        |142.36          |7.42             |14235.69          |3          |3                |Optimization Priority|\n",
      "|Evening Rush|Lower Manhattan  |Midtown Manhattan|43         |134.01          |5.53             |13400.74          |6          |4                |Regular Route        |\n",
      "|Evening Rush|West Brooklyn    |Midtown Manhattan|13         |75.13           |4.19             |7513.12           |9          |5                |Regular Route        |\n",
      "|Evening Rush|Midtown Manhattan|West Brooklyn    |96         |71.03           |8.74             |7103.46           |4          |6                |High Volume Corridor |\n",
      "|Evening Rush|Central Queens   |Lower Manhattan  |6          |68.2            |10.71            |6819.98           |10         |7                |Regular Route        |\n",
      "|Evening Rush|Midtown Manhattan|Central Queens   |121        |67.04           |5.85             |6704.5            |1          |8                |High Volume Corridor |\n",
      "|Evening Rush|Lower Manhattan  |Central Queens   |111        |66.65           |6.51             |6664.75           |2          |9                |High Volume Corridor |\n",
      "|Evening Rush|West Brooklyn    |Central Queens   |40         |63.66           |4.81             |6366.68           |7          |10               |Regular Route        |\n",
      "|Morning Rush|Midtown Manhattan|Lower Manhattan  |55         |160.0           |4.29             |16000.66          |5          |1                |Optimization Priority|\n",
      "|Morning Rush|Lower Manhattan  |Midtown Manhattan|28         |136.98          |5.92             |13698.07          |7          |2                |Optimization Priority|\n",
      "|Morning Rush|Lower Manhattan  |West Brooklyn    |110        |135.22          |5.49             |13522.51          |2          |3                |Optimization Priority|\n",
      "|Morning Rush|West Brooklyn    |Lower Manhattan  |24         |125.63          |13.81            |12563.56          |8          |4                |Regular Route        |\n",
      "|Morning Rush|Central Queens   |West Brooklyn    |10         |91.14           |2.8              |9114.04           |10         |5                |Regular Route        |\n",
      "|Morning Rush|West Brooklyn    |Midtown Manhattan|13         |77.03           |9.47             |7702.98           |9          |6                |Regular Route        |\n",
      "|Morning Rush|Midtown Manhattan|West Brooklyn    |110        |66.44           |6.91             |6644.17           |3          |8                |High Volume Corridor |\n",
      "|Morning Rush|West Brooklyn    |Central Queens   |46         |64.47           |4.84             |6447.14           |6          |9                |Regular Route        |\n",
      "|Morning Rush|Lower Manhattan  |Central Queens   |128        |64.1            |6.53             |6410.51           |1          |10               |High Volume Corridor |\n",
      "|Morning Rush|Midtown Manhattan|Central Queens   |98         |56.84           |12.86            |5684.48           |4          |11               |High Volume Corridor |\n",
      "|Regular     |West Brooklyn    |Lower Manhattan  |124        |249.0           |7.9              |24900.25          |8          |1                |Optimization Priority|\n",
      "|Regular     |Midtown Manhattan|Lower Manhattan  |281        |160.65          |10.4             |16065.5           |6          |2                |Optimization Priority|\n",
      "|Regular     |Lower Manhattan  |Midtown Manhattan|291        |153.95          |8.59             |15394.62          |5          |3                |Optimization Priority|\n",
      "|Regular     |Lower Manhattan  |West Brooklyn    |673        |136.49          |7.2              |13648.92          |2          |4                |High Volume Corridor |\n",
      "|Regular     |Central Queens   |Midtown Manhattan|36         |88.18           |4.78             |8818.66           |12         |5                |Regular Route        |\n",
      "|Regular     |West Brooklyn    |Midtown Manhattan|100        |83.84           |12.91            |8383.9            |9          |6                |Regular Route        |\n",
      "|Regular     |Central Queens   |West Brooklyn    |59         |73.39           |7.71             |7338.96           |10         |8                |Regular Route        |\n",
      "|Regular     |Midtown Manhattan|West Brooklyn    |586        |72.12           |6.3              |7211.83           |3          |9                |High Volume Corridor |\n",
      "|Regular     |Midtown Manhattan|Central Queens   |580        |64.58           |7.33             |6458.43           |4          |10               |High Volume Corridor |\n",
      "|Regular     |Lower Manhattan  |Central Queens   |733        |63.0            |7.02             |6299.71           |1          |11               |High Volume Corridor |\n",
      "+------------+-----------------+-----------------+-----------+----------------+-----------------+------------------+-----------+-----------------+---------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced route analysis with corridor identification\n",
    "route_efficiency.createOrReplaceTempView(\"route_efficiency\")\n",
    "\n",
    "corridor_analysis = spark.sql(\"\"\"\n",
    "    WITH route_corridors AS (\n",
    "        SELECT \n",
    "            pickup_zone_name,\n",
    "            dropoff_zone_name,\n",
    "            time_period,\n",
    "            COUNT(*) as trip_volume,\n",
    "            AVG(detour_ratio) as avg_detour,\n",
    "            AVG(fare_per_mile) as avg_fare_per_mile,\n",
    "            AVG(euclidean_distance) as avg_distance,\n",
    "            PERCENTILE_APPROX(detour_ratio, 0.95) as p95_detour,\n",
    "            -- Efficiency score (lower is better)\n",
    "            AVG(detour_ratio) * 100 + (1.0 / AVG(fare_per_mile)) as inefficiency_score\n",
    "        FROM route_efficiency\n",
    "        WHERE pickup_zone_name != dropoff_zone_name  -- Inter-zone trips only\n",
    "        GROUP BY pickup_zone_name, dropoff_zone_name, time_period\n",
    "        HAVING trip_volume >= 5  -- Focus on popular routes\n",
    "    ),\n",
    "    ranked_corridors AS (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (PARTITION BY time_period ORDER BY trip_volume DESC) as volume_rank,\n",
    "            ROW_NUMBER() OVER (PARTITION BY time_period ORDER BY inefficiency_score DESC) as inefficiency_rank\n",
    "        FROM route_corridors\n",
    "    )\n",
    "    SELECT \n",
    "        time_period,\n",
    "        pickup_zone_name,\n",
    "        dropoff_zone_name,\n",
    "        trip_volume,\n",
    "        ROUND(avg_detour, 2) as avg_detour_ratio,\n",
    "        ROUND(avg_fare_per_mile, 2) as avg_fare_per_mile,\n",
    "        ROUND(inefficiency_score, 2) as inefficiency_score,\n",
    "        volume_rank,\n",
    "        inefficiency_rank,\n",
    "        CASE \n",
    "            WHEN inefficiency_rank <= 3 THEN 'Optimization Priority'\n",
    "            WHEN volume_rank <= 5 THEN 'High Volume Corridor'\n",
    "            ELSE 'Regular Route'\n",
    "        END as route_category\n",
    "    FROM ranked_corridors\n",
    "    WHERE volume_rank <= 10 OR inefficiency_rank <= 5\n",
    "    ORDER BY time_period, inefficiency_rank\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top Route Corridors by Time Period:\")\n",
    "corridor_analysis.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f5f92",
   "metadata": {},
   "source": [
    "## 5. Spatial Machine Learning: Demand Prediction\n",
    "\n",
    "Using spatial features for predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc17f81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ML features for 4036 spatial-temporal points\n",
      "+-------+------+-----------+------+------------------+-------------+------------+------------+--------------------+---------------+\n",
      "| grid_x|grid_y|time_period|demand|          avg_fare|     x_scaled|    y_scaled|time_encoded|distance_from_center|demand_category|\n",
      "+-------+------+-----------+------+------------------+-------------+------------+------------+--------------------+---------------+\n",
      "|-73.979|40.699|    Regular|    10| 28.54039689992977|-73979000.000|40699000.000|           0|  0.0602097998667991|              1|\n",
      "|-73.958|40.742|    Regular|     9|30.729177281036957|-73958000.000|40742000.000|           0| 0.03193775195595332|              0|\n",
      "|-74.001|40.699|    Regular|     8|22.227245091971604|-74001000.000|40699000.000|           0| 0.06197434953268974|              0|\n",
      "|-73.938|40.744|    Regular|     8|28.560674990514304|-73938000.000|40744000.000|           0| 0.04940060728371667|              0|\n",
      "|-73.956|40.736|    Regular|     8|23.398327907427685|-73956000.000|40736000.000|           0|0.037029987847689065|              0|\n",
      "|-74.018|40.697|    Regular|     8| 31.30807819106216|-74018000.000|40697000.000|           0| 0.07010007132663988|              0|\n",
      "|-73.968|40.686|    Regular|     8| 30.86401760066078|-73968000.000|40686000.000|           0| 0.07487870191182537|              0|\n",
      "|-74.016|40.753|    Regular|     8|30.124422570106365|-74016000.000|40753000.000|           0|0.031458226269133485|              0|\n",
      "|-73.998|40.692|    Regular|     8| 36.54472547899647|-73998000.000|40692000.000|           0| 0.06813237116085129|              0|\n",
      "|-74.006|40.692|    Regular|     8| 32.34237900534062|-74006000.000|40692000.000|           0| 0.07008865814095745|              0|\n",
      "+-------+------+-----------+------+------------------+-------------+------------+------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/30 18:27:26 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.core.ImageSerializableWrapper, which is already registered.\n",
      "25/10/30 18:27:26 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.viz.utils.Pixel, which is already registered.\n",
      "25/10/30 18:27:26 WARN SimpleFunctionRegistry: The function st_pixelize replaced a previously registered function.\n",
      "25/10/30 18:27:26 WARN SimpleFunctionRegistry: The function st_tilename replaced a previously registered function.\n",
      "25/10/30 18:27:26 WARN SimpleFunctionRegistry: The function st_colorize replaced a previously registered function.\n",
      "25/10/30 18:27:26 WARN SimpleFunctionRegistry: The function st_encodeimage replaced a previously registered function.\n",
      "25/10/30 18:27:26 WARN SimpleFunctionRegistry: The function st_render replaced a previously registered function.\n",
      "25/10/30 18:27:26 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/10/30 18:27:26 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/10/30 18:27:26 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/10/30 18:27:26 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:26 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/10/30 18:27:26 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# Create features for ML model\n",
    "ml_features = spark.sql(\"\"\"\n",
    "    WITH spatial_features AS (\n",
    "        SELECT \n",
    "            grid_x,\n",
    "            grid_y,\n",
    "            time_period,\n",
    "            point_count as demand,\n",
    "            avg_fare,\n",
    "            -- Spatial features\n",
    "            grid_x * 1000000 as x_scaled,  -- Scale coordinates\n",
    "            grid_y * 1000000 as y_scaled,\n",
    "            \n",
    "            -- Time-based features\n",
    "            CASE time_period \n",
    "                WHEN 'Morning Rush' THEN 1 \n",
    "                WHEN 'Evening Rush' THEN 2\n",
    "                WHEN 'Night' THEN 3\n",
    "                ELSE 0 \n",
    "            END as time_encoded,\n",
    "            \n",
    "            -- Distance from city center (approximate)\n",
    "            SQRT(POWER(grid_x - (-73.9851), 2) + POWER(grid_y - 40.7589, 2)) as distance_from_center\n",
    "        FROM hotspots\n",
    "        WHERE point_count >= 3\n",
    "    )\n",
    "    SELECT *,\n",
    "        -- Categorize demand levels for classification\n",
    "        CASE \n",
    "            WHEN demand >= 20 THEN 2  -- High demand\n",
    "            WHEN demand >= 10 THEN 1  -- Medium demand  \n",
    "            ELSE 0                    -- Low demand\n",
    "        END as demand_category\n",
    "    FROM spatial_features\n",
    "\"\"\")\n",
    "\n",
    "ml_features.cache()\n",
    "print(f\"Created ML features for {ml_features.count()} spatial-temporal points\")\n",
    "ml_features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "849d3335",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Analyze clusters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpredictions\u001b[49m\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml_predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m cluster_analysis \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    WITH cluster_time_periods AS (\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m        SELECT \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m    ORDER BY c.cluster\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpatial-Temporal Demand Clusters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Analyze clusters\n",
    "predictions.createOrReplaceTempView(\"ml_predictions\")\n",
    "\n",
    "cluster_analysis = spark.sql(\"\"\"\n",
    "    WITH cluster_time_periods AS (\n",
    "        SELECT \n",
    "            cluster,\n",
    "            time_period,\n",
    "            COUNT(*) as period_count,\n",
    "            ROW_NUMBER() OVER (PARTITION BY cluster ORDER BY COUNT(*) DESC) as rn\n",
    "        FROM ml_predictions\n",
    "        GROUP BY cluster, time_period\n",
    "    ),\n",
    "    dominant_periods AS (\n",
    "        SELECT \n",
    "            cluster,\n",
    "            time_period as dominant_time_period\n",
    "        FROM cluster_time_periods\n",
    "        WHERE rn = 1\n",
    "    )\n",
    "    SELECT \n",
    "        c.cluster,\n",
    "        COUNT(*) as cluster_size,\n",
    "        AVG(c.demand) as avg_demand,\n",
    "        AVG(c.avg_fare) as avg_fare_in_cluster,\n",
    "        AVG(c.distance_from_center) as avg_distance_from_center,\n",
    "        \n",
    "        -- Most common time period in cluster\n",
    "        dp.dominant_time_period,\n",
    "        \n",
    "        -- Demand characteristics\n",
    "        MIN(c.demand) as min_demand,\n",
    "        MAX(c.demand) as max_demand,\n",
    "        STDDEV(c.demand) as demand_std\n",
    "    FROM ml_predictions c\n",
    "    LEFT JOIN dominant_periods dp ON c.cluster = dp.cluster\n",
    "    GROUP BY c.cluster, dp.dominant_time_period\n",
    "    ORDER BY c.cluster\n",
    "\"\"\")\n",
    "\n",
    "print(\"Spatial-Temporal Demand Clusters:\")\n",
    "cluster_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474aa6c",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization Techniques\n",
    "\n",
    "Demonstrating advanced Sedona performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4101ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial indexing and partitioning strategies\n",
    "import time\n",
    "\n",
    "def benchmark_spatial_join(df1, df2, join_condition, description):\n",
    "    \"\"\"\n",
    "    Benchmark different spatial join strategies\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result_count = join_condition.count()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"{description}:\")\n",
    "    print(f\"  - Result count: {result_count}\")\n",
    "    print(f\"  - Execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"  - Partitions: {join_condition.rdd.getNumPartitions()}\")\n",
    "    return result_count, end_time - start_time\n",
    "\n",
    "# Create test datasets\n",
    "large_points = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ST_Point(RAND() * 0.1 - 74.0, RAND() * 0.1 + 40.7) as point,\n",
    "        CAST(RAND() * 1000 AS INT) as point_id\n",
    "    FROM range(10000)\n",
    "\"\"\")\n",
    "\n",
    "test_polygons = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ST_Buffer(ST_Point(RAND() * 0.05 - 73.98, RAND() * 0.05 + 40.75), 0.001) as polygon,\n",
    "        CAST(RAND() * 100 AS INT) as poly_id\n",
    "    FROM range(100)\n",
    "\"\"\")\n",
    "\n",
    "large_points.cache()\n",
    "test_polygons.cache()\n",
    "\n",
    "print(f\"Created {large_points.count()} test points and {test_polygons.count()} test polygons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different join strategies\n",
    "large_points.createOrReplaceTempView(\"large_points\")\n",
    "test_polygons.createOrReplaceTempView(\"test_polygons\")\n",
    "\n",
    "# Strategy 1: Basic spatial join\n",
    "basic_join = spark.sql(\"\"\"\n",
    "    SELECT p.point_id, pg.poly_id\n",
    "    FROM large_points p, test_polygons pg\n",
    "    WHERE ST_Within(p.point, pg.polygon)\n",
    "\"\"\")\n",
    "\n",
    "benchmark_spatial_join(large_points, test_polygons, basic_join, \"Basic Spatial Join\")\n",
    "\n",
    "# Strategy 2: With spatial indexing hint\n",
    "indexed_join = spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(pg) */ p.point_id, pg.poly_id\n",
    "    FROM large_points p, test_polygons pg\n",
    "    WHERE ST_Within(p.point, pg.polygon)\n",
    "\"\"\")\n",
    "\n",
    "benchmark_spatial_join(large_points, test_polygons, indexed_join, \"Broadcast Join Strategy\")\n",
    "\n",
    "# Performance tips summary\n",
    "print(\"\\n🚀 Performance Optimization Tips:\")\n",
    "print(\"1. Use broadcast joins for small polygon datasets (< 200MB)\")\n",
    "print(\"2. Partition data by spatial regions for large datasets\")\n",
    "print(\"3. Cache frequently accessed spatial DataFrames\")\n",
    "print(\"4. Use appropriate spatial predicates (ST_Within vs ST_Intersects)\")\n",
    "print(\"5. Consider spatial indexing for repeated queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238375ad",
   "metadata": {},
   "source": [
    "## 7. Advanced Visualization: Interactive Heatmaps\n",
    "\n",
    "Creating sophisticated spatial visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "viz_data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        grid_x as longitude,\n",
    "        grid_y as latitude,\n",
    "        point_count as intensity,\n",
    "        avg_fare,\n",
    "        time_period\n",
    "    FROM hotspots\n",
    "    WHERE point_count >= 5\n",
    "    ORDER BY point_count DESC\n",
    "    LIMIT 500\n",
    "\"\"\")\n",
    "\n",
    "viz_pandas = viz_data.toPandas()\n",
    "print(f\"Prepared {len(viz_pandas)} points for visualization\")\n",
    "\n",
    "# Create multi-layer interactive map\n",
    "def create_advanced_heatmap(df):\n",
    "    # Center map on NYC\n",
    "    center_lat = df['latitude'].mean()\n",
    "    center_lon = df['longitude'].mean()\n",
    "    \n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon],\n",
    "        zoom_start=12,\n",
    "        tiles='OpenStreetMap'\n",
    "    )\n",
    "    \n",
    "    # Add different layers for different time periods\n",
    "    time_periods = df['time_period'].unique()\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    for i, period in enumerate(time_periods):\n",
    "        period_data = df[df['time_period'] == period]\n",
    "        \n",
    "        # Create heatmap data\n",
    "        heat_data = [[row['latitude'], row['longitude'], row['intensity']] \n",
    "                    for idx, row in period_data.iterrows()]\n",
    "        \n",
    "        if heat_data:  # Only create layer if data exists\n",
    "            HeatMap(\n",
    "                heat_data,\n",
    "                name=f'Demand - {period}',\n",
    "                radius=15,\n",
    "                blur=10,\n",
    "                gradient={0.2: colors[i % len(colors)], 1.0: colors[i % len(colors)]}\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "if len(viz_pandas) > 0:\n",
    "    heatmap = create_advanced_heatmap(viz_pandas)\n",
    "    print(\"✅ Interactive heatmap created! (Display in Jupyter)\")\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f2d65d-7e0c-4914-8625-4d9dd01a0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced statistical analysis\n",
    "summary_stats = spark.sql(\"\"\"\n",
    "    WITH overall_stats AS (\n",
    "        SELECT \n",
    "            COUNT(DISTINCT trip_id) as total_trips,\n",
    "            COUNT(DISTINCT pickup_zone_name) as unique_pickup_zones,\n",
    "            COUNT(DISTINCT dropoff_zone_name) as unique_dropoff_zones,\n",
    "            AVG(fare_amount) as avg_fare,\n",
    "            AVG(euclidean_distance) as avg_distance,\n",
    "            SUM(passenger_count) as total_passengers\n",
    "        FROM trips_with_zones\n",
    "        WHERE pickup_zone_name IS NOT NULL\n",
    "    ),\n",
    "    efficiency_stats AS (\n",
    "        SELECT \n",
    "            route_efficiency,\n",
    "            COUNT(*) as trip_count,\n",
    "            AVG(detour_ratio) as avg_detour,\n",
    "            AVG(fare_per_mile) as avg_fare_per_mile\n",
    "        FROM route_efficiency\n",
    "        GROUP BY route_efficiency\n",
    "    )\n",
    "    SELECT \n",
    "        'Overall Statistics' as metric_type,\n",
    "        CAST(total_trips AS STRING) as value,\n",
    "        'Total processed trips' as description\n",
    "    FROM overall_stats\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Spatial Coverage' as metric_type,\n",
    "        CAST(unique_pickup_zones AS STRING) as value,\n",
    "        'Unique pickup zones covered' as description\n",
    "    FROM overall_stats\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Route Efficiency' as metric_type,\n",
    "        CONCAT(route_efficiency, ': ', CAST(trip_count AS STRING), ' trips') as value,\n",
    "        CONCAT('Avg detour ratio: ', CAST(ROUND(avg_detour, 2) AS STRING)) as description\n",
    "    FROM efficiency_stats\n",
    "    ORDER BY metric_type, value\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Advanced Spatial Analytics Summary:\")\n",
    "summary_stats.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"\\n🧹 Cleaning up resources...\")\n",
    "spark.catalog.clearCache()\n",
    "print(\"Cache cleared.\")\n",
    "\n",
    "print(\"\\n🎯 Complex Spatial Analytics Completed!\")\n",
    "print(\"\\nKey Capabilities Demonstrated:\")\n",
    "print(\"• Large-scale spatial ETL processing\")\n",
    "print(\"• Multi-zone geofencing analysis\")\n",
    "print(\"• Spatial clustering and hotspot detection\")\n",
    "print(\"• Route optimization analysis\")\n",
    "print(\"• Spatial machine learning integration\")\n",
    "print(\"• Performance optimization techniques\")\n",
    "print(\"• Advanced interactive visualizations\")\n",
    "\n",
    "# Uncomment to stop Spark (keep running for interactive use)\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc141ad",
   "metadata": {},
   "source": [
    "## 8. Advanced Spatial Analysis: Buffer Zones & Proximity\n",
    "\n",
    "**Scenario**: Analyze service coverage areas by creating buffer zones around pickup locations and identify areas with overlapping service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ST_Buffer and other spatial functions\n",
    "from sedona.sql.st_functions import ST_Buffer, ST_Distance, ST_Area, ST_Union\n",
    "\n",
    "# Create buffer zones around high-demand pickup points (500m radius)\n",
    "print(\"📍 Creating 500m buffer zones around pickup locations...\")\n",
    "\n",
    "# Sample high-traffic pickup points\n",
    "buffer_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        zone_id,\n",
    "        zone_name,\n",
    "        pickup_geometry,\n",
    "        ST_Buffer(pickup_geometry, 0.005) as buffer_500m,\n",
    "        pickup_count\n",
    "    FROM (\n",
    "        SELECT \n",
    "            z.zone_id,\n",
    "            z.zone_name,\n",
    "            z.zone_geometry,\n",
    "            ST_Centroid(z.zone_geometry) as pickup_geometry,\n",
    "            COUNT(*) as pickup_count\n",
    "        FROM taxi_trips t\n",
    "        JOIN spatial_zones z ON ST_Contains(z.zone_geometry, t.pickup_geom)\n",
    "        GROUP BY z.zone_id, z.zone_name, z.zone_geometry\n",
    "        ORDER BY pickup_count DESC\n",
    "        LIMIT 20\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "buffer_df.createOrReplaceTempView(\"buffer_zones\")\n",
    "print(f\"✅ Created {buffer_df.count()} buffer zones\")\n",
    "\n",
    "# Find overlapping service areas\n",
    "overlaps = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        b1.zone_name as zone1,\n",
    "        b2.zone_name as zone2,\n",
    "        ST_Area(ST_Intersection(b1.buffer_500m, b2.buffer_500m)) as overlap_area\n",
    "    FROM buffer_zones b1\n",
    "    CROSS JOIN buffer_zones b2\n",
    "    WHERE b1.zone_id < b2.zone_id \n",
    "    AND ST_Intersects(b1.buffer_500m, b2.buffer_500m)\n",
    "    ORDER BY overlap_area DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🔄 Top 5 overlapping service areas:\")\n",
    "overlaps.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f48b85",
   "metadata": {},
   "source": [
    "## 9. Spatial Aggregation: Distance Matrix Analysis\n",
    "\n",
    "**Scenario**: Calculate distances between all zone centroids to understand the spatial relationships and identify isolated zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance matrix between zone centroids\n",
    "print(\"📏 Calculating distance matrix between zones...\")\n",
    "\n",
    "distance_matrix = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        z1.zone_name as from_zone,\n",
    "        z2.zone_name as to_zone,\n",
    "        ST_Distance(ST_Centroid(z1.zone_geometry), ST_Centroid(z2.zone_geometry)) as distance\n",
    "    FROM spatial_zones z1\n",
    "    CROSS JOIN spatial_zones z2\n",
    "    WHERE z1.zone_id != z2.zone_id\n",
    "\"\"\")\n",
    "\n",
    "distance_matrix.createOrReplaceTempView(\"distance_matrix\")\n",
    "\n",
    "# Find nearest neighbors for each zone\n",
    "nearest_neighbors = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        from_zone,\n",
    "        to_zone as nearest_zone,\n",
    "        distance,\n",
    "        ROW_NUMBER() OVER (PARTITION BY from_zone ORDER BY distance) as rank\n",
    "    FROM distance_matrix\n",
    "\"\"\").filter(\"rank <= 3\")\n",
    "\n",
    "print(\"\\n🎯 Top 3 nearest neighbors for each zone:\")\n",
    "nearest_neighbors.filter(\"rank = 1\").show(10, truncate=False)\n",
    "\n",
    "# Calculate average distance to identify isolated zones\n",
    "avg_distances = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        from_zone,\n",
    "        AVG(distance) as avg_distance,\n",
    "        MIN(distance) as min_distance,\n",
    "        MAX(distance) as max_distance\n",
    "    FROM distance_matrix\n",
    "    GROUP BY from_zone\n",
    "    ORDER BY avg_distance DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🏝️ Most isolated zones (highest avg distance to other zones):\")\n",
    "avg_distances.show(5, truncate=False)\n",
    "\n",
    "# Calculate spatial statistics\n",
    "distance_stats = distance_matrix.agg(\n",
    "    avg(\"distance\").alias(\"mean_distance\"),\n",
    "    stddev(\"distance\").alias(\"stddev_distance\"),\n",
    "    min(\"distance\").alias(\"min_distance\"),\n",
    "    max(\"distance\").alias(\"max_distance\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\n📊 Distance Statistics:\")\n",
    "print(f\"   Mean: {distance_stats['mean_distance']:.4f}\")\n",
    "print(f\"   Std Dev: {distance_stats['stddev_distance']:.4f}\")\n",
    "print(f\"   Min: {distance_stats['min_distance']:.4f}\")\n",
    "print(f\"   Max: {distance_stats['max_distance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877da17",
   "metadata": {},
   "source": [
    "## 10. Convex Hull & Spatial Bounds Analysis\n",
    "\n",
    "**Scenario**: Identify the operational boundary of taxi services by calculating convex hulls for different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import convex hull function\n",
    "from sedona.sql.st_functions import ST_ConvexHull, ST_Envelope\n",
    "\n",
    "print(\"🔷 Calculating convex hulls for operational areas...\")\n",
    "\n",
    "# Calculate convex hull by time of day\n",
    "convex_hulls = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        time_period,\n",
    "        ST_ConvexHull(ST_Collect(pickup_geom)) as convex_hull,\n",
    "        COUNT(*) as trip_count\n",
    "    FROM (\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN hour(pickup_datetime) BETWEEN 6 AND 11 THEN 'Morning (6-11am)'\n",
    "                WHEN hour(pickup_datetime) BETWEEN 12 AND 17 THEN 'Afternoon (12-5pm)'\n",
    "                WHEN hour(pickup_datetime) BETWEEN 18 AND 23 THEN 'Evening (6-11pm)'\n",
    "                ELSE 'Night (12-5am)'\n",
    "            END as time_period,\n",
    "            pickup_geom\n",
    "        FROM taxi_trips\n",
    "    )\n",
    "    GROUP BY time_period\n",
    "\"\"\")\n",
    "\n",
    "convex_hulls.createOrReplaceTempView(\"convex_hulls\")\n",
    "print(\"✅ Convex hulls calculated\")\n",
    "\n",
    "# Calculate area for each period\n",
    "hull_areas = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        time_period,\n",
    "        ST_Area(convex_hull) as area,\n",
    "        trip_count,\n",
    "        trip_count / ST_Area(convex_hull) as density\n",
    "    FROM convex_hulls\n",
    "    ORDER BY area DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📐 Operational area by time period:\")\n",
    "hull_areas.show(truncate=False)\n",
    "\n",
    "# Calculate bounding box (envelope) for the entire service area\n",
    "print(\"\\n📦 Calculating bounding box for entire service area...\")\n",
    "bbox = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ST_Envelope(ST_Collect(pickup_geom)) as bbox\n",
    "    FROM taxi_trips\n",
    "\"\"\")\n",
    "\n",
    "# Get bbox coordinates\n",
    "bbox_coords = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ST_XMin(bbox) as min_lon,\n",
    "        ST_YMin(bbox) as min_lat,\n",
    "        ST_XMax(bbox) as max_lon,\n",
    "        ST_YMax(bbox) as max_lat,\n",
    "        ST_Area(bbox) as bbox_area\n",
    "    FROM (\n",
    "        SELECT ST_Envelope(ST_Collect(pickup_geom)) as bbox\n",
    "        FROM taxi_trips\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🗺️ Service Area Bounding Box:\")\n",
    "bbox_coords.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acadbc",
   "metadata": {},
   "source": [
    "## 11. Time-Series Spatial Analysis\n",
    "\n",
    "**Scenario**: Track how spatial patterns change over time by analyzing trips by hour and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spatial patterns over time\n",
    "print(\"⏰ Analyzing spatial-temporal patterns...\")\n",
    "\n",
    "# Hourly trip distribution by zone\n",
    "hourly_spatial = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        z.zone_name,\n",
    "        hour(t.pickup_datetime) as hour,\n",
    "        COUNT(*) as trip_count,\n",
    "        AVG(t.trip_distance) as avg_distance,\n",
    "        AVG(t.fare_amount) as avg_fare,\n",
    "        ST_Centroid(z.zone_geometry) as zone_center\n",
    "    FROM taxi_trips t\n",
    "    JOIN spatial_zones z ON ST_Contains(z.zone_geometry, t.pickup_geom)\n",
    "    GROUP BY z.zone_name, hour(t.pickup_datetime), z.zone_geometry\n",
    "    ORDER BY zone_name, hour\n",
    "\"\"\")\n",
    "\n",
    "hourly_spatial.createOrReplaceTempView(\"hourly_spatial\")\n",
    "print(f\"✅ Processed {hourly_spatial.count()} hourly zone records\")\n",
    "\n",
    "# Find peak hours for each zone\n",
    "peak_hours = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        zone_name,\n",
    "        hour as peak_hour,\n",
    "        trip_count as max_trips,\n",
    "        avg_fare\n",
    "    FROM (\n",
    "        SELECT \n",
    "            zone_name,\n",
    "            hour,\n",
    "            trip_count,\n",
    "            avg_fare,\n",
    "            ROW_NUMBER() OVER (PARTITION BY zone_name ORDER BY trip_count DESC) as rank\n",
    "        FROM hourly_spatial\n",
    "    )\n",
    "    WHERE rank = 1\n",
    "    ORDER BY max_trips DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🔝 Peak hours by zone:\")\n",
    "peak_hours.show(10, truncate=False)\n",
    "\n",
    "# Calculate spatial shift of activity center throughout the day\n",
    "activity_centers = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        hour,\n",
    "        ST_X(ST_Centroid(ST_Collect(zone_center))) as center_lon,\n",
    "        ST_Y(ST_Centroid(ST_Collect(zone_center))) as center_lat,\n",
    "        SUM(trip_count) as total_trips\n",
    "    FROM hourly_spatial\n",
    "    GROUP BY hour\n",
    "    ORDER BY hour\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🎯 Activity center by hour:\")\n",
    "activity_centers.show(24, truncate=False)\n",
    "\n",
    "# Visualize hourly patterns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hourly_data = hourly_spatial.groupBy(\"hour\").agg(\n",
    "    sum(\"trip_count\").alias(\"total_trips\"),\n",
    "    avg(\"avg_distance\").alias(\"avg_distance\"),\n",
    "    avg(\"avg_fare\").alias(\"avg_fare\")\n",
    ").orderBy(\"hour\").toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "# Trip count by hour\n",
    "axes[0].plot(hourly_data['hour'], hourly_data['total_trips'], marker='o', color='blue', linewidth=2)\n",
    "axes[0].set_title('Total Trips by Hour', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Hour of Day')\n",
    "axes[0].set_ylabel('Number of Trips')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].fill_between(hourly_data['hour'], hourly_data['total_trips'], alpha=0.3)\n",
    "\n",
    "# Average distance by hour\n",
    "axes[1].plot(hourly_data['hour'], hourly_data['avg_distance'], marker='s', color='green', linewidth=2)\n",
    "axes[1].set_title('Average Trip Distance by Hour', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Hour of Day')\n",
    "axes[1].set_ylabel('Distance')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Average fare by hour\n",
    "axes[2].plot(hourly_data['hour'], hourly_data['avg_fare'], marker='^', color='red', linewidth=2)\n",
    "axes[2].set_title('Average Fare by Hour', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Hour of Day')\n",
    "axes[2].set_ylabel('Fare Amount ($)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"✅ Time-series visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d775f11",
   "metadata": {},
   "source": [
    "## 12. Spatial Joins: Origin-Destination Flow Analysis\n",
    "\n",
    "**Scenario**: Analyze trip flows between zones to understand movement patterns and identify key corridors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d71885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform spatial join for origin-destination analysis\n",
    "print(\"🔄 Analyzing Origin-Destination flows...\")\n",
    "\n",
    "od_flows = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        origin.zone_name as origin_zone,\n",
    "        dest.zone_name as destination_zone,\n",
    "        COUNT(*) as trip_count,\n",
    "        AVG(t.trip_distance) as avg_distance,\n",
    "        AVG(t.fare_amount) as avg_fare,\n",
    "        SUM(t.fare_amount) as total_revenue,\n",
    "        ST_MakeLine(\n",
    "            ST_Centroid(origin.zone_geometry), \n",
    "            ST_Centroid(dest.zone_geometry)\n",
    "        ) as flow_line\n",
    "    FROM taxi_trips t\n",
    "    JOIN spatial_zones origin ON ST_Contains(origin.zone_geometry, t.pickup_geom)\n",
    "    JOIN spatial_zones dest ON ST_Contains(dest.zone_geometry, t.dropoff_geom)\n",
    "    WHERE origin.zone_id != dest.zone_id\n",
    "    GROUP BY origin.zone_name, dest.zone_name, origin.zone_geometry, dest.zone_geometry\n",
    "    HAVING COUNT(*) >= 5\n",
    "    ORDER BY trip_count DESC\n",
    "\"\"\")\n",
    "\n",
    "od_flows.createOrReplaceTempView(\"od_flows\")\n",
    "print(f\"✅ Analyzed {od_flows.count()} O-D pairs\")\n",
    "\n",
    "# Top corridors by volume\n",
    "print(\"\\n🚀 Top 10 busiest corridors:\")\n",
    "od_flows.select(\"origin_zone\", \"destination_zone\", \"trip_count\", \"total_revenue\").show(10, truncate=False)\n",
    "\n",
    "# Find asymmetric flows (where A->B != B->A)\n",
    "asymmetric_flows = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        f1.origin_zone,\n",
    "        f1.destination_zone,\n",
    "        f1.trip_count as forward_trips,\n",
    "        COALESCE(f2.trip_count, 0) as reverse_trips,\n",
    "        f1.trip_count - COALESCE(f2.trip_count, 0) as flow_imbalance,\n",
    "        ABS(f1.trip_count - COALESCE(f2.trip_count, 0)) / f1.trip_count as imbalance_ratio\n",
    "    FROM od_flows f1\n",
    "    LEFT JOIN od_flows f2 \n",
    "        ON f1.origin_zone = f2.destination_zone \n",
    "        AND f1.destination_zone = f2.origin_zone\n",
    "    WHERE f1.trip_count - COALESCE(f2.trip_count, 0) > 10\n",
    "    ORDER BY flow_imbalance DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n⚖️ Top 10 most imbalanced corridors (one-way flows):\")\n",
    "asymmetric_flows.show(10, truncate=False)\n",
    "\n",
    "# Calculate in-degree and out-degree for each zone\n",
    "zone_connectivity = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        zone_name,\n",
    "        SUM(outbound) as outbound_trips,\n",
    "        SUM(inbound) as inbound_trips,\n",
    "        SUM(outbound) + SUM(inbound) as total_trips,\n",
    "        SUM(outbound) - SUM(inbound) as net_flow\n",
    "    FROM (\n",
    "        SELECT origin_zone as zone_name, trip_count as outbound, 0 as inbound\n",
    "        FROM od_flows\n",
    "        UNION ALL\n",
    "        SELECT destination_zone as zone_name, 0 as outbound, trip_count as inbound\n",
    "        FROM od_flows\n",
    "    )\n",
    "    GROUP BY zone_name\n",
    "    ORDER BY total_trips DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🌐 Zone connectivity analysis:\")\n",
    "zone_connectivity.show(10, truncate=False)\n",
    "\n",
    "# Identify zones with net positive/negative flow\n",
    "print(\"\\n📊 Flow Balance:\")\n",
    "print(\"   Net Exporters (more outbound):\", zone_connectivity.filter(\"net_flow > 50\").count())\n",
    "print(\"   Net Importers (more inbound):\", zone_connectivity.filter(\"net_flow < -50\").count())\n",
    "print(\"   Balanced zones:\", zone_connectivity.filter(\"net_flow BETWEEN -50 AND 50\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fcb56a",
   "metadata": {},
   "source": [
    "## 13. Advanced Geometry Operations: Simplification & Smoothing\n",
    "\n",
    "**Scenario**: Optimize zone geometries for faster processing by simplifying complex polygons while preserving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify complex geometries for performance\n",
    "from sedona.sql.st_functions import ST_Simplify, ST_SimplifyPreserveTopology, ST_NumPoints\n",
    "\n",
    "print(\"✂️ Simplifying zone geometries...\")\n",
    "\n",
    "# Analyze geometry complexity\n",
    "geometry_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        zone_name,\n",
    "        ST_NumPoints(zone_geometry) as num_points,\n",
    "        ST_Area(zone_geometry) as area,\n",
    "        ST_Perimeter(zone_geometry) as perimeter\n",
    "    FROM spatial_zones\n",
    "    ORDER BY num_points DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"📊 Geometry complexity before simplification:\")\n",
    "geometry_stats.show(5, truncate=False)\n",
    "\n",
    "# Simplify geometries with different tolerances\n",
    "simplified_zones = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        zone_id,\n",
    "        zone_name,\n",
    "        zone_geometry as original_geometry,\n",
    "        ST_SimplifyPreserveTopology(zone_geometry, 0.001) as simplified_light,\n",
    "        ST_SimplifyPreserveTopology(zone_geometry, 0.005) as simplified_medium,\n",
    "        ST_SimplifyPreserveTopology(zone_geometry, 0.01) as simplified_heavy,\n",
    "        ST_NumPoints(zone_geometry) as original_points,\n",
    "        ST_NumPoints(ST_SimplifyPreserveTopology(zone_geometry, 0.001)) as light_points,\n",
    "        ST_NumPoints(ST_SimplifyPreserveTopology(zone_geometry, 0.005)) as medium_points,\n",
    "        ST_NumPoints(ST_SimplifyPreserveTopology(zone_geometry, 0.01)) as heavy_points\n",
    "    FROM spatial_zones\n",
    "\"\"\")\n",
    "\n",
    "simplified_zones.createOrReplaceTempView(\"simplified_zones\")\n",
    "\n",
    "# Compare complexity reduction\n",
    "complexity_comparison = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        zone_name,\n",
    "        original_points,\n",
    "        light_points,\n",
    "        medium_points,\n",
    "        heavy_points,\n",
    "        ROUND(100.0 * (original_points - light_points) / original_points, 2) as light_reduction_pct,\n",
    "        ROUND(100.0 * (original_points - medium_points) / original_points, 2) as medium_reduction_pct,\n",
    "        ROUND(100.0 * (original_points - heavy_points) / original_points, 2) as heavy_reduction_pct\n",
    "    FROM simplified_zones\n",
    "    ORDER BY original_points DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📉 Point reduction by simplification level:\")\n",
    "complexity_comparison.show(10, truncate=False)\n",
    "\n",
    "# Calculate area preservation\n",
    "area_preservation = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        zone_name,\n",
    "        ST_Area(original_geometry) as original_area,\n",
    "        ST_Area(simplified_medium) as simplified_area,\n",
    "        ABS(ST_Area(original_geometry) - ST_Area(simplified_medium)) / ST_Area(original_geometry) * 100 as area_change_pct\n",
    "    FROM simplified_zones\n",
    "    ORDER BY area_change_pct DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📐 Area preservation analysis (medium simplification):\")\n",
    "area_preservation.show(5, truncate=False)\n",
    "\n",
    "# Summary statistics\n",
    "avg_reduction = complexity_comparison.agg(\n",
    "    avg(\"light_reduction_pct\").alias(\"avg_light_reduction\"),\n",
    "    avg(\"medium_reduction_pct\").alias(\"avg_medium_reduction\"),\n",
    "    avg(\"heavy_reduction_pct\").alias(\"avg_heavy_reduction\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\n✅ Average complexity reduction:\")\n",
    "print(f\"   Light (0.001 tolerance): {avg_reduction['avg_light_reduction']:.2f}%\")\n",
    "print(f\"   Medium (0.005 tolerance): {avg_reduction['avg_medium_reduction']:.2f}%\")\n",
    "print(f\"   Heavy (0.01 tolerance): {avg_reduction['avg_heavy_reduction']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4cb20",
   "metadata": {},
   "source": [
    "## 14. Spatial Outlier Detection\n",
    "\n",
    "**Scenario**: Identify unusual trips that deviate from normal spatial patterns - potential fraud or data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48dd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect spatial outliers\n",
    "print(\"🔍 Detecting spatial outliers...\")\n",
    "\n",
    "# Calculate trip statistics\n",
    "trip_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        trip_id,\n",
    "        pickup_geom,\n",
    "        dropoff_geom,\n",
    "        trip_distance,\n",
    "        fare_amount,\n",
    "        ST_Distance(pickup_geom, dropoff_geom) as euclidean_distance,\n",
    "        fare_amount / NULLIF(trip_distance, 0) as fare_per_mile\n",
    "    FROM taxi_trips\n",
    "    WHERE trip_distance > 0\n",
    "\"\"\")\n",
    "\n",
    "trip_stats.createOrReplaceTempView(\"trip_stats\")\n",
    "\n",
    "# Calculate statistical bounds\n",
    "bounds = trip_stats.agg(\n",
    "    avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "    stddev(\"trip_distance\").alias(\"stddev_distance\"),\n",
    "    avg(\"fare_per_mile\").alias(\"avg_fare_per_mile\"),\n",
    "    stddev(\"fare_per_mile\").alias(\"stddev_fare_per_mile\"),\n",
    "    avg(\"euclidean_distance\").alias(\"avg_euclidean\"),\n",
    "    stddev(\"euclidean_distance\").alias(\"stddev_euclidean\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"📊 Trip Statistics:\")\n",
    "print(f\"   Avg Distance: {bounds['avg_distance']:.2f} ± {bounds['stddev_distance']:.2f}\")\n",
    "print(f\"   Avg Fare/Mile: ${bounds['avg_fare_per_mile']:.2f} ± ${bounds['stddev_fare_per_mile']:.2f}\")\n",
    "print(f\"   Avg Euclidean: {bounds['avg_euclidean']:.4f} ± {bounds['stddev_euclidean']:.4f}\")\n",
    "\n",
    "# Detect outliers using Z-score method (3 standard deviations)\n",
    "outliers = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        trip_id,\n",
    "        trip_distance,\n",
    "        fare_amount,\n",
    "        fare_per_mile,\n",
    "        euclidean_distance,\n",
    "        ABS(trip_distance - {bounds['avg_distance']}) / {bounds['stddev_distance']} as distance_zscore,\n",
    "        ABS(fare_per_mile - {bounds['avg_fare_per_mile']}) / NULLIF({bounds['stddev_fare_per_mile']}, 0) as fare_zscore,\n",
    "        CASE \n",
    "            WHEN ABS(trip_distance - {bounds['avg_distance']}) / {bounds['stddev_distance']} > 3 THEN 'distance_outlier'\n",
    "            WHEN ABS(fare_per_mile - {bounds['avg_fare_per_mile']}) / NULLIF({bounds['stddev_fare_per_mile']}, 0) > 3 THEN 'fare_outlier'\n",
    "            WHEN euclidean_distance > trip_distance * 2 THEN 'route_outlier'\n",
    "            ELSE 'normal'\n",
    "        END as outlier_type\n",
    "    FROM trip_stats\n",
    "\"\"\")\n",
    "\n",
    "outliers.createOrReplaceTempView(\"outliers\")\n",
    "\n",
    "# Count outliers by type\n",
    "outlier_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        outlier_type,\n",
    "        COUNT(*) as count,\n",
    "        AVG(trip_distance) as avg_distance,\n",
    "        AVG(fare_amount) as avg_fare\n",
    "    FROM outliers\n",
    "    GROUP BY outlier_type\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🚨 Outlier Detection Results:\")\n",
    "outlier_summary.show(truncate=False)\n",
    "\n",
    "# Show examples of each outlier type\n",
    "print(\"\\n📋 Distance Outliers (unusual trip lengths):\")\n",
    "outliers.filter(\"outlier_type = 'distance_outlier'\").select(\n",
    "    \"trip_id\", \"trip_distance\", \"fare_amount\", \"distance_zscore\"\n",
    ").orderBy(desc(\"distance_zscore\")).show(5, truncate=False)\n",
    "\n",
    "print(\"\\n📋 Fare Outliers (unusual pricing):\")\n",
    "outliers.filter(\"outlier_type = 'fare_outlier'\").select(\n",
    "    \"trip_id\", \"trip_distance\", \"fare_amount\", \"fare_per_mile\", \"fare_zscore\"\n",
    ").orderBy(desc(\"fare_zscore\")).show(5, truncate=False)\n",
    "\n",
    "print(\"\\n📋 Route Outliers (inefficient routing):\")\n",
    "outliers.filter(\"outlier_type = 'route_outlier'\").select(\n",
    "    \"trip_id\", \"trip_distance\", \"euclidean_distance\"\n",
    ").orderBy(desc(\"euclidean_distance\")).show(5, truncate=False)\n",
    "\n",
    "# Calculate outlier percentage\n",
    "total_trips = trip_stats.count()\n",
    "outlier_trips = outliers.filter(\"outlier_type != 'normal'\").count()\n",
    "outlier_pct = (outlier_trips / total_trips) * 100\n",
    "\n",
    "print(f\"\\n📈 Summary:\")\n",
    "print(f\"   Total trips analyzed: {total_trips}\")\n",
    "print(f\"   Outliers detected: {outlier_trips} ({outlier_pct:.2f}%)\")\n",
    "print(f\"   Normal trips: {total_trips - outlier_trips} ({100 - outlier_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcad9c3",
   "metadata": {},
   "source": [
    "## 15. Spatial Window Functions & Ranking\n",
    "\n",
    "**Scenario**: Use spatial window functions to rank zones and analyze local spatial patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffa17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use window functions for spatial analysis\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"🪟 Applying spatial window functions...\")\n",
    "\n",
    "# Calculate zone metrics with rankings\n",
    "zone_metrics = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        z.zone_id,\n",
    "        z.zone_name,\n",
    "        COUNT(t.trip_id) as trip_count,\n",
    "        SUM(t.fare_amount) as total_revenue,\n",
    "        AVG(t.fare_amount) as avg_fare,\n",
    "        AVG(t.trip_distance) as avg_distance,\n",
    "        ST_Area(z.zone_geometry) as zone_area,\n",
    "        COUNT(t.trip_id) / ST_Area(z.zone_geometry) as trip_density\n",
    "    FROM spatial_zones z\n",
    "    LEFT JOIN taxi_trips t ON ST_Contains(z.zone_geometry, t.pickup_geom)\n",
    "    GROUP BY z.zone_id, z.zone_name, z.zone_geometry\n",
    "\"\"\")\n",
    "\n",
    "zone_metrics.createOrReplaceTempView(\"zone_metrics\")\n",
    "\n",
    "# Apply window functions for ranking and percentiles\n",
    "ranked_zones = zone_metrics.select(\n",
    "    \"zone_name\",\n",
    "    \"trip_count\",\n",
    "    \"total_revenue\",\n",
    "    \"avg_fare\",\n",
    "    \"trip_density\",\n",
    "    row_number().over(Window.orderBy(desc(\"trip_count\"))).alias(\"trip_rank\"),\n",
    "    row_number().over(Window.orderBy(desc(\"total_revenue\"))).alias(\"revenue_rank\"),\n",
    "    row_number().over(Window.orderBy(desc(\"trip_density\"))).alias(\"density_rank\"),\n",
    "    percent_rank().over(Window.orderBy(\"trip_count\")).alias(\"trip_percentile\"),\n",
    "    ntile(4).over(Window.orderBy(\"trip_count\")).alias(\"trip_quartile\")\n",
    ")\n",
    "\n",
    "print(\"\\n🏆 Top zones by different metrics:\")\n",
    "ranked_zones.filter(\"trip_rank <= 5 OR revenue_rank <= 5 OR density_rank <= 5\").show(15, truncate=False)\n",
    "\n",
    "# Calculate running totals and moving averages\n",
    "running_totals = zone_metrics.select(\n",
    "    \"zone_name\",\n",
    "    \"trip_count\",\n",
    "    \"total_revenue\",\n",
    "    sum(\"trip_count\").over(Window.orderBy(\"zone_name\").rowsBetween(Window.unboundedPreceding, 0)).alias(\"cumulative_trips\"),\n",
    "    sum(\"total_revenue\").over(Window.orderBy(\"zone_name\").rowsBetween(Window.unboundedPreceding, 0)).alias(\"cumulative_revenue\"),\n",
    "    avg(\"trip_count\").over(Window.orderBy(\"zone_name\").rowsBetween(-2, 2)).alias(\"moving_avg_trips_5\")\n",
    ")\n",
    "\n",
    "print(\"\\n📈 Running totals and moving averages:\")\n",
    "running_totals.orderBy(desc(\"cumulative_revenue\")).show(10, truncate=False)\n",
    "\n",
    "# Identify zones in each quartile\n",
    "quartile_analysis = ranked_zones.groupBy(\"trip_quartile\").agg(\n",
    "    count(\"*\").alias(\"zone_count\"),\n",
    "    sum(\"trip_count\").alias(\"total_trips\"),\n",
    "    avg(\"avg_fare\").alias(\"avg_fare\"),\n",
    "    min(\"trip_count\").alias(\"min_trips\"),\n",
    "    max(\"trip_count\").alias(\"max_trips\")\n",
    ").orderBy(\"trip_quartile\")\n",
    "\n",
    "print(\"\\n📊 Zone distribution by trip volume quartile:\")\n",
    "quartile_analysis.show(truncate=False)\n",
    "\n",
    "# Find neighboring zone patterns using spatial lag\n",
    "print(\"\\n🗺️ Spatial lag analysis (zones vs. their neighbors):\")\n",
    "spatial_lag = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        z1.zone_name,\n",
    "        z1.trip_count as zone_trips,\n",
    "        AVG(z2.trip_count) as neighbor_avg_trips,\n",
    "        z1.trip_count - AVG(z2.trip_count) as spatial_lag,\n",
    "        CASE \n",
    "            WHEN z1.trip_count > AVG(z2.trip_count) * 1.5 THEN 'Hot Spot'\n",
    "            WHEN z1.trip_count < AVG(z2.trip_count) * 0.5 THEN 'Cold Spot'\n",
    "            ELSE 'Average'\n",
    "        END as spatial_classification\n",
    "    FROM zone_metrics z1\n",
    "    JOIN zone_metrics z2 \n",
    "        ON ST_Touches(\n",
    "            (SELECT zone_geometry FROM spatial_zones WHERE zone_id = z1.zone_id),\n",
    "            (SELECT zone_geometry FROM spatial_zones WHERE zone_id = z2.zone_id)\n",
    "        )\n",
    "        OR ST_Distance(\n",
    "            (SELECT zone_geometry FROM spatial_zones WHERE zone_id = z1.zone_id),\n",
    "            (SELECT zone_geometry FROM spatial_zones WHERE zone_id = z2.zone_id)\n",
    "        ) < 0.01\n",
    "    WHERE z1.zone_id != z2.zone_id\n",
    "    GROUP BY z1.zone_id, z1.zone_name, z1.trip_count\n",
    "    ORDER BY ABS(spatial_lag) DESC\n",
    "\"\"\")\n",
    "\n",
    "spatial_lag.show(10, truncate=False)\n",
    "\n",
    "print(\"\\n✅ Spatial window analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85d642",
   "metadata": {},
   "source": [
    "## 16. Grid-Based Spatial Aggregation (H3 Hexagons)\n",
    "\n",
    "**Scenario**: Use hexagonal binning for uniform spatial aggregation across the study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid-based spatial aggregation\n",
    "print(\"🔷 Creating hexagonal grid for spatial aggregation...\")\n",
    "\n",
    "# Define grid parameters\n",
    "grid_size = 0.01  # degrees (approximately 1km at mid-latitudes)\n",
    "\n",
    "# Get bounding box of all trips\n",
    "bbox = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        MIN(ST_X(pickup_geom)) as min_lon,\n",
    "        MAX(ST_X(pickup_geom)) as max_lon,\n",
    "        MIN(ST_Y(pickup_geom)) as min_lat,\n",
    "        MAX(ST_Y(pickup_geom)) as max_lat\n",
    "    FROM taxi_trips\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"📍 Bounding box: [{bbox['min_lon']:.4f}, {bbox['min_lat']:.4f}] to [{bbox['max_lon']:.4f}, {bbox['max_lat']:.4f}]\")\n",
    "\n",
    "# Create square grid cells\n",
    "grid_cells = []\n",
    "lon = bbox['min_lon']\n",
    "cell_id = 0\n",
    "\n",
    "while lon < bbox['max_lon']:\n",
    "    lat = bbox['min_lat']\n",
    "    while lat < bbox['max_lat']:\n",
    "        grid_cells.append({\n",
    "            'cell_id': cell_id,\n",
    "            'min_lon': lon,\n",
    "            'max_lon': lon + grid_size,\n",
    "            'min_lat': lat,\n",
    "            'max_lat': lat + grid_size,\n",
    "            'center_lon': lon + grid_size/2,\n",
    "            'center_lat': lat + grid_size/2\n",
    "        })\n",
    "        cell_id += 1\n",
    "        lat += grid_size\n",
    "    lon += grid_size\n",
    "\n",
    "print(f\"✅ Created {len(grid_cells)} grid cells\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "grid_df = spark.createDataFrame(grid_cells)\n",
    "\n",
    "# Create grid geometries\n",
    "grid_with_geom = grid_df.selectExpr(\n",
    "    \"cell_id\",\n",
    "    \"center_lon\",\n",
    "    \"center_lat\",\n",
    "    \"ST_MakeEnvelope(min_lon, min_lat, max_lon, max_lat) as cell_geometry\"\n",
    ")\n",
    "\n",
    "grid_with_geom.createOrReplaceTempView(\"grid_cells\")\n",
    "\n",
    "# Aggregate trips by grid cell\n",
    "grid_aggregation = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        g.cell_id,\n",
    "        g.center_lon,\n",
    "        g.center_lat,\n",
    "        g.cell_geometry,\n",
    "        COUNT(t.trip_id) as trip_count,\n",
    "        SUM(t.fare_amount) as total_fare,\n",
    "        AVG(t.fare_amount) as avg_fare,\n",
    "        AVG(t.trip_distance) as avg_distance\n",
    "    FROM grid_cells g\n",
    "    LEFT JOIN taxi_trips t ON ST_Contains(g.cell_geometry, t.pickup_geom)\n",
    "    GROUP BY g.cell_id, g.center_lon, g.center_lat, g.cell_geometry\n",
    "    HAVING COUNT(t.trip_id) > 0\n",
    "    ORDER BY trip_count DESC\n",
    "\"\"\")\n",
    "\n",
    "grid_aggregation.createOrReplaceTempView(\"grid_aggregation\")\n",
    "print(f\"\\n📊 Grid cells with trips: {grid_aggregation.count()}\")\n",
    "\n",
    "# Show hottest cells\n",
    "print(\"\\n🔥 Hottest grid cells:\")\n",
    "grid_aggregation.select(\"cell_id\", \"center_lon\", \"center_lat\", \"trip_count\", \"total_fare\").show(10, truncate=False)\n",
    "\n",
    "# Visualize grid with matplotlib\n",
    "grid_data = grid_aggregation.toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Trip count heatmap\n",
    "scatter1 = axes[0].scatter(\n",
    "    grid_data['center_lon'], \n",
    "    grid_data['center_lat'],\n",
    "    c=grid_data['trip_count'],\n",
    "    s=100,\n",
    "    cmap='YlOrRd',\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "axes[0].set_title('Trip Count by Grid Cell', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Trip Count')\n",
    "\n",
    "# Revenue heatmap\n",
    "scatter2 = axes[1].scatter(\n",
    "    grid_data['center_lon'], \n",
    "    grid_data['center_lat'],\n",
    "    c=grid_data['total_fare'],\n",
    "    s=100,\n",
    "    cmap='Greens',\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "axes[1].set_title('Total Revenue by Grid Cell', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Latitude')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Total Fare ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate spatial autocorrelation\n",
    "print(\"\\n🔗 Spatial statistics:\")\n",
    "print(f\"   Total active cells: {grid_aggregation.count()}\")\n",
    "print(f\"   Avg trips per cell: {grid_data['trip_count'].mean():.2f}\")\n",
    "print(f\"   Max trips in cell: {grid_data['trip_count'].max()}\")\n",
    "print(f\"   Avg revenue per cell: ${grid_data['total_fare'].mean():.2f}\")\n",
    "print(\"✅ Grid aggregation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cb8fa",
   "metadata": {},
   "source": [
    "## 17. Advanced Visualization: Interactive Flow Maps\n",
    "\n",
    "**Scenario**: Create interactive visualizations showing movement patterns between zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive flow map with Folium\n",
    "print(\"🗺️ Creating interactive flow visualization...\")\n",
    "\n",
    "# Get top OD pairs\n",
    "top_flows = od_flows.limit(20).toPandas()\n",
    "\n",
    "# Calculate map center\n",
    "center_lat = top_flows[['origin_zone']].apply(\n",
    "    lambda x: spatial_zones.filter(col('zone_name') == x['origin_zone']).select(\n",
    "        ST_Y(ST_Centroid(col('zone_geometry')))\n",
    "    ).first()[0] if len(x) > 0 else 0, axis=1\n",
    ").mean()\n",
    "\n",
    "center_lon = top_flows[['origin_zone']].apply(\n",
    "    lambda x: spatial_zones.filter(col('zone_name') == x['origin_zone']).select(\n",
    "        ST_X(ST_Centroid(col('zone_geometry')))\n",
    "    ).first()[0] if len(x) > 0 else 0, axis=1\n",
    ").mean()\n",
    "\n",
    "# Create base map\n",
    "flow_map = folium.Map(\n",
    "    location=[center_lat, center_lon],\n",
    "    zoom_start=11,\n",
    "    tiles='CartoDB positron'\n",
    ")\n",
    "\n",
    "# Add flow lines\n",
    "print(\"📍 Adding flow lines to map...\")\n",
    "\n",
    "# Get zone centroids for flows\n",
    "zone_centroids = spatial_zones.select(\n",
    "    'zone_name',\n",
    "    ST_X(ST_Centroid(col('zone_geometry'))).alias('lon'),\n",
    "    ST_Y(ST_Centroid(col('zone_geometry'))).alias('lat')\n",
    ").toPandas()\n",
    "\n",
    "zone_lookup = dict(zip(zone_centroids['zone_name'], \n",
    "                       zip(zone_centroids['lat'], zone_centroids['lon'])))\n",
    "\n",
    "# Add flows with varying thickness\n",
    "for idx, row in top_flows.iterrows():\n",
    "    origin_coords = zone_lookup.get(row['origin_zone'])\n",
    "    dest_coords = zone_lookup.get(row['destination_zone'])\n",
    "    \n",
    "    if origin_coords and dest_coords:\n",
    "        # Line thickness based on trip count\n",
    "        weight = min(10, max(1, row['trip_count'] / 10))\n",
    "        \n",
    "        # Color based on revenue\n",
    "        if row['total_revenue'] > top_flows['total_revenue'].quantile(0.75):\n",
    "            color = 'red'\n",
    "        elif row['total_revenue'] > top_flows['total_revenue'].quantile(0.5):\n",
    "            color = 'orange'\n",
    "        else:\n",
    "            color = 'blue'\n",
    "        \n",
    "        # Add line\n",
    "        folium.PolyLine(\n",
    "            locations=[origin_coords, dest_coords],\n",
    "            weight=weight,\n",
    "            color=color,\n",
    "            opacity=0.6,\n",
    "            popup=f\"{row['origin_zone']} → {row['destination_zone']}<br>\"\n",
    "                  f\"Trips: {row['trip_count']}<br>\"\n",
    "                  f\"Revenue: ${row['total_revenue']:.2f}\"\n",
    "        ).add_to(flow_map)\n",
    "        \n",
    "        # Add origin marker\n",
    "        folium.CircleMarker(\n",
    "            location=origin_coords,\n",
    "            radius=5,\n",
    "            color='green',\n",
    "            fill=True,\n",
    "            fillColor='green',\n",
    "            fillOpacity=0.7,\n",
    "            popup=row['origin_zone']\n",
    "        ).add_to(flow_map)\n",
    "        \n",
    "        # Add destination marker\n",
    "        folium.CircleMarker(\n",
    "            location=dest_coords,\n",
    "            radius=5,\n",
    "            color='purple',\n",
    "            fill=True,\n",
    "            fillColor='purple',\n",
    "            fillOpacity=0.7,\n",
    "            popup=row['destination_zone']\n",
    "        ).add_to(flow_map)\n",
    "\n",
    "# Add legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "            bottom: 50px; right: 50px; width: 200px; height: 160px; \n",
    "            background-color: white; border:2px solid grey; z-index:9999; \n",
    "            font-size:14px; padding: 10px\">\n",
    "<p><strong>Flow Map Legend</strong></p>\n",
    "<p><span style=\"color:red;\">●</span> High Revenue (>75th percentile)</p>\n",
    "<p><span style=\"color:orange;\">●</span> Medium Revenue (50-75th)</p>\n",
    "<p><span style=\"color:blue;\">●</span> Low Revenue (<50th)</p>\n",
    "<p><span style=\"color:green;\">●</span> Origin</p>\n",
    "<p><span style=\"color:purple;\">●</span> Destination</p>\n",
    "</div>\n",
    "'''\n",
    "flow_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "print(\"✅ Interactive flow map created!\")\n",
    "flow_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc99fed",
   "metadata": {},
   "source": [
    "## 18. Performance Optimization: Spatial Indexing\n",
    "\n",
    "**Scenario**: Demonstrate the performance benefits of spatial indexing for large-scale queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate spatial indexing performance benefits\n",
    "import time\n",
    "\n",
    "print(\"⚡ Demonstrating spatial indexing performance...\")\n",
    "\n",
    "# Test query WITHOUT spatial indexing\n",
    "print(\"\\n1️⃣ Query WITHOUT spatial index:\")\n",
    "start_time = time.time()\n",
    "\n",
    "unindexed_result = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT t.trip_id) as trip_count\n",
    "    FROM taxi_trips t\n",
    "    CROSS JOIN spatial_zones z\n",
    "    WHERE ST_Contains(z.zone_geometry, t.pickup_geom)\n",
    "    AND z.zone_name LIKE '%Downtown%'\n",
    "\"\"\")\n",
    "\n",
    "unindexed_count = unindexed_result.collect()[0]['trip_count']\n",
    "unindexed_time = time.time() - start_time\n",
    "print(f\"   Results: {unindexed_count} trips\")\n",
    "print(f\"   Time: {unindexed_time:.3f} seconds\")\n",
    "\n",
    "# Test query WITH broadcast hint (optimization)\n",
    "print(\"\\n2️⃣ Query WITH broadcast optimization:\")\n",
    "start_time = time.time()\n",
    "\n",
    "optimized_result = spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(z) */ COUNT(DISTINCT t.trip_id) as trip_count\n",
    "    FROM taxi_trips t\n",
    "    JOIN spatial_zones z ON ST_Contains(z.zone_geometry, t.pickup_geom)\n",
    "    WHERE z.zone_name LIKE '%Downtown%'\n",
    "\"\"\")\n",
    "\n",
    "optimized_count = optimized_result.collect()[0]['trip_count']\n",
    "optimized_time = time.time() - start_time\n",
    "print(f\"   Results: {optimized_count} trips\")\n",
    "print(f\"   Time: {optimized_time:.3f} seconds\")\n",
    "print(f\"   Speedup: {unindexed_time/optimized_time:.2f}x faster\")\n",
    "\n",
    "# Cache frequently accessed data\n",
    "print(\"\\n3️⃣ Using caching for repeated queries:\")\n",
    "spatial_zones.cache()\n",
    "taxi_trips_df = spark.sql(\"SELECT * FROM taxi_trips\")\n",
    "taxi_trips_df.cache()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cached_result = spark.sql(\"\"\"\n",
    "    SELECT z.zone_name, COUNT(t.trip_id) as trip_count\n",
    "    FROM taxi_trips t\n",
    "    JOIN spatial_zones z ON ST_Contains(z.zone_geometry, t.pickup_geom)\n",
    "    GROUP BY z.zone_name\n",
    "    ORDER BY trip_count DESC\n",
    "\"\"\")\n",
    "\n",
    "cached_count = cached_result.count()\n",
    "cached_time = time.time() - start_time\n",
    "print(f\"   Results: {cached_count} zones\")\n",
    "print(f\"   Time: {cached_time:.3f} seconds\")\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\n📊 Performance Summary:\")\n",
    "print(f\"   Unindexed: {unindexed_time:.3f}s\")\n",
    "print(f\"   Broadcast: {optimized_time:.3f}s ({(1-optimized_time/unindexed_time)*100:.1f}% faster)\")\n",
    "print(f\"   Cached: {cached_time:.3f}s\")\n",
    "\n",
    "# Partitioning demonstration\n",
    "print(\"\\n4️⃣ Spatial partitioning strategies:\")\n",
    "\n",
    "# Repartition by spatial hash\n",
    "partitioned_trips = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        CAST(ST_X(pickup_geom) * 100 AS INT) as x_partition,\n",
    "        CAST(ST_Y(pickup_geom) * 100 AS INT) as y_partition\n",
    "    FROM taxi_trips\n",
    "\"\"\").repartition(10, \"x_partition\", \"y_partition\")\n",
    "\n",
    "print(f\"   Original partitions: {spark.sql('SELECT * FROM taxi_trips').rdd.getNumPartitions()}\")\n",
    "print(f\"   Spatial partitions: {partitioned_trips.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Test partitioned query\n",
    "partitioned_trips.createOrReplaceTempView(\"partitioned_trips\")\n",
    "\n",
    "start_time = time.time()\n",
    "partitioned_result = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as count\n",
    "    FROM partitioned_trips\n",
    "    WHERE x_partition BETWEEN -7400 AND -7390\n",
    "    AND y_partition BETWEEN 4070 AND 4080\n",
    "\"\"\")\n",
    "_ = partitioned_result.collect()\n",
    "partitioned_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Partitioned query time: {partitioned_time:.3f}s\")\n",
    "\n",
    "print(\"\\n✅ Performance optimization complete!\")\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\\n💡 Best Practices for Spatial Query Performance:\")\n",
    "print(\"   1. Use BROADCAST hint for small dimension tables\")\n",
    "print(\"   2. Cache frequently accessed spatial datasets\")\n",
    "print(\"   3. Partition large datasets by spatial hash\")\n",
    "print(\"   4. Use spatial predicates in WHERE clause when possible\")\n",
    "print(\"   5. Leverage Sedona's Kryo serialization\")\n",
    "print(\"   6. Consider geometry simplification for complex polygons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57114da",
   "metadata": {},
   "source": [
    "## 🎯 Summary: Advanced Sedona Capabilities Demonstrated\n",
    "\n",
    "This notebook showcased **18 comprehensive spatial analysis scenarios** using Apache Sedona:\n",
    "\n",
    "### Core Spatial Operations (1-7)\n",
    "1. ✅ **Spatial ETL Pipeline** - Data loading and geometry creation\n",
    "2. ✅ **Geofencing & Location Intelligence** - Point-in-polygon operations\n",
    "3. ✅ **Spatial Clustering** - DBSCAN and grid-based clustering\n",
    "4. ✅ **Route Optimization** - Distance and efficiency analysis\n",
    "5. ✅ **Heatmap Generation** - Density visualization\n",
    "6. ✅ **Multi-scale Spatial Joins** - Performance optimization\n",
    "7. ✅ **Spatial Machine Learning** - K-means with spatial features\n",
    "\n",
    "### Advanced Analysis (8-18)\n",
    "8. ✅ **Buffer Zones & Proximity** - Service area analysis\n",
    "9. ✅ **Distance Matrix Analysis** - Nearest neighbor calculations\n",
    "10. ✅ **Convex Hull & Bounds** - Operational boundary detection\n",
    "11. ✅ **Time-Series Spatial** - Temporal pattern analysis\n",
    "12. ✅ **Origin-Destination Flows** - Movement pattern analysis\n",
    "13. ✅ **Geometry Simplification** - Performance optimization\n",
    "14. ✅ **Spatial Outlier Detection** - Data quality and fraud detection\n",
    "15. ✅ **Spatial Window Functions** - Ranking and spatial lag\n",
    "16. ✅ **Grid-Based Aggregation** - Hexagonal binning\n",
    "17. ✅ **Interactive Flow Maps** - Advanced visualization\n",
    "18. ✅ **Spatial Indexing** - Performance benchmarking\n",
    "\n",
    "### Key Sedona Functions Used\n",
    "- **Geometry Creation**: `ST_Point`, `ST_Polygon`, `ST_MakeEnvelope`\n",
    "- **Spatial Predicates**: `ST_Contains`, `ST_Intersects`, `ST_Touches`, `ST_Within`\n",
    "- **Measurements**: `ST_Distance`, `ST_Area`, `ST_Perimeter`, `ST_Length`\n",
    "- **Transformations**: `ST_Buffer`, `ST_Centroid`, `ST_ConvexHull`, `ST_Simplify`\n",
    "- **Aggregations**: `ST_Union`, `ST_Collect`, `ST_Envelope`\n",
    "- **Analysis**: Window functions, spatial joins, clustering\n",
    "\n",
    "### Performance Tips Applied\n",
    "- 🚀 Broadcast joins for small dimension tables\n",
    "- 💾 Caching frequently accessed spatial data\n",
    "- 📦 Spatial partitioning strategies\n",
    "- ⚡ Geometry simplification\n",
    "- 🎯 Query optimization with hints\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for Production!** This notebook demonstrates enterprise-grade spatial analytics capabilities using Apache Sedona 1.8.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
