# ğŸ—ºï¸ Apache Sedona Docker Environment

A complete, production-ready Docker environment for **Apache Sedona** (formerly GeoSpark) with Apache Spark, featuring comprehensive spatial analytics capabilities, interactive Jupyter notebooks, and a full Python data science stack.

[![Apache Sedona](https://img.shields.io/badge/Apache%20Sedona-1.8.0-blue)](https://sedona.apache.org/)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.4.0-orange)](https://spark.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-green)](https://www.docker.com/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Lab%20%26%20Notebook-orange)](https://jupyter.org/)

## ğŸš€ Quick Start

### 1ï¸âƒ£ Launch Environment
```bash
# Start all services
docker-compose up -d

# Verify everything is working
./scripts/verify-setup.sh
```

### 2ï¸âƒ£ Access Jupyter
- **URL:** http://localhost:8888
- **Password:** `sedona123` 
- **Recommended:** Open `notebooks/advanced-sedona-examples.ipynb`

### 3ï¸âƒ£ Run Spatial Analytics
Execute the **"ğŸš€ QUICK START: Run All Prerequisites"** cell in the notebook to set up sample data, then explore:
- NYC taxi trip analysis with geofencing
- Spatial clustering and hotspot detection  
- Machine learning with location features
- Interactive visualizations

## ğŸŒŸ What's Included

### ğŸ”§ Core Technologies
- **Apache Spark 3.4.0** - Distributed computing with Hadoop 3.3.4
- **Apache Sedona 1.8.0** - Complete spatial analytics (Core, SQL, Viz modules)
- **GeoTools Wrapper 1.6.1-28.2** - Full spatial functionality (fixes FactoryException)
- **Python 3.11** - Latest stable Python runtime

### ğŸ“Š Data Science Stack
- **Visualization:** matplotlib, seaborn, plotly, folium (interactive maps)
- **Spatial Analysis:** geopandas, shapely, pyproj
- **Machine Learning:** scikit-learn, scipy, numpy, pandas  
- **Development:** jupyter, jupyterlab, ipywidgets

### ğŸš¢ Docker Services
| Service | Purpose | URL |
|---------|---------|-----|
| `spark-master` | Spark cluster coordination | http://localhost:8080 |
| `spark-worker` | Distributed processing | (scales horizontally) |
| `jupyter` | Interactive notebooks | http://localhost:8888 |
| `pyspark` | Python Spark shell | (terminal access) |

## ğŸ¯ Advanced Spatial Analytics Features

### ğŸ“ Spatial Operations (100+ Functions)
```sql
-- Distance calculations
SELECT ST_Distance(point1, point2) as distance_km

-- Geometric operations  
SELECT ST_Buffer(geometry, 1000) as buffer_zone

-- Spatial relationships
SELECT * FROM trips WHERE ST_Contains(zone_geom, pickup_point)

-- Spatial aggregations
SELECT ST_ConvexHull(ST_Collect(points)) as service_area
```

### ğŸ—ºï¸ Real-World Use Cases
- **ğŸš• Transportation:** NYC taxi analysis, route optimization, traffic patterns
- **ğŸ˜ï¸ Urban Planning:** Zoning analysis, service area coverage, density studies  
- **ğŸ“ˆ Business Intelligence:** Location-based customer analytics, market analysis
- **ğŸŒ Environmental:** Spatial clustering, hotspot detection, impact analysis
- **ğŸ¤– Machine Learning:** Spatial feature engineering, location-based prediction

### ğŸ“Š Visualization Capabilities
- **Interactive Maps:** Folium with Leaflet.js integration
- **Statistical Plots:** Matplotlib, Seaborn for spatial distributions
- **Real-time Dashboards:** Plotly for dynamic visualizations  
- **Jupyter Widgets:** Interactive parameter adjustment

## ğŸ“š Comprehensive Examples

The **`advanced-sedona-examples.ipynb`** notebook provides:

### ğŸ“ Learning Path
1. **Environment Setup** - Verify installation and test spatial functions
2. **Data Generation** - Create realistic NYC taxi trip datasets
3. **Spatial Operations** - Distance, area, buffer, intersection calculations  
4. **Spatial Analytics** - Geofencing, point-in-polygon joins, clustering
5. **Visualization** - Interactive maps, statistical analysis, performance charts
6. **Machine Learning** - Spatial clustering (DBSCAN), feature engineering
7. **Performance Optimization** - Indexing, caching, partitioning strategies

### ğŸ” Code Examples
```python
# Spatial join with geofencing
trips_in_zones = spark.sql("""
    SELECT t.*, z.zone_name
    FROM spatial_trips t
    JOIN spatial_zones z ON ST_Contains(z.zone_geometry, t.pickup_point)
""")

# Spatial clustering for hotspot detection  
from pyspark.ml.clustering import DBSCAN
dbscan = DBSCAN(featuresCol="location_features", eps=0.01, minPts=10)
clusters = dbscan.fit(spatial_features).transform(spatial_features)

# Interactive visualization
import folium
m = folium.Map(location=[40.7589, -73.9851])
folium.plugins.HeatMap(hotspot_data).add_to(m)
```

## ğŸ› ï¸ Technical Architecture

### âš™ï¸ System Specifications
| Component | Version | Configuration |
|-----------|---------|---------------|
| Apache Spark | 3.4.0 | Standalone cluster mode |
| Apache Sedona | 1.8.0 | Core + SQL + Viz modules |
| Hadoop | 3.3.4 | Distributed file system |
| Scala | 2.12 | JVM runtime |
| Python | 3.11 | Latest stable |
| JDK | 11 | OpenJDK with spatial libs |

### ğŸ§  Memory Configuration
```yaml
Spark Master: 1GB heap
Spark Worker: 2GB heap (configurable)  
Spark Executor: 1GB heap
Spark Driver: 1GB heap
```

### ğŸ—ƒï¸ Supported Data Formats
- **Vector:** Shapefile, GeoJSON, WKT, WKB, GeoParquet, PostGIS
- **Coordinate Systems:** WGS84, UTM zones, State Plane, custom CRS
- **Geometry Types:** Point, LineString, Polygon, MultiPolygon, Collections

## ğŸ”§ Advanced Configuration

### ğŸ“ˆ Scaling Workers
```bash
# Scale to 3 worker nodes
docker-compose up -d --scale spark-worker=3

# Monitor cluster in Spark UI
open http://localhost:8080
```

### ğŸ”„ Development Workflow  
```bash
# Rebuild after changes
docker-compose down
docker-compose build --no-cache  
docker-compose up -d

# Test comprehensive functionality
./scripts/comprehensive-test.sh

# Interactive debugging
docker-compose exec jupyter bash
```

### âš¡ Performance Optimization
```python
# Spatial indexing for faster queries
df.createOrReplaceTempView("indexed_trips")
spark.sql("CREATE INDEX idx_pickup ON indexed_trips USING rtree (pickup_point)")

# Data partitioning by spatial bounds
spatial_df.repartition("zone_id").cache()

# Broadcast small reference datasets
broadcast_zones = broadcast(zones_df)
```

## ğŸ› Troubleshooting Guide

### âœ… Common Issues & Solutions

| Issue | Solution | Verification |
|-------|----------|--------------|
| FactoryException | âœ… **Resolved** - GeoTools wrapper included | `./scripts/verify-setup.sh` |
| Missing packages | âœ… **Resolved** - All packages pre-installed | Check imports in notebook |
| TABLE_OR_VIEW_NOT_FOUND | Execute "Quick Start" cell first | Run prerequisites setup |
| Memory errors | Increase worker memory in docker-compose.yml | Monitor Spark UI |
| Slow queries | Add spatial indices and cache DataFrames | Use `.explain()` |

### ğŸ” Debugging Commands
```bash
# Check service status
docker-compose ps

# View service logs  
docker-compose logs jupyter
docker-compose logs spark-master

# Interactive shell access
docker-compose exec jupyter python
docker-compose exec spark-master pyspark

# Resource monitoring
docker stats
```

### ğŸ¯ Performance Monitoring
- **Spark UI:** http://localhost:8080 - Monitor jobs, stages, executors
- **Application UI:** http://localhost:4040 - Active job details  
- **Resource Usage:** `docker stats` - Memory and CPU utilization
- **Query Plans:** Use `.explain()` on DataFrames for optimization

## ğŸ§ª Testing & Validation

### ğŸ”¬ Automated Tests
```bash
# Quick verification (30 seconds)
./scripts/verify-setup.sh

# Comprehensive testing (2-3 minutes)  
./scripts/comprehensive-test.sh

# Custom spatial function tests
./scripts/geotools-wrapper-fix.sh
```

### âœ… Expected Results
```
ğŸ¯ Core Functions:
âœ… ST_Distance: 5.0 degrees
âœ… ST_Area: 4.0 square degrees  
âœ… ST_Buffer: Valid geometry
âœ… Spatial Join: 15000+ trip-zone matches

ğŸ“Š Performance Benchmarks:
- Spatial operations: <100ms per 1K records
- Spatial joins: <5s per 100K x 1K records  
- Clustering: <30s per 50K points
```

## ğŸ“– Documentation & Resources

### ğŸ“ Learning Resources
- **Apache Sedona Docs:** [sedona.apache.org](https://sedona.apache.org/)
- **Spatial SQL Reference:** [PostGIS Functions](https://postgis.net/docs/reference.html)
- **Python Geospatial:** [Geopandas Guide](https://geopandas.org/)
- **Spark Performance:** [Tuning Guide](https://spark.apache.org/docs/latest/tuning.html)

### ğŸ¤ Community & Support  
- **GitHub Issues:** Report bugs and request features
- **Sedona Mailing List:** [dev@sedona.apache.org](mailto:dev@sedona.apache.org)
- **Stack Overflow:** Tag questions with `apache-sedona`
- **Spatial Analytics Community:** Join GIS and spatial data discussions

## ğŸ“„ License & Acknowledgments

This project leverages **Apache Sedona** (Apache License 2.0) and the broader Apache Spark ecosystem. Special thanks to the Apache Sedona community for creating powerful open-source spatial analytics tools.

**Built with â¤ï¸ for the spatial data science community**